
@misc{zotero-4,
  title = {{{LaTeX}}-Examples},
  abstract = {LaTeX-examples - Examples for the usage of LaTeX},
  howpublished = {https://github.com/MartinThoma/LaTeX-examples},
  journal = {GitHub}
}

@article{Amit1997,
  title = {Model of Global Spontaneous Activity and Local Structured Activity during Delay Periods in the Cerebral Cortex.},
  volume = {7},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/7.3.237},
  abstract = {We investigate self-sustaining stable states (attractors) in networks of integrate-and-fire neurons. First, we study the stability of spontaneous activity in an unstructured network. It is shown that the stochastic background activity, of 1-5 spikes/s, is unstable if all neurons are excitatory. On the other hand, spontaneous activity becomes self-stabilizing in presence of local inhibition, given reasonable values of the parameters of the network. Second, in a network sustaining physiological spontaneous rates, we study the effect of learning in a local module, expressed in synaptic modifications in specific populations of synapses. We find that if the average synaptic potentiation (LTP) is too low, no stimulus specific activity manifests itself in the delay period. Instead, following the presentation and removal of any stimulus there is, in the local module, a delay activity in which all neurons selective (responding visually) to any of the stimuli presented for learning have rates which gradually increase with the amplitude of synaptic potentiation. When the average LTP increases beyond a critical value, specific local attractors (stable states) appear abruptly against the background of the global uniform spontaneous attractor. In this case the local module has two available types of collective delay activity: if the stimulus is unfamiliar, the activity is spontaneous; if it is similar to a learned stimulus, delay activity is selective. These new attractors reflect the synaptic structure developed during learning. In each of them a small population of neurons have elevated rates, which depend on the strength of LTP. The remaining neurons of the module have their activity at spontaneous rates. The predictions made in this paper could be checked by single unit recordings in delayed response experiments.},
  language = {en},
  number = {3},
  journal = {Cerebral Cortex},
  author = {Amit, D. J. and Brunel, N.},
  month = jan,
  year = {1997},
  pages = {237-252},
  file = {articles/Amit1997.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/2FBK9UE2/237.html},
  pmid = {9143444}
}

@article{Schaffer2013,
  title = {A {{Complex}}-{{Valued Firing}}-{{Rate Model That Approximates}} the {{Dynamics}} of {{Spiking Networks}}},
  volume = {9},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003301},
  language = {en},
  number = {10},
  journal = {PLoS Computational Biology},
  author = {Schaffer, Evan S. and Ostojic, Srdjan and Abbott, L. F.},
  editor = {Ermentrout, Bard},
  month = oct,
  year = {2013},
  pages = {e1003301},
  file = {articles/Schaffer2013.pdf}
}

@techreport{zotero-7,
  title = {Easylist},
  file = {manuals/latex/easylist.pdf},
  note = {manuals/latex}
}

@unpublished{Depperschmidt2011,
  title = {Markovketten},
  author = {Depperschmidt, Andrej},
  year = {2011},
  file = {manuscripts/Depperschmidt2011_Markovketten.pdf}
}

@unpublished{Hoffmann2015h,
  title = {Optimierung},
  author = {Hoffmann},
  year = {2015},
  file = {manuscripts/Hoffmann2015_Optimierung.pdf}
}

@article{Kuljis2010,
  title = {Integrative Understanding of Emergent Brain Properties, Quantum Brain Hypotheses, and Connectome Alterations in Dementia Are Key Challenges to Conquer {{Alzheimer}}'s Disease},
  volume = {1},
  doi = {10.3389/fneur.2010.00015},
  abstract = {The biological substrate for cognition remains a challenge as much as defining this function of living beings. Here, we examine some of the difficulties to understand normal and disordered cognition in humans. We use aspects of Alzheimer's disease and related disorders to illustrate how the wealth of information at many conceptually separate, even intellectually decoupled, physical scales \textendash{} in particular at the Molecular Neuroscience versus Systems Neuroscience/Neuropsychology levels \textendash{} presents a challenge in terms of true interdisciplinary integration towards a coherent understanding. These unresolved dilemmas include critically the as yet untested quantum brain hypothesis, and the embryonic attempts to develop and define the so-called connectome in humans and in non-human models of disease. To mitigate these challenges, we propose a scheme incorporating the vast array of scales of the space and time (space\textendash{}time) manifold from at least the subatomic through cognitive-behavioral dimensions of inquiry, to achieve a new understanding of both normal and disordered cognition, that is essential for a new era of progress in the Generative Sciences and its application to translational efforts for disease prevention and treatment.},
  journal = {Dementia},
  author = {Kulji{\v s}, Rodrigo O.},
  year = {2010},
  keywords = {Alzheimer’s disease,Connectome,cognition,mesoscale,quantum brain},
  pages = {15},
  file = {articles/Kuljiš2010.pdf}
}

@article{Klausberger2008,
  title = {Neuronal {{Diversity}} and {{Temporal Dynamics}}: {{The Unity}} of {{Hippocampal Circuit Operations}}},
  volume = {321},
  copyright = {American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  shorttitle = {Neuronal {{Diversity}} and {{Temporal Dynamics}}},
  doi = {10.1126/science.1149381},
  abstract = {In the cerebral cortex, diverse types of neurons form intricate circuits and cooperate in time for the processing and storage of information. Recent advances reveal a spatiotemporal division of labor in cortical circuits, as exemplified in the CA1 hippocampal area. In particular, distinct GABAergic ($\gamma$-aminobutyric acid\textendash{}releasing) cell types subdivide the surface of pyramidal cells and act in discrete time windows, either on the same or on different subcellular compartments. They also interact with glutamatergic pyramidal cell inputs in a domain-specific manner and support synaptic temporal dynamics, network oscillations, selection of cell assemblies, and the implementation of brain states. The spatiotemporal specializations in cortical circuits reveal that cellular diversity and temporal dynamics coemerged during evolution, providing a basis for cognitive behavior.},
  language = {en},
  number = {5885},
  journal = {Science},
  author = {Klausberger, Thomas and Somogyi, Peter},
  month = jul,
  year = {2008},
  pages = {53-57},
  file = {articles/Klausberger2008.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/HQ6M4WI6/53.html},
  pmid = {18599766}
}

@unpublished{Goette2009,
  title = {Analysis {{I}}-{{III}}},
  author = {Goette, Sebastian},
  year = {2009},
  file = {manuscripts/Goette2009_Analysis-I-III.pdf}
}

@techreport{zotero-13,
  title = {The {{Docker Book}}},
  file = {manuals/docker/the_docker_book.pdf},
  note = {manuals/docker}
}

@article{Shadlen2013,
  title = {Decision {{Making}} as a {{Window}} on {{Cognition}}},
  volume = {80},
  issn = {08966273},
  doi = {10.1016/j.neuron.2013.10.047},
  language = {en},
  number = {3},
  journal = {Neuron},
  author = {Shadlen, Michael N. and Kiani, Roozbeh},
  month = oct,
  year = {2013},
  pages = {791-806},
  file = {articles/Shadlen2013.pdf}
}

@article{Barbour2007,
  title = {What Can We Learn from Synaptic Weight Distributions?},
  volume = {30},
  issn = {01662236},
  doi = {10.1016/j.tins.2007.09.005},
  language = {en},
  number = {12},
  journal = {Trends in Neurosciences},
  author = {Barbour, Boris and Brunel, Nicolas and Hakim, Vincent and Nadal, Jean-Pierre},
  month = dec,
  year = {2007},
  pages = {622-629},
  file = {articles/Barbour2007.pdf}
}

@techreport{zotero-16,
  title = {Minted {{Source Code Highlighting}}},
  file = {manuals/latex/minted_source_code_highlighting.pdf},
  note = {manuals/latex}
}

@book{Hogg1978,
  address = {New York},
  edition = {4th ed},
  title = {Introduction to Mathematical Statistics},
  isbn = {978-0-02-355710-1},
  lccn = {QA276 .H59 1978},
  publisher = {{Macmillan}},
  author = {Hogg, Robert V. and Craig, Allen T.},
  year = {1978},
  keywords = {Mathematical statistics},
  file = {books/Hogg1978_Introduction-to-mathematical-statistics.pdf}
}

@misc{Hoffmann2014,
  title = {Structural and Dynamical Aspects of Neural Networks with Anisotropic Tissue Geometry},
  abstract = {Non-random connectivity has been repeatedly reported in cortical
networks, yet underlying connection principles of these patterns
remain elusive. Proposing an abstract geometric network model
reflecting stereotypical axonal and dendritic morphology of local
cortical layer 5 networks, we here investigate in how far anisotropy
in connectivity can constitute such an underlying connectivity
rule. Using a combination of analytical considerations and numerical
analysis, we find that while standard network measures and pair
connectivity remain unaffected, higher order connectivity is strongly
influenced by anisotropy, in many cases reflecting patterns found in
local cortical circuits. Presenting an abstract network model
featuring connectivity principles beyond distance-dependency, the
results shown here not only make a strong case for morphology-induced
rules as underlying connection principles of non-random patterns, but
may provide another step towards a network archetype greatly improving
upon the standard random model.},
  author = {Hoffmann, Felix},
  month = jun,
  year = {2014},
  file = {documents/Hoffmann2014.pdf}
}

@article{Miner2015,
  title = {[{{Preprint}}] {{Plasticity}}-{{Driven Self}}-{{Organization}} under {{Topological Constraints Accounts}} for {{Non}}-{{Random Features}} of {{Cortical Synaptic Wiring}}.},
  author = {Miner and Triesch},
  year = {2015},
  file = {articles/Miner2015.pdf}
}

@article{Dahlhaus1997,
  title = {Identification of Synaptic Connections in Neural Ensembles by Graphical Models},
  volume = {77},
  issn = {0165-0270},
  doi = {10.1016/S0165-0270(97)00100-3},
  abstract = {A method for the identification of direct synaptic connections in a larger neural net is presented. It is based on a conditional correlation graph for multivariate point processes. The connections are identified via the partial spectral coherence of two neurons, given all others. It is shown how these coherences can be calculated by inversion of the spectral density matrix. In simulations with GENESIS, we discuss the relevance of the method for identifying different neural ensembles including an excitatory feedback loop and networks with lateral inhibitions.},
  number = {1},
  journal = {Journal of Neuroscience Methods},
  author = {Dahlhaus, Rainer and Eichler, Michael and Sandk\"uhler, J\"urgen},
  month = nov,
  year = {1997},
  keywords = {Graphical models,Multivariate point processes,Neuronal net,Partial spectral coherence,Synaptic connectivity},
  pages = {93-107},
  file = {articles/Dahlhaus1997.pdf}
}

@book{Mesbahi2010,
  address = {Princeton},
  series = {Princeton series in applied mathematics},
  title = {Graph Theoretic Methods in Multiagent Networks},
  isbn = {978-0-691-14061-2},
  lccn = {T57.85 .M43 2010},
  publisher = {{Princeton University Press}},
  author = {Mesbahi, Mehran and Egerstedt, Magnus},
  year = {2010},
  keywords = {Graphic methods,Mathematical models,Multiagent systems,Network analysis (Planning)},
  file = {books/Mesbahi2010_Graph-theoretic-methods-in-multiagent-networks.pdf}
}

@book{Seung2013,
  address = {Boston},
  edition = {First Mariner Books edition},
  title = {Connectome: How the Brain's Wiring Makes Us Who We Are},
  isbn = {978-0-547-67859-7},
  lccn = {QP376 .S432 2013},
  shorttitle = {Connectome},
  publisher = {{Mariner Books, Houghton Mifflin Harcourt}},
  author = {Seung, Sebastian},
  year = {2013},
  file = {books/Seung2013_Connectome-how-the-brain's-wiring-makes-us-who-we-are.epub}
}

@article{Borst2014,
  title = {Fly Visual Course Control: Behaviour, Algorithms and Circuits},
  volume = {15},
  issn = {1471-003X, 1471-0048},
  shorttitle = {Fly Visual Course Control},
  doi = {10.1038/nrn3799},
  number = {9},
  journal = {Nature Reviews Neuroscience},
  author = {Borst, Alexander},
  month = aug,
  year = {2014},
  pages = {590-599},
  file = {articles/Borst2014.pdf}
}

@article{Branco2010,
  title = {Dendritic {{Discrimination}} of {{Temporal Input Sequences}} in {{Cortical Neurons}}},
  volume = {329},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1189664},
  abstract = {The detection and discrimination of temporal sequences is fundamental to brain function and underlies perception, cognition, and motor output. By applying patterned, two-photon glutamate uncaging, we found that single dendrites of cortical pyramidal neurons exhibit sensitivity to the sequence of synaptic activation. This sensitivity is encoded by both local dendritic calcium signals and somatic depolarization, leading to sequence-selective spike output. The mechanism involves dendritic impedance gradients and nonlinear synaptic N-methyl-d-aspartate receptor activation and is generalizable to dendrites in different neuronal types. This enables discrimination of patterns delivered to a single dendrite, as well as patterns distributed randomly across the dendritic tree. Pyramidal cell dendrites can thus act as processing compartments for the detection of synaptic sequences, thereby implementing a fundamental cortical computation.},
  language = {en},
  number = {5999},
  journal = {Science},
  author = {Branco, Tiago and Clark, Beverley A. and H\"ausser, Michael},
  month = sep,
  year = {2010},
  pages = {1671-1675},
  file = {articles/Branco2010.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/2INIKHAX/1671.html},
  pmid = {20705816}
}

@techreport{zotero-25,
  title = {{{TikZ}} - {{A}} Brief {{Introduction}}},
  file = {manuals/latex/tikz_-_a_brief_introduction.pdf},
  note = {manuals/latex}
}

@article{Yang2013,
  title = {Presynaptic Long-Term Plasticity},
  volume = {5},
  issn = {1663-3563},
  doi = {10.3389/fnsyn.2013.00008},
  abstract = {Long-term synaptic plasticity is a major cellular substrate for learning, memory, and behavioral adaptation. Although early examples of long-term synaptic plasticity described a mechanism by which postsynaptic signal transduction was potentiated, it is now apparent that there is a vast array of mechanisms for long-term synaptic plasticity that involve modifications to either or both the presynaptic terminal and postsynaptic site. In this article, we discuss current and evolving approaches to identify presynaptic mechanisms as well as discuss their limitations. We next provide examples of the diverse circuits in which presynaptic forms of long-term synaptic plasticity have been described and discuss the potential contribution this form of plasticity might add to circuit function. Finally, we examine the present evidence for the molecular pathways and cellular events underlying presynaptic long-term synaptic plasticity.},
  journal = {Frontiers in Synaptic Neuroscience},
  author = {Yang, Ying and Calakos, Nicole},
  month = oct,
  year = {2013},
  file = {articles/Yang2013.pdf},
  pmid = {24146648},
  pmcid = {PMC3797957}
}

@techreport{zotero-27,
  title = {Adjustbox {{Package}}},
  file = {manuals/latex/adjustbox_package.pdf},
  note = {manuals/latex}
}

@article{Hansel2013,
  title = {Short-{{Term Plasticity Explains Irregular Persistent Activity}} in {{Working Memory Tasks}}},
  volume = {33},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3455-12.2013},
  language = {en},
  number = {1},
  journal = {Journal of Neuroscience},
  author = {Hansel, D. and Mato, G.},
  month = jan,
  year = {2013},
  pages = {133-149},
  file = {articles/Hansel2013.pdf}
}

@article{Lefort2009,
  title = {The {{Excitatory Neuronal Network}} of the {{C2 Barrel Column}} in {{Mouse Primary Somatosensory Cortex}}},
  volume = {61},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2008.12.020},
  abstract = {Summary
Local microcircuits within neocortical columns form key determinants of sensory processing. Here, we investigate the excitatory synaptic neuronal network of an anatomically defined cortical column, the C2 barrel column of mouse primary somatosensory cortex. This cortical column is known to process tactile information related to the C2 whisker. Through multiple simultaneous whole-cell recordings, we quantify connectivity maps between individual excitatory neurons located across all cortical layers of the C2 barrel column. Synaptic connectivity depended strongly upon somatic laminar location of both presynaptic and postsynaptic neurons, providing definitive evidence for layer-specific signaling pathways. The strongest excitatory influence upon the cortical column was provided by presynaptic layer 4 neurons. In all layers we found rare large-amplitude synaptic connections, which are likely to contribute strongly to reliable information processing. Our data set provides the first functional description of the excitatory synaptic wiring diagram of a physiologically relevant and anatomically well-defined cortical column at single-cell resolution.},
  number = {2},
  journal = {Neuron},
  author = {Lefort, Sandrine and Tomm, Christian and Floyd Sarria, J. -C. and Petersen, Carl C. H.},
  month = jan,
  year = {2009},
  keywords = {SYSNEURO},
  pages = {301-316},
  file = {articles/Lefort2009.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/8ZBRIFJ8/S0896627308010921.html}
}

@article{Mainen1996,
  title = {Influence of Dendritic Structure on Firing Pattern in Model Neocortical Neurons},
  volume = {382},
  copyright = {\textcopyright{} 1996 Nature Publishing Group},
  doi = {10.1038/382363a0},
  language = {en},
  number = {6589},
  journal = {Nature},
  author = {Mainen, Zachary F. and Sejnowski, Terrence J.},
  month = jul,
  year = {1996},
  pages = {363-366},
  file = {articles/Mainen1996.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/64K6STJ5/382363a0.html}
}

@article{Britten1992,
  title = {The Analysis of Visual Motion: A Comparison of Neuronal and Psychophysical Performance},
  volume = {12},
  issn = {0270-6474, 1529-2401},
  shorttitle = {The Analysis of Visual Motion},
  abstract = {We compared the ability of psychophysical observers and single cortical neurons to discriminate weak motion signals in a stochastic visual display. All data were obtained from rhesus monkeys trained to perform a direction discrimination task near psychophysical threshold. The conditions for such a comparison were ideal in that both psychophysical and physiological data were obtained in the same animals, on the same sets of trials, and using the same visual display. In addition, the psychophysical task was tailored in each experiment to the physiological properties of the neuron under study; the visual display was matched to each neuron's preference for size, speed, and direction of motion. Under these conditions, the sensitivity of most MT neurons was very similar to the psychophysical sensitivity of the animal observers. In fact, the responses of single neurons typically provided a satisfactory account of both absolute psychophysical threshold and the shape of the psychometric function relating performance to the strength of the motion signal. Thus, psychophysical decisions in our task are likely to be based upon a relatively small number of neural signals. These signals could be carried by a small number of neurons if the responses of the pooled neurons are statistically independent. Alternatively, the signals may be carried by a much larger pool of neurons if their responses are partially intercorrelated.},
  language = {en},
  number = {12},
  journal = {The Journal of Neuroscience},
  author = {Britten, K. H. and Shadlen, M. N. and Newsome, W. T. and Movshon, J. A.},
  month = jan,
  year = {1992},
  pages = {4745-4765},
  file = {articles/Britten1992.pdf},
  pmid = {1464765}
}

@article{Park2013,
  title = {Structural and {{Functional Brain Networks}}: {{From Connections}} to {{Cognition}}},
  volume = {342},
  issn = {0036-8075, 1095-9203},
  shorttitle = {Structural and {{Functional Brain Networks}}},
  doi = {10.1126/science.1238411},
  abstract = {How rich functionality emerges from the invariant structural architecture of the brain remains a major mystery in neuroscience. Recent applications of network theory and theoretical neuroscience to large-scale brain networks have started to dissolve this mystery. Network analyses suggest that hierarchical modular brain networks are particularly suited to facilitate local (segregated) neuronal operations and the global integration of segregated functions. Although functional networks are constrained by structural connections, context-sensitive integration during cognition tasks necessarily entails a divergence between structural and functional networks. This degenerate (many-to-one) function-structure mapping is crucial for understanding the nature of brain networks. The emergence of dynamic functional networks from static structural connections calls for a formal (computational) approach to neuronal information processing that may resolve this dialectic between structure and function.
Background The human brain presents a puzzling and challenging paradox: Despite a fixed anatomy, characterized by its connectivity, its functional repertoire is vast, enabling action, perception, and cognition. This contrasts with organs like the heart that have a dynamic anatomy but just one function. The resolution of this paradox may reside in the brain's network architecture, which organizes local interactions to cope with diverse environmental demands\textemdash{}ensuring adaptability, robustness, resilience to damage, efficient message passing, and diverse functionality from a fixed structure. This review asks how recent advances in understanding brain networks elucidate the brain's many-to-one (degenerate) function-structure relationships. In other words, how does diverse function arise from an apparently static neuronal architecture? We conclude that the emergence of dynamic functional connectivity, from static structural connections, calls for formal (computational) approaches to neuronal information processing that may resolve the dialectic between structure and function.
Schematic of the multiscale hierarchical organization of brain networks. Brain function or cognition can be described as the global integration of local (segregated) neuronal operations that underlies hierarchical message passing among cortical areas, and which is facilitated by hierarchical modular network architectures.
Advances Much of our understanding of brain connectivity rests on the way that it is measured and modeled. We consider two complementary approaches: the first has its basis in graph theory that aims to describe the network topology of (undirected) connections of the sort measured by noninvasive brain imaging of anatomical connections and functional connectivity (correlations) between remote sites. This is compared with model-based definitions of context-sensitive (directed) effective connectivity that are grounded in the biophysics of neuronal interactions. Recent topological network analyses of brain circuits suggest that modular and hierarchical structural networks are particularly suited for the functional integration of local (functionally specialized) neuronal operations that underlie cognition. Measurements of spontaneous activity reveal functional connectivity patterns that are similar to structural connectivity, suggesting that structural networks constrain functional networks. However, task-related responses that require context-sensitive integration disclose a divergence between function and structure that appears to rest mainly on long-range connections. In contrast to methods that describe network topology phenomenologically, model-based theoretical and computational approaches focus on the mechanisms of neuronal interactions that accommodate the dynamic reconfiguration of effective connectivity. We highlight the consilience between hierarchical topologies (based on structural and functional connectivity) and the effective connectivity that would be required for hierarchical message passing of the sort suggested by computational neuroscience.
Outlook In summary, neuronal interactions represent dynamics on a fixed structural connectivity that underlie cognition and behavior. Such divergence of function from structure is, perhaps, the most intriguing property of the brain and invites intensive future research. By studying the dynamics and self-organization of functional networks, we may gain insight into the true nature of the brain as the embodiment of the mind. The repertoire of functional networks rests upon the (hidden) structural architecture of connections that enables hierarchical functional integration. Understanding these networks will require theoretical models of neuronal processing that underlies cognition.},
  language = {en},
  number = {6158},
  journal = {Science},
  author = {Park, Hae-Jeong and Friston, Karl},
  month = jan,
  year = {2013},
  pages = {1238411},
  file = {articles/Park2013.pdf},
  pmid = {24179229}
}

@book{Bollobas2001,
  address = {Cambridge ; New York},
  edition = {2nd edition},
  title = {Random {{Graphs}}},
  isbn = {978-0-521-80920-7},
  language = {English},
  publisher = {{Cambridge University Press}},
  author = {Bollob\'as, B\'ela},
  month = oct,
  year = {2001},
  file = {books/Bollobás2001_Random-Graphs.pdf}
}

@book{Klenke2006,
  address = {Berlin},
  edition = {Auflage: 1},
  title = {{Wahrscheinlichkeitstheorie}},
  isbn = {978-3-540-25545-1},
  abstract = {Dieses Lehrbuch bietet eine umfassende moderne Einf\"uhrung in die wichtigsten Gebiete der Wahrscheinlichkeitstheorie und ihre ma\ss{}theoretischen Grundlagen. Themenschwerpunkte sind u.a.: Ma\ss{}- und Integrationstheorie, Grenzwerts\"atze f\"ur Summen von Zufallsvariablen, Martingale oder Perkolation. \"Uber 200 \"Ubungsaufgaben und zahlreiche Abbildungen runden die Darstellung ab. Breite und Auswahl der Themen sind einmalig in der deutschsprachigen Literatur.},
  language = {Deutsch},
  publisher = {{Springer}},
  author = {Klenke, Achim},
  year = {2006},
  file = {books/Klenke2006_Wahrscheinlichkeitstheorie.pdf}
}

@article{Tannenbaum2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.03005},
  primaryClass = {q-bio},
  title = {Shaping Neural Circuits by High Order Synaptic Interactions},
  abstract = {Spike timing dependent plasticity (STDP) is believed to play an important role in shaping the structure of neural circuits. Here we show that STDP generates effective interactions between synapses of different neurons, which were neglected in previous theoretical treatments, and can be described as a sum over contributions from structural motifs. These interactions can have a pivotal influence on the connectivity patterns that emerge under the influence of STDP. In particular, we consider two highly ordered forms of structure: wide synfire chains, in which groups of neurons project to each other sequentially, and self connected assemblies. We show that high order synaptic interactions can enable the formation of both structures, depending on the form of the STDP function and the time course of synaptic currents. Furthermore, within a certain regime of biophysical parameters, emergence of the ordered connectivity occurs robustly and autonomously in a stochastic network of spiking neurons, without a need to expose the neural network to structured inputs during learning.},
  journal = {arXiv:1605.03005 [q-bio]},
  author = {Tannenbaum, Neta Ravid and Burak, Yoram},
  month = may,
  year = {2016},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {articles/Tannenbaum2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/25TVUBIS/1605.html}
}

@article{Butz2009,
  title = {Activity-Dependent Structural Plasticity},
  volume = {60},
  issn = {01650173},
  doi = {10.1016/j.brainresrev.2008.12.023},
  language = {en},
  number = {2},
  journal = {Brain Research Reviews},
  author = {Butz, Markus and W\"org\"otter, Florentin and {van Ooyen}, Arjen},
  month = may,
  year = {2009},
  pages = {287-305},
  file = {articles/Butz2009.pdf}
}

@article{Knoblauch2009,
  title = {Memory {{Capacities}} for {{Synaptic}} and {{Structural Plasticity}}},
  volume = {22},
  issn = {0899-7667},
  doi = {10.1162/neco.2009.08-07-588},
  abstract = {Neural associative networks with plastic synapses have been proposed as computational models of brain functions and also for applications such as pattern recognition and information retrieval. To guide biological models and optimize technical applications, several definitions of memory capacity have been used to measure the efficiency of associative memory. Here we explain why the currently used performance measures bias the comparison between models and cannot serve as a theoretical benchmark. We introduce fair measures for information-theoretic capacity in associative memory that also provide a theoretical benchmark.},
  number = {2},
  journal = {Neural Computation},
  author = {Knoblauch, Andreas and Palm, G\"unther and Sommer, Friedrich T.},
  month = nov,
  year = {2009},
  pages = {289-341},
  file = {articles/Knoblauch2009.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/QRIN68CX/neco.2009.html}
}

@article{Morrison2007,
  title = {Spike-{{Timing}}-{{Dependent Plasticity}} in {{Balanced Random Networks}}},
  volume = {19},
  issn = {0899-7667},
  doi = {10.1162/neco.2007.19.6.1437},
  abstract = {The balanced random network model attracts considerable interest because it explains the irregular spiking activity at low rates and large membrane potential fluctuations exhibited by cortical neurons in vivo. In this article, we investigate to what extent this model is also compatible with the experimentally observed phenomenon of spike-timing-dependent plasticity (STDP).},
  number = {6},
  journal = {Neural Computation},
  author = {Morrison, Abigail and Aertsen, Ad and Diesmann, Markus},
  month = apr,
  year = {2007},
  pages = {1437-1467},
  file = {articles/Morrison2007.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/AN5ZM2ER/neco.2007.19.6.html}
}

@book{VanOoyen2003,
  address = {Cambridge, Mass},
  series = {Developmental cognitive neuroscience},
  title = {Modeling Neural Development},
  isbn = {978-0-262-22066-8},
  lccn = {QP363.5 .M63 2003},
  publisher = {{MIT Press}},
  editor = {Van Ooyen, Arjen},
  year = {2003},
  keywords = {Developmental neurobiology,Methodology,Neural networks (Neurobiology)},
  file = {books/Van Ooyen2003_Modeling-neural-development.pdf}
}

@book{Diestel2000,
  address = {New York},
  edition = {2nd edition},
  title = {Graph {{Theory}}},
  isbn = {978-0-387-95014-3},
  language = {English},
  publisher = {{Springer}},
  author = {Diestel, Reinhard},
  month = feb,
  year = {2000},
  file = {books/Diestel2000_Graph-Theory.pdf}
}

@article{Roxin2011,
  title = {On the {{Distribution}} of {{Firing Rates}} in {{Networks}} of {{Cortical Neurons}}},
  volume = {31},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1677-11.2011},
  abstract = {The distribution of in vivo average firing rates within local cortical networks has been reported to be highly skewed and long tailed. The distribution of average single-cell inputs, conversely, is expected to be Gaussian by the central limit theorem. This raises the issue of how a skewed distribution of firing rates might result from a symmetric distribution of inputs. We argue that skewed rate distributions are a signature of the nonlinearity of the in vivo f\textendash{}I curve. During in vivo conditions, ongoing synaptic activity produces significant fluctuations in the membrane potential of neurons, resulting in an expansive nonlinearity of the f\textendash{}I curve for low and moderate inputs. Here, we investigate the effects of single-cell and network parameters on the shape of the f\textendash{}I curve and, by extension, on the distribution of firing rates in randomly connected networks.},
  language = {en},
  number = {45},
  journal = {The Journal of Neuroscience},
  author = {Roxin, Alex and Brunel, Nicolas and Hansel, David and Mongillo, Gianluigi and van Vreeswijk, Carl},
  month = sep,
  year = {2011},
  pages = {16217-16226},
  file = {articles/Roxin2011_2.pdf},
  pmid = {22072673}
}

@techreport{zotero-42,
  title = {Latexdiff},
  file = {manuals/latex/latexdiff.pdf},
  note = {manuals/latex}
}

@article{Hansel2012,
  title = {The {{Mechanism}} of {{Orientation Selectivity}} in {{Primary Visual Cortex}} without a {{Functional Map}}},
  volume = {32},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.6284-11.2012},
  abstract = {Neurons in primary visual cortex (V1) display substantial orientation selectivity even in species where V1 lacks an orientation map, such as in mice and rats. The mechanism underlying orientation selectivity in V1 with such a salt-and-pepper organization is unknown; it is unclear whether a connectivity that depends on feature similarity is required, or a random connectivity suffices. Here we argue for the latter. We study the response to a drifting grating of a network model of layer 2/3 with random recurrent connectivity and feedforward input from layer 4 neurons with random preferred orientations. We show that even though the total feedforward and total recurrent excitatory and inhibitory inputs all have a very weak orientation selectivity, strong selectivity emerges in the neuronal spike responses if the network operates in the balanced excitation/inhibition regime. This is because in this regime the (large) untuned components in the excitatory and inhibitory contributions approximately cancel. As a result the untuned part of the input into a neuron as well as its modulation with orientation and time all have a size comparable to the neuronal threshold. However, the tuning of the F0 and F1 components of the input are uncorrelated and the high-frequency fluctuations are not tuned. This is reflected in the subthreshold voltage response. Remarkably, due to the nonlinear voltage-firing rate transfer function, the preferred orientation of the F0 and F1 components of the spike response are highly correlated.},
  language = {en},
  number = {12},
  journal = {The Journal of Neuroscience},
  author = {Hansel, David and van Vreeswijk, Carl},
  month = mar,
  year = {2012},
  pages = {4049-4064},
  file = {articles/Hansel2012.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/RBNXD9AB/4049.html},
  pmid = {22442071}
}

@article{Fauth2016,
  title = {Opposing {{Effects}} of {{Neuronal Activity}} on {{Structural Plasticity}}},
  doi = {10.3389/fnana.2016.00075},
  abstract = {The connectivity of the brain is continuously adjusted to new environmental influences by several activity-dependent adaptive processes. The most investigated adaptive mechanism is activity-dependent functional or synaptic plasticity regulating the transmission efficacy of existing synapses. Another important but less prominently discussed adaptive process is structural plasticity, which changes the connectivity by the formation and deletion of synapses. In this review, we show, based on experimental evidence, that structural plasticity can be classified similar to synaptic plasticity into two categories: (i) Hebbian structural plasticity, which leads to an increase (decrease) of the number of synapses during phases of high (low) neuronal activity and (ii) homeostatic structural plasticity, which balances these changes by removing and adding synapses. Furthermore, based on experimental and theoretical insights, we argue that each type of structural plasticity fulfills a different function. While Hebbian structural changes enhance memory lifetime, storage capacity, and memory robustness, homeostatic structural plasticity self-organizes the connectivity of the neural network to assure stability. However, the link between functional synaptic and structural plasticity as well as the detailed interactions between Hebbian and homeostatic structural plasticity are more complex. This implies even richer dynamics requiring further experimental and theoretical investigations.},
  journal = {Frontiers in Neuroanatomy},
  author = {Fauth, Michael and Tetzlaff, Christian},
  year = {2016},
  keywords = {architectural plasticity,network topology,structural plasticity,synaptic plasticity,timescales},
  pages = {75},
  file = {articles/Fauth2016.pdf}
}

@book{Rieke1997,
  address = {Cambridge, Mass},
  series = {Computational neuroscience},
  title = {Spikes: Exploring the Neural Code},
  isbn = {978-0-262-18174-7},
  lccn = {QP364.5 .S66 1997},
  shorttitle = {Spikes},
  publisher = {{MIT Press}},
  editor = {Rieke, Fred},
  year = {1997},
  keywords = {Neural transmission,Sensory neurons,low_qual},
  file = {books/Rieke1997_Spikes-exploring-the-neural-code.pdf}
}

@book{Gabbiani2010,
  address = {Amsterdam},
  edition = {1. ed},
  title = {Mathematics for Neuroscientists},
  isbn = {978-0-12-374882-9},
  language = {eng},
  publisher = {{Elsevier, Acad. Press}},
  author = {Gabbiani, Fabrizio and Cox, Steven},
  year = {2010},
  keywords = {Neurosciences Mathematics},
  file = {books/Gabbiani2010_Mathematics-for-neuroscientists_2.pdf},
  note = {OCLC: 837352028}
}

@article{Mongillo2010,
  title = {Irregular {{Spiking}} and {{Multi}}-Stability in {{Recurrent Networks}} with {{Non}}-Linear {{Synaptic Transmission}}},
  author = {Mongillo, Gianluigi and {van Vreeswijk}, Carl and Hansel, David},
  year = {2010},
  file = {articles/Mongillo2010.pdf}
}

@article{vanPelt2013,
  title = {Estimating Neuronal Connectivity from Axonal and Dendritic Density Fields},
  volume = {7},
  issn = {1662-5188},
  doi = {10.3389/fncom.2013.00160},
  abstract = {Neurons innervate space by extending axonal and dendritic arborizations. When axons and dendrites come in close proximity of each other, synapses between neurons can be formed. Neurons vary greatly in their morphologies and synaptic connections with other neurons. The size and shape of the arborizations determine the way neurons innervate space. A neuron may therefore be characterized by the spatial distribution of its axonal and dendritic ``mass.'' A population mean ``mass'' density field of a particular neuron type can be obtained by averaging over the individual variations in neuron geometries. Connectivity in terms of candidate synaptic contacts between neurons can be determined directly on the basis of their arborizations but also indirectly on the basis of their density fields. To decide when a candidate synapse can be formed, we previously developed a criterion defining that axonal and dendritic line pieces should cross in 3D and have an orthogonal distance less than a threshold value. In this paper, we developed new methodology for applying this criterion to density fields. We show that estimates of the number of contacts between neuron pairs calculated from their density fields are fully consistent with the number of contacts calculated from the actual arborizations. However, the estimation of the connection probability and the expected number of contacts per connection cannot be calculated directly from density fields, because density fields do not carry anymore the correlative structure in the spatial distribution of synaptic contacts. Alternatively, these two connectivity measures can be estimated from the expected number of contacts by using empirical mapping functions. The neurons used for the validation studies were generated by our neuron simulator NETMORPH. An example is given of the estimation of average connectivity and Euclidean pre- and postsynaptic distance distributions in a network of neurons represented by their population mean density fields.},
  journal = {Frontiers in Computational Neuroscience},
  author = {{van Pelt}, Jaap and {van Ooyen}, Arjen},
  month = nov,
  year = {2013},
  file = {articles/van Pelt2013.pdf},
  pmid = {24324430},
  pmcid = {PMC3839411}
}

@article{Hoffmann2016a,
  title = {Non-Random Network Connectivity Comes in Pairs},
  doi = {10.12751/nncn.bc2016.0079},
  author = {Hoffmann, Felix Z. and Triesch, Jochen},
  year = {2016},
  keywords = {me,poster},
  file = {articles/Hoffmann_BCCN2016.pdf},
  note = {poster=BCCN}
}

@article{Okun2009,
  title = {Balance of Excitation and Inhibition},
  volume = {4},
  issn = {1941-6016},
  doi = {10.4249/scholarpedia.7467},
  language = {en},
  number = {8},
  journal = {Scholarpedia},
  author = {Okun, Michael and Lampl, Ilan},
  year = {2009},
  pages = {7467},
  file = {articles/Okun2009.pdf}
}

@article{Kampa2006,
  title = {Cortical Feed-Forward Networks for Binding Different Streams of Sensory Information},
  volume = {9},
  copyright = {\textcopyright{} 2006 Nature Publishing Group},
  issn = {1097-6256},
  doi = {10.1038/nn1798},
  abstract = {Different streams of sensory information are transmitted to the cortex where they are merged into a percept in a process often termed 'binding.' Using recordings from triplets of rat cortical layer 2/3 and layer 5 pyramidal neurons, we show that specific subnetworks within layer 5 receive input from different layer 2/3 subnetworks. This cortical microarchitecture may represent a mechanism that enables the main output of the cortex (layer 5) to bind different features of a sensory stimulus.},
  language = {en},
  number = {12},
  journal = {Nature Neuroscience},
  author = {Kampa, Bj\"orn M. and Letzkus, Johannes J. and Stuart, Greg J.},
  month = dec,
  year = {2006},
  pages = {1472-1473},
  file = {articles/Kampa2006.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/4D22ZBS3/nn1798.html}
}

@article{Butz2014,
  title = {Homeostatic Structural Plasticity Increases the Efficiency of Small-World Networks},
  volume = {6},
  issn = {1663-3563},
  doi = {10.3389/fnsyn.2014.00007},
  abstract = {In networks with small-world topology, which are characterized by a high clustering coefficient and a short characteristic path length, information can be transmitted efficiently and at relatively low costs. The brain is composed of small-world networks, and evolution may have optimized brain connectivity for efficient information processing. Despite many studies on the impact of topology on information processing in neuronal networks, little is known about the development of network topology and the emergence of efficient small-world networks. We investigated how a simple growth process that favors short-range connections over long-range connections in combination with a synapse formation rule that generates homeostasis in post-synaptic firing rates shapes neuronal network topology. Interestingly, we found that small-world networks benefited from homeostasis by an increase in efficiency, defined as the averaged inverse of the shortest paths through the network. Efficiency particularly increased as small-world networks approached the desired level of electrical activity. Ultimately, homeostatic small-world networks became almost as efficient as random networks. The increase in efficiency was caused by the emergent property of the homeostatic growth process that neurons started forming more long-range connections, albeit at a low rate, when their electrical activity was close to the homeostatic set-point. Although global network topology continued to change when neuronal activities were around the homeostatic equilibrium, the small-world property of the network was maintained over the entire course of development. Our results may help understand how complex systems such as the brain could set up an efficient network topology in a self-organizing manner. Insights from our work may also lead to novel techniques for constructing large-scale neuronal networks by self-organization.},
  journal = {Frontiers in Synaptic Neuroscience},
  author = {Butz, Markus and Steenbuck, Ines D. and {van Ooyen}, Arjen},
  month = apr,
  year = {2014},
  file = {articles/Butz2014.pdf},
  pmid = {24744727},
  pmcid = {PMC3978244}
}

@unpublished{Hoffmann2009f,
  title = {Wahrscheinlichkeitstheorie},
  author = {Hoffmann, Felix},
  year = {2009},
  file = {manuscripts/Hoffmann2009_Wahrscheinlichkeitstheorie.pdf}
}

@article{Hellwig1994,
  title = {Synapses on Axon Collaterals of Pyramidal Cells Are Spaced at Random Intervals: A {{Golgi}} Study in the Mouse Cerebral Cortex},
  volume = {71},
  issn = {0340-1200, 1432-0770},
  shorttitle = {Synapses on Axon Collaterals of Pyramidal Cells Are Spaced at Random Intervals},
  doi = {10.1007/BF00198906},
  abstract = {In this study we investigated the arrangement of synapses on local axon collaterals of Golgi-stained pyramidal neurons in the mouse cerebral cortex. As synaptic markers we considered axonal swellings visible at high magnification under the light microscope. Such axonal swellings coincide with synaptic boutons, as has been demonstrated in a number of combined light and electron microscopic studies. These studies also indicated that, in most cases, one bouton corresponds precisely to one synapse. Golgi-impregnated axonal trees of 20 neocortical pyramidal neurons were drawn with a camera lucida. Axonal swellings were marked on the drawings. Most swellings were `en passant'; occasionally, they were situated at the tip of short, spine-like processes. On axon collaterals, the average interval between swellings was 4.5 $\mu$m. On the axonal main stem, the swellings were always less densely packed than on the collaterals. Statistical analysis of the spatial distribution of the swellings did not reveal any special patterns. Instead, the arrangement of swellings on individual collaterals follows a Poisson distribution. Moreover, the same holds to a large extent for the entire collection of pyramidal cell collaterals. This suggests that a single Poisson process, characterized by only one rate parameter (number of synapses per unit length), describes most of the spatial distribution of synapses along pyramidal cell collaterals. These findings do not speak in favour of a pronounced target specificity of pyramidal neurons at the synaptic level. Instead, our results support a probabilistic model of cortical connectivity.},
  language = {en},
  number = {1},
  journal = {Biological Cybernetics},
  author = {Hellwig, Bernhard and Sch\"uz, Almut and Aertsen, Ad},
  month = may,
  year = {1994},
  keywords = {Bioinformatics,Computer Appl. in Life Sciences,Neurobiology},
  pages = {1-12},
  file = {articles/Hellwig1994.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/75HB8WU3/BF00198906.html}
}

@article{Erdos1959,
  title = {On Random Graphs, {{I}}},
  volume = {6},
  journal = {Publicationes Mathematicae (Debrecen)},
  author = {Erd{\H o}s, P. and R\'enyi, A.},
  year = {1959},
  keywords = {graphs,random},
  pages = {290-297},
  file = {articles/Erd˝os1959.pdf}
}

@article{Nudo2013,
  title = {Recovery after Brain Injury: Mechanisms and Principles},
  volume = {7},
  issn = {1662-5161},
  shorttitle = {Recovery after Brain Injury},
  doi = {10.3389/fnhum.2013.00887},
  journal = {Frontiers in Human Neuroscience},
  author = {Nudo, Randolph J.},
  year = {2013},
  file = {articles/Nudo2013.pdf}
}

@article{Hawkes1971a,
  title = {Spectra of Some Self-Exciting and Mutually Exciting Point Processes},
  volume = {58},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/58.1.83},
  abstract = {SUMMARY In recent years methods of data analysis for point processes have received some attention, for example, by Cox \& Lewis (1966) and Lewis (1964). In particular Bartlett (1963a, b) has introduced methods of analysis based on the point spectrum. Theoretical models are relatively sparse. In this paper the theoretical properties of a class of processes with particular reference to the point spectrum or corresponding covariance density functions are discussed. A particular result is a self-exciting process with the same second-order properties as a certain doubly stochastic process. These are not distinguishable by methods of data analysis based on these properties.},
  language = {en},
  number = {1},
  journal = {Biometrika},
  author = {Hawkes, Alan G.},
  month = jan,
  year = {1971},
  keywords = {Covariance density,Point process,Self-exciting point process,Spectrum of point process},
  pages = {83-90},
  file = {articles/Hawkes1971.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/MCS8DJQM/83.html}
}

@book{Mardia2000,
  address = {Chichester ; New York},
  series = {Wiley series in probability and statistics},
  title = {Directional Statistics},
  isbn = {978-0-471-95333-3},
  lccn = {QA276 .J864 2000},
  publisher = {{J. Wiley}},
  author = {Mardia, K. V. and Jupp, Peter E.},
  year = {2000},
  keywords = {Distribution (Probability theory),Mathematical statistics,Sampling (Statistics)},
  file = {books/Mardia2000_Directional-statistics2.pdf}
}

@unpublished{Hoffmann2009e,
  title = {Ringe},
  author = {Hoffmann, Felix},
  year = {2009},
  file = {manuscripts/Hoffmann2009_Ringe.pdf}
}

@article{Regehr2012,
  title = {Short-{{Term Presynaptic Plasticity}}},
  volume = {4},
  issn = {, 1943-0264},
  doi = {10.1101/cshperspect.a005702},
  abstract = {Different types of synapses are specialized to interpret spike trains in their own way by virtue of the complement of short-term synaptic plasticity mechanisms they possess. Numerous types of short-term, use-dependent synaptic plasticity regulate neurotransmitter release. Short-term depression is prominent after a single conditioning stimulus and recovers in seconds. Sustained presynaptic activation can result in more profound depression that recovers more slowly. An enhancement of release known as facilitation is prominent after single conditioning stimuli and lasts for hundreds of milliseconds. Finally, tetanic activation can enhance synaptic strength for tens of seconds to minutes through processes known as augmentation and posttetantic potentiation. Progress in clarifying the properties, mechanisms, and functional roles of these forms of short-term plasticity is reviewed here.},
  language = {en},
  number = {7},
  journal = {Cold Spring Harbor Perspectives in Biology},
  author = {Regehr, Wade G.},
  month = jan,
  year = {2012},
  pages = {a005702},
  file = {articles/Regehr2012.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/FM3RNA9F/a005702.html},
  pmid = {22751149}
}

@article{Vogels2011,
  title = {Inhibitory {{Plasticity Balances Excitation}} and {{Inhibition}} in {{Sensory Pathways}} and {{Memory Networks}}},
  volume = {334},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1211095},
  abstract = {Cortical neurons receive balanced excitatory and inhibitory synaptic currents. Such a balance could be established and maintained in an experience-dependent manner by synaptic plasticity at inhibitory synapses. We show that this mechanism provides an explanation for the sparse firing patterns observed in response to natural stimuli and fits well with a recently observed interaction of excitatory and inhibitory receptive field plasticity. The introduction of inhibitory plasticity in suitable recurrent networks provides a homeostatic mechanism that leads to asynchronous irregular network states. Further, it can accommodate synaptic memories with activity patterns that become indiscernible from the background state but can be reactivated by external stimuli. Our results suggest an essential role of inhibitory plasticity in the formation and maintenance of functional cortical circuitry.},
  language = {en},
  number = {6062},
  journal = {Science},
  author = {Vogels, T. P. and Sprekeler, H. and Zenke, F. and Clopath, C. and Gerstner, W.},
  month = dec,
  year = {2011},
  pages = {1569-1573},
  file = {articles/Vogels2011.pdf},
  pmid = {22075724}
}

@book{Hastie2009,
  address = {New York, NY},
  edition = {2nd ed},
  series = {Springer series in statistics},
  title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  isbn = {978-0-387-84857-0 978-0-387-84858-7},
  lccn = {Q325.5 .H39 2009},
  shorttitle = {The Elements of Statistical Learning},
  publisher = {{Springer}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.},
  year = {2009},
  keywords = {Bioinformatics,Computational intelligence,Data mining,Forecasting,Inference,Machine learning,Methodology,Statistics},
  file = {books/Hastie2009_The-elements-of-statistical-learning-data-mining,-inference,-and-prediction.pdf}
}

@unpublished{Huber-Klawitter2010,
  title = {Kommutative {{Algebra}} Und {{Einf\"uhrung}} in Die Algebraische {{Geometrie}} (Mit {{Notizen}})},
  author = {{Huber-Klawitter} and Hoffmann, Felix},
  year = {2010},
  file = {manuscripts/Huber-Klawitter2010_Kommutative-Algebra-und-Einführung-in-die-algebraische-Geometrie-(mit-Notizen).pdf}
}

@article{Platschek2016,
  title = {A General Homeostatic Principle Following Lesion Induced Dendritic Remodeling},
  volume = {4},
  issn = {2051-5960},
  doi = {10.1186/s40478-016-0285-8},
  abstract = {Neuronal death and subsequent denervation of target areas are hallmarks of many neurological disorders. Denervated neurons lose part of their dendritic tree, and are considered "atrophic", i.e. pathologically altered and damaged. The functional consequences of this phenomenon are poorly understood.},
  journal = {Acta Neuropathologica Communications},
  author = {Platschek, Steffen and Cuntz, Hermann and Vuksic, Mario and Deller, Thomas and Jedlicka, Peter},
  year = {2016},
  keywords = {Backpropagating action potential,Compartmental modeling,Computer Simulation,Electrotonic analysis,Granule cell,Homeostatic plasticity,Morphological modeling,Voltage attenuation},
  pages = {19},
  file = {articles/Platschek2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/2J8NJJ2T/s40478-016-0285-8.html}
}

@article{Rochefort2011,
  title = {Development of {{Direction Selectivity}} in {{Mouse Cortical Neurons}}},
  volume = {71},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2011.06.013},
  abstract = {Summary
Previous studies of the ferret visual cortex indicate that the development of direction selectivity requires visual experience. Here, we used two-photon calcium imaging to study the development of direction selectivity in layer 2/3 neurons of the mouse visual cortex in~vivo. Surprisingly, just after eye opening nearly all orientation-selective neurons were also direction selective. During later development, the number of neurons responding to drifting gratings increased in parallel with the fraction of neurons that were orientation, but not direction, selective. Our experiments demonstrate that direction selectivity develops normally in dark-reared mice, indicating that the early development of direction selectivity is independent of visual experience. Furthermore, remarkable functional similarities exist between the development of direction selectivity in cortical neurons and the previously reported development of direction selectivity in the mouse retina. Together, these findings provide strong evidence that the development of orientation and direction selectivity in the mouse brain is distinctly different from that in ferrets.},
  number = {3},
  journal = {Neuron},
  author = {Rochefort, Nathalie L. and Narushima, Madoka and Grienberger, Christine and Marandi, Nima and Hill, Daniel N. and Konnerth, Arthur},
  month = aug,
  year = {2011},
  pages = {425-432},
  file = {articles/Rochefort2011.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/FTHZG2PV/S0896627311005186.html}
}

@article{Brown2009,
  title = {Intracortical Circuits of Pyramidal Neurons Reflect Their Long-Range Axonal Targets},
  volume = {457},
  copyright = {\textcopyright{} 2009 Nature Publishing Group},
  issn = {0028-0836},
  doi = {10.1038/nature07658},
  abstract = {Cortical columns generate separate streams of information that are distributed to numerous cortical and subcortical brain regions. We asked whether local intracortical circuits reflect these different processing streams by testing whether the intracortical connectivity among pyramidal neurons reflects their long-range axonal targets. We recorded simultaneously from up to four retrogradely labelled pyramidal neurons that projected to the superior colliculus, the contralateral striatum or the contralateral cortex to assess their synaptic connectivity. Here we show that the probability of synaptic connection depends on the functional identities of both the presynaptic and postsynaptic neurons. We first found that the frequency of monosynaptic connections among corticostriatal pyramidal neurons is significantly higher than among corticocortical or corticotectal pyramidal neurons. We then show that the probability of feed-forward connections from corticocortical neurons to corticotectal neurons is approximately three- to fourfold higher than the probability of monosynaptic connections among corticocortical or corticotectal cells. Moreover, we found that the average axodendritic overlap of the presynaptic and postsynaptic pyramidal neurons could not fully explain the differences in connection probability that we observed. The selective synaptic interactions we describe demonstrate that the organization of local networks of pyramidal cells reflects the long-range targets of both the presynaptic and postsynaptic neurons.},
  language = {en},
  number = {7233},
  journal = {Nature},
  author = {Brown, Solange P. and Hestrin, Shaul},
  month = feb,
  year = {2009},
  pages = {1133-1136},
  file = {articles/Brown2009.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/ZMNVUE9S/nature07658.html}
}

@article{Hodgkin1952,
  title = {A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve},
  volume = {117},
  issn = {1469-7793},
  doi = {10.1113/jphysiol.1952.sp004764},
  language = {en},
  number = {4},
  journal = {The Journal of Physiology},
  author = {Hodgkin, A. L. and Huxley, A. F.},
  month = aug,
  year = {1952},
  pages = {500-544},
  file = {articles/Hodgkin1952.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/4MZ6B758/abstract.html}
}

@book{Meister2015,
  address = {Wiesbaden},
  edition = {5., \"uberarb. Aufl},
  series = {Lehrbuch},
  title = {{Numerik linearer Gleichungssysteme: eine Einf\"uhrung in moderne Verfahren ; mit MATLAB-Implementierungen von C. V\"omel}},
  isbn = {978-3-658-07200-1 978-3-658-07199-8},
  shorttitle = {{Numerik linearer Gleichungssysteme}},
  language = {ger},
  publisher = {{Springer Spektrum}},
  author = {Meister, Andreas},
  year = {2015},
  keywords = {Lineares Gleichungssystem,Numerisches Verfahren},
  file = {articles/Book/Meister20152.pdf;books/Meister2015_Numerik-linearer-Gleichungssysteme-eine-Einführung-in-moderne-Verfahren-\;-mit-MATLAB-Implementierungen-von-C.-Vömel.pdf}
}

@phdthesis{Gjorgjieva2011,
  title = {Spontaneous Activity and Plasticity in the Developing Nervous System},
  author = {Gjorgjieva, Julijana},
  year = {2011},
  file = {thesis/Gjorgjieva2011.pdf}
}

@article{Gollo2014,
  title = {Mechanisms of {{Zero}}-{{Lag Synchronization}} in {{Cortical Motifs}}},
  volume = {10},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003548},
  abstract = {Author Summary   Understanding large-scale neuronal dynamics \textendash{} and how they relate to the cortical anatomy \textendash{} is one of the key areas of neuroscience research. Despite a wealth of recent research, the key principles of this relationship have yet to be established. Here we employ computational modeling to study neuronal dynamics on small subgraphs \textendash{} or motifs \textendash{} across a hierarchy of spatial scales. We establish a novel organizing principle that we term a ``resonance pair'' (two mutually coupled nodes), which promotes stable, zero-lag synchrony amongst motif nodes. The bidirectional coupling between a resonance pair acts to mutually adjust their dynamics onto a common and relatively stable synchronized regime, which then propagates and stabilizes the synchronization of other nodes within the motif. Remarkably, we find that this effect can propagate along chains of coupled nodes and hence holds the potential to promote stable zero-lag synchrony in larger sub-networks of cortical systems. Our findings hence suggest a potential unifying account of the existence of zero-lag synchrony, an important phenomenon that may underlie crucial cognitive processes in the brain. Moreover, such pairs of mutually coupled oscillators are found in a wide variety of physical and biological systems suggesting a new, broadly relevant and unifying principle.},
  number = {4},
  journal = {PLOS Comput Biol},
  author = {Gollo, Leonardo L. and Mirasso, Claudio and Sporns, Olaf and Breakspear, Michael},
  month = apr,
  year = {2014},
  keywords = {Action potentials,Dynamical systems,Membrane potential,Network motifs,Neurons,Single neuron function,Synapses,neural networks},
  pages = {e1003548},
  file = {articles/Gollo2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/F3GUIIHF/article.html}
}

@article{Bi1998,
  title = {Synaptic {{Modifications}} in {{Cultured Hippocampal Neurons}}: {{Dependence}} on {{Spike Timing}}, {{Synaptic Strength}}, and {{Postsynaptic Cell Type}}},
  volume = {18},
  issn = {0270-6474, 1529-2401},
  shorttitle = {Synaptic {{Modifications}} in {{Cultured Hippocampal Neurons}}},
  abstract = {In cultures of dissociated rat hippocampal neurons, persistent potentiation and depression of glutamatergic synapses were induced by correlated spiking of presynaptic and postsynaptic neurons. The relative timing between the presynaptic and postsynaptic spiking determined the direction and the extent of synaptic changes. Repetitive postsynaptic spiking within a time window of 20 msec after presynaptic activation resulted in long-term potentiation (LTP), whereas postsynaptic spiking within a window of 20 msec before the repetitive presynaptic activation led to long-term depression (LTD). Significant LTP occurred only at synapses with relatively low initial strength, whereas the extent of LTD did not show obvious dependence on the initial synaptic strength. Both LTP and LTD depended on the activation of NMDA receptors and were absent in cases in which the postsynaptic neurons were GABAergic in nature. Blockade of L-type calcium channels with nimodipine abolished the induction of LTD and reduced the extent of LTP. These results underscore the importance of precise spike timing, synaptic strength, and postsynaptic cell type in the activity-induced modification of central synapses and suggest that Hebb's rule may need to incorporate a quantitative consideration of spike timing that reflects the narrow and asymmetric window for the induction of synaptic modification.},
  language = {en},
  number = {24},
  journal = {The Journal of Neuroscience},
  author = {Bi, Guo-qiang and Poo, Mu-ming},
  month = dec,
  year = {1998},
  keywords = {Hebbian,Hebb’s rule,LTD,LTP,cell culture,correlated-activity,hippocampal neurons,plasticity,spike timing,spiking,synaptic modification,target specificity},
  pages = {10464-10472},
  file = {articles/Bi1998.pdf},
  pmid = {9852584}
}

@book{Filo2010,
  address = {Hoboken, N.J},
  title = {Information Processing by Biochemical Systems: Neural Network-Type Configurations},
  isbn = {978-0-470-50094-1},
  lccn = {QA76.884 .F55 2010},
  shorttitle = {Information Processing by Biochemical Systems},
  publisher = {{John Wiley \& Sons}},
  author = {Filo, Orna and Lotan, Noah},
  year = {2010},
  keywords = {Automatic Data Processing,Biochemical Phenomena,Biocomputers,Information technology,Neural Networks (Computer),Neural networks (Computer science)},
  file = {books/Filo2010_Information-processing-by-biochemical-systems-neural-network-type-configurations.pdf}
}

@article{Petz1986,
  title = {On the Equality in {{Jensen}}'s Inequality for Operator Convex Functions},
  volume = {9},
  issn = {0378-620X, 1420-8989},
  doi = {10.1007/BF01195811},
  abstract = {Let A and B be C*-algebras with unit and assume that $\phi{}:$A$\rightarrow$B is a positive unit preserving linear mapping. Choi proved thatf($\Phi$(a))$\leqq\Phi$(f(a))f($\backslash$Phi (a)) $\backslash$leqq $\backslash$Phi (f(a)) if a=a*$\in$A and Sp(a)$\subset$($\alpha$, $\beta$) for every operator convex function f: ($\alpha$, $\beta$) $\rightarrow$ $\mathbb{R}$. We prove that the equality holds if and only if $\phi$ restricted to the subalgebra generated by \{a\} is multiplicative. An example is shown as an application.},
  language = {en},
  number = {5},
  journal = {Integral Equations and Operator Theory},
  author = {Petz, D\'enes},
  month = sep,
  year = {1986},
  keywords = {Analysis},
  pages = {744-747},
  file = {articles/Petz1986.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/VVTVF936/10.html}
}

@article{Hennequin2014,
  title = {Optimal {{Control}} of {{Transient Dynamics}} in {{Balanced Networks Supports Generation}} of {{Complex Movements}}},
  volume = {82},
  issn = {08966273},
  doi = {10.1016/j.neuron.2014.04.045},
  language = {en},
  number = {6},
  journal = {Neuron},
  author = {Hennequin, Guillaume and Vogels, Tim P. and Gerstner, Wulfram},
  month = jun,
  year = {2014},
  pages = {1394-1406},
  file = {articles/Hennequin2014.pdf}
}

@article{Haider2006,
  title = {Neocortical {{Network Activity In Vivo Is Generated}} through a {{Dynamic Balance}} of {{Excitation}} and {{Inhibition}}},
  volume = {26},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5297-05.2006},
  language = {en},
  number = {17},
  journal = {Journal of Neuroscience},
  author = {Haider, B.},
  month = apr,
  year = {2006},
  pages = {4535-4545},
  file = {articles/Haider2006.pdf}
}

@book{Cover2006,
  address = {Hoboken, N.J},
  edition = {2 edition},
  title = {Elements of {{Information Theory}} 2nd {{Edition}}},
  isbn = {978-0-471-24195-9},
  abstract = {The latest edition of this classic is updated with new problem sets and material  The Second Edition of this fundamental textbook maintains the book's tradition of clear, thought-provoking instruction. Readers are provided once again with an instructive mix of mathematics, physics, statistics, and information theory.  All the essential topics in information theory are covered in detail, including entropy, data compression, channel capacity, rate distortion, network information theory, and hypothesis testing. The authors provide readers with a solid understanding of the underlying theory and applications. Problem sets and a telegraphic summary at the end of each chapter further assist readers. The historical notes that follow each chapter recap the main points.  The Second Edition features: * Chapters reorganized to improve teaching * 200 new problems * New material on source coding, portfolio theory, and feedback capacity * Updated references  Now current and enhanced, the Second Edition of Elements of Information Theory remains the ideal textbook for upper-level undergraduate and graduate courses in electrical engineering, statistics, and telecommunications.  An Instructor's Manual presenting detailed solutions to all the problems in the book is available from the Wiley editorial department.},
  language = {English},
  publisher = {{Wiley-Interscience}},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  month = jul,
  year = {2006},
  file = {books/Cover2006_Elements-of-Information-Theory-2nd-Edition.pdf;books/Cover2006_solutions_to_exercises.pdf}
}

@book{Kampen2008,
  address = {Amsterdam},
  edition = {3. ed., repr},
  series = {North-Holland personal library},
  title = {Stochastic Processes in Physics and Chemistry},
  isbn = {978-0-444-52965-7},
  language = {eng},
  publisher = {{Elsevier}},
  author = {van Kampen, Nicolaas Godfried},
  year = {2008},
  keywords = {Chemie,Chemistry; Physical and theoretical Statistical methods,Fluktuation,Lehrbuch,Physik,Statistical physics,Stochastic processes,Stochastischer Prozess},
  file = {books/Kampen2008_Stochastic-processes-in-physics-and-chemistry.djvu},
  note = {OCLC: 315990030}
}

@article{Sachdev2012,
  title = {Surround Suppression and Sparse Coding in Visual and Barrel Cortices},
  volume = {6},
  issn = {1662-5110},
  doi = {10.3389/fncir.2012.00043},
  abstract = {During natural vision the entire retina is stimulated. Likewise, during natural tactile behaviors, spatially extensive regions of the somatosensory surface are co-activated. The large spatial extent of naturalistic stimulation means that surround suppression, a phenomenon whose neural mechanisms remain a matter of debate, must arise during natural behavior. To identify common neural motifs that might instantiate surround suppression across modalities, we review models of surround suppression and compare the evidence supporting the competing ideas that surround suppression has either cortical or sub-cortical origins in visual and barrel cortex. In the visual system there is general agreement lateral inhibitory mechanisms contribute to surround suppression, but little direct experimental evidence that intracortical inhibition plays a major role. Two intracellular recording studies of V1, one using naturalistic stimuli (Haider et al., ), the other sinusoidal gratings (Ozeki et al., ), sought to identify the causes of reduced activity in V1 with increasing stimulus size, a hallmark of surround suppression. The former attributed this effect to increased inhibition, the latter to largely balanced withdrawal of excitation and inhibition. In rodent primary somatosensory barrel cortex, multi-whisker responses are generally weaker than single whisker responses, suggesting multi-whisker stimulation engages similar surround suppressive mechanisms. The origins of suppression in S1 remain elusive: studies have implicated brainstem lateral/internuclear interactions and both thalamic and cortical inhibition. Although the anatomical organization and instantiation of surround suppression in the visual and somatosensory systems differ, we consider the idea that one common function of surround suppression, in both modalities, is to remove the statistical redundancies associated with natural stimuli by increasing the sparseness or selectivity of sensory responses.},
  journal = {Frontiers in Neural Circuits},
  author = {Sachdev, Robert N. S. and Krause, Matthew R. and Mazer, James A.},
  month = jul,
  year = {2012},
  file = {articles/Sachdev2012.pdf},
  pmid = {22783169},
  pmcid = {PMC3389675}
}

@article{Krieg2014,
  title = {A Unifying Theory of Synaptic Long-Term Plasticity Based on a Sparse Distribution of Synaptic Strength},
  volume = {6},
  doi = {10.3389/fnsyn.2014.00003},
  abstract = {Long-term synaptic plasticity is fundamental to learning and network function. It has been studied under various induction protocols and depends on firing rates, membrane voltage, and precise timing of action potentials. These protocols show different facets of a common underlying mechanism but they are mostly modeled as distinct phenomena. Here, we show that all of these different dependencies can be explained from a single computational principle. The objective is a sparse distribution of excitatory synaptic strength, which may help to reduce metabolic costs associated with synaptic transmission. Based on this objective we derive a stochastic gradient ascent learning rule which is of differential-Hebbian type. It is formulated in biophysical quantities and can be related to current mechanistic theories of synaptic plasticity. The learning rule accounts for experimental findings from all major induction protocols and explains a classic phenomenon of metaplasticity. Furthermore, our model predicts the existence of metaplasticity for spike-timing-dependent plasticity Thus, we provide a theory of long-term synaptic plasticity that unifies different induction protocols and provides a connection between functional and mechanistic levels of description.},
  journal = {Frontiers in Synaptic Neuroscience},
  author = {Krieg, Daniel and Triesch, Jochen},
  year = {2014},
  keywords = {STDP,computational,metaplasticity,sparseness,synaptic plasticity},
  pages = {3},
  file = {articles/Krieg2014.pdf}
}

@book{Amthor2012,
  address = {Mississauga},
  series = {for dummies},
  title = {Neuroscience for Dummies: [Making Everything Easier!]},
  isbn = {978-1-118-08968-2 978-1-118-08686-5 978-1-118-08967-5},
  shorttitle = {Neuroscience for Dummies},
  abstract = {Neuroscience for Dummies gives the reader an understanding of the brain's structure and function, as well as a look into the relationship between memory, learning, emotions, and the brain. Providing insight into the biology of mental illness and a glimpse at future treatments and applications of neuroscience--},
  language = {eng},
  publisher = {{Wiley}},
  author = {Amthor, Frank},
  year = {2012},
  keywords = {Brain,Neurosciences,Neurosciences--Popular works,Neurowissenschaften},
  file = {books/Amthor2012_Neuroscience-for-dummies-[making-everything-easier!]2.pdf}
}

@article{Deger2012a,
  title = {Spike-{{Timing Dependence}} of {{Structural Plasticity Explains Cooperative Synapse Formation}} in the {{Neocortex}}},
  volume = {8},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002689},
  language = {en},
  number = {9},
  journal = {PLoS Computational Biology},
  author = {Deger, Moritz and Helias, Moritz and Rotter, Stefan and Diesmann, Markus},
  editor = {Sporns, Olaf},
  month = sep,
  year = {2012},
  pages = {e1002689},
  file = {articles/Deger2012.pdf}
}

@article{Triesch2007,
  title = {Synergies {{Between Intrinsic}} and {{Synaptic Plasticity Mechanisms}}},
  volume = {19},
  issn = {0899-7667},
  doi = {10.1162/neco.2007.19.4.885},
  abstract = {We propose a model of intrinsic plasticity for a continuous activation model neuron based on information theory. We then show how intrinsic and synaptic plasticity mechanisms interact and allow the neuron to discover heavy-tailed directions in the input. We also demonstrate that intrinsic plasticity may be an alternative explanation for the sliding threshold postulated in the BCM theory of synaptic plasticity. We present a theoretical analysis of the interaction of intrinsic plasticity with different Hebbian learning rules for the case of clustered inputs. Finally, we perform experiments on the ``bars'' problem, a popular nonlinear independent component analysis problem.},
  number = {4},
  journal = {Neural Computation},
  author = {Triesch, Jochen},
  month = mar,
  year = {2007},
  pages = {885-909},
  file = {articles/Triesch2007.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/HXDKH37S/neco.2007.19.4.html}
}

@article{Pillow2008,
  title = {Spatio-Temporal Correlations and Visual Signalling in a Complete Neuronal Population},
  volume = {454},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature07140},
  number = {7207},
  journal = {Nature},
  author = {Pillow, Jonathan W. and Shlens, Jonathon and Paninski, Liam and Sher, Alexander and Litke, Alan M. and Chichilnisky, E. J. and Simoncelli, Eero P.},
  month = aug,
  year = {2008},
  pages = {995-999},
  file = {articles/Pillow2008.pdf}
}

@article{Pernice2011,
  title = {How {{Structure Determines Correlations}} in {{Neuronal Networks}}},
  volume = {7},
  doi = {10.1371/journal.pcbi.1002059},
  abstract = {Author Summary
Many biological systems have been described as networks whose complex properties influence the behaviour of the system. Correlations of activity in such networks are of interest in a variety of fields, from gene-regulatory networks to neuroscience. Due to novel experimental techniques allowing the recording of the activity of many pairs of neurons and their importance with respect to the functional interpretation of spike data, spike train correlations in neural networks have recently attracted a considerable amount of attention. Although origin and function of these correlations is not known in detail, they are believed to have a fundamental influence on information processing and learning. We present a detailed explanation of how recurrent connectivity induces correlations in local neural networks and how structural features affect their size and distribution. We examine under which conditions network characteristics like distance dependent connectivity, hubs or patches markedly influence correlations and population signals.},
  number = {5},
  journal = {PLoS Comput Biol},
  author = {Pernice, Volker and Staude, Benjamin and Cardanobile, Stefano and Rotter, Stefan},
  month = may,
  year = {2011},
  pages = {e1002059},
  file = {articles/Pernice2011.pdf}
}

@article{Haas2011,
  title = {Activity-Dependent Long-Term Depression of Electrical Synapses},
  volume = {334},
  issn = {1095-9203},
  doi = {10.1126/science.1207502},
  abstract = {Use-dependent forms of synaptic plasticity have been extensively characterized at chemical synapses, but a relationship between natural activity and strength at electrical synapses remains elusive. The thalamic reticular nucleus (TRN), a brain area rich in gap-junctional (electrical) synapses, regulates cortical attention to the sensory surround and participates in shifts between arousal states; plasticity of electrical synapses may be a key mechanism underlying these processes. We observed long-term depression resulting from coordinated burst firing in pairs of coupled TRN neurons. Changes in gap-junctional communication were asymmetrical, indicating that regulation of connectivity depends on the direction of use. Modification of electrical synapses resulting from activity in coupled neurons is likely to be a widespread and powerful mechanism for dynamic reorganization of electrically coupled neuronal networks.},
  language = {eng},
  number = {6054},
  journal = {Science (New York, N.Y.)},
  author = {Haas, Julie S. and Zavala, Baltazar and Landisman, Carole E.},
  month = oct,
  year = {2011},
  keywords = {Action potentials,Animals,Electrical Synapses,In Vitro Techniques,Intralaminar Thalamic Nuclei,Long-Term Synaptic Depression,Membrane Potentials,Nerve Net,Neurons,Patch-Clamp Techniques,Rats,Rats; Sprague-Dawley,Sodium,Tetrodotoxin},
  pages = {389-393},
  file = {articles/Haas2011.pdf},
  pmid = {22021860}
}

@article{Mukai2015,
  title = {Molecular {{Substrates}} of {{Altered Axonal Growth}} and {{Brain Connectivity}} in a {{Mouse Model}} of {{Schizophrenia}}},
  volume = {86},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.04.003},
  language = {en},
  number = {3},
  journal = {Neuron},
  author = {Mukai, Jun and Tamura, Makoto and F\'enelon, Karine and Rosen, Andrew M. and Spellman, Timothy J. and Kang, Rujun and MacDermott, Amy B. and Karayiorgou, Maria and Gordon, Joshua A. and Gogos, Joseph A.},
  month = may,
  year = {2015},
  pages = {680-695},
  file = {articles/Mukai2015.pdf}
}

@book{Oppenheim1997,
  address = {Upper Saddle River, N.J},
  edition = {2nd ed},
  series = {Prentice-Hall signal processing series},
  title = {Signals \& Systems},
  isbn = {978-0-13-814757-0},
  lccn = {QA402 .O63 1997},
  publisher = {{Prentice Hall}},
  author = {Oppenheim, Alan V. and Willsky, Alan S. and Nawab, Syed Hamid},
  year = {1997},
  keywords = {Signal theory (Telecommunication),System analysis},
  file = {books/Oppenheim1997_Signals-&-systems.djvu}
}

@article{Deger2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.05730},
  primaryClass = {q-bio},
  title = {Multi-Contact Synapses for Stable Networks: A Spike-Timing Dependent Model of Dendritic Spine Plasticity and Turnover},
  shorttitle = {Multi-Contact Synapses for Stable Networks},
  abstract = {Excitatory synaptic connections in the adult neocortex consist of multiple synaptic contacts, almost exclusively formed on dendritic spines. Changes of dendritic spine shape and volume, a correlate of synaptic strength, can be tracked in vivo for weeks. Here, we present a combined model of spike-timing dependent dendritic spine plasticity and turnover that explains the steady state multi-contact configuration of synapses in adult neocortical networks. In this model, many presynaptic neurons compete to make strong synaptic connections onto postsynaptic neurons, while the synaptic contacts comprising each connection cooperate via postsynaptic firing. We demonstrate that the model is consistent with experimentally observed long-term dendritic spine dynamics under steady-state and lesion induced conditions, and show that cooperation of multiple synaptic contacts is crucial for stable, long-term synaptic memories. In simulations of a simplified network of barrel cortex, our plasticity rule reproduces whisker-trimming induced rewiring of thalamo-cortical and recurrent synaptic connectivity on realistic time scales.},
  journal = {arXiv:1609.05730 [q-bio]},
  author = {Deger, Moritz and Seeholzer, Alexander and Gerstner, Wulfram},
  month = sep,
  year = {2016},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {articles/Deger2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/XC5852VA/1609.html}
}

@article{Klampfl2013,
  title = {Emergence of {{Dynamic Memory Traces}} in {{Cortical Microcircuit Models}} through {{STDP}}},
  volume = {33},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5044-12.2013},
  language = {en},
  number = {28},
  journal = {Journal of Neuroscience},
  author = {Klampfl, S. and Maass, W.},
  month = jul,
  year = {2013},
  pages = {11515-11529},
  file = {articles/Klampfl2013.pdf}
}

@article{Medinilla2013,
  title = {Features of Proximal and Distal Excitatory Synaptic Inputs to Layer {{V}} Neurons of the Rat Medial Entorhinal Cortex},
  volume = {591},
  issn = {0022-3751},
  doi = {10.1113/jphysiol.2012.237172},
  abstract = {The entorhinal cortex (EC) has a fundamental function in transferring information between the hippocampus and the neocortex. EC layer V principal neurons are the main recipients of the hippocampal output and send processed information to the neocortex, likely playing an important role in memory processing and consolidation. Most of these neurons have apical dendrites that extend to the superficial layers and are rich in spines, which could be the targets of excitatory inputs from fibres innervating that region. We have used electrical stimulation of afferent fibres coupled with whole-cell patch-clamp somatic recordings to study the features of distal excitatory inputs and compare them with those of proximal ones. The amplitude of putative unitary excitatory responses was $\sim$1.5 times larger for distal compared with proximal inputs. The responses were purely glutamatergic, as they were abolished by a combination of AMPA and NMDA glutamatergic receptor antagonists. Blockade of Ih by 4-ethylphenylamino-1,2-dimethyl-6-methylaminopyrimidinium chloride (ZD7288) increased temporal summation; the increase was comparable for proximal and distal inputs. Proximal inputs initiated a somatic spike more reliably than distal ones; in some instances, somatic action potentials triggered by distal stimulation were preceded by dendritic spikes that fully propagated to the soma. Altogether, our results show that medial layer V entorhinal neurons receive excitatory synapses at distal dendritic locations, which gives them access to information encoded by inputs to the superficial layers as well as the deep layers. These findings are fundamentally relevant to understanding the role of the EC in the formation and consolidation of episodic memory.},
  number = {Pt 1},
  journal = {The Journal of Physiology},
  author = {Medinilla, Virginia and Johnson, Oralee and Gasparini, Sonia},
  month = jan,
  year = {2013},
  pages = {169-183},
  file = {articles/Medinilla2013.pdf},
  pmid = {23006478},
  pmcid = {PMC3630779}
}

@article{Stepanyants2008,
  title = {Local {{Potential Connectivity}} in {{Cat Primary Visual Cortex}}},
  volume = {18},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhm027},
  abstract = {Time invariant description of synaptic connectivity in cortical circuits may be precluded by the ongoing growth and retraction of dendritic spines accompanied by the formation and elimination of synapses. On the other hand, the spatial arrangement of axonal and dendritic branches appears stable. This suggests that an invariant description of connectivity can be cast in terms of potential synapses, which are locations in the neuropil where an axon branch of one neuron is proximal to a dendritic branch of another neuron. In this paper, we attempt to reconstruct the potential connectivity in local cortical circuits of the cat primary visual cortex (V1). Based on multiple single-neuron reconstructions of axonal and dendritic arbors in 3 dimensions, we evaluate the expected number of potential synapses and the probability of potential connectivity among excitatory (pyramidal and spiny stellate) neurons and inhibitory basket cells. The results provide a quantitative description of structural organization of local cortical circuits. For excitatory neurons from different cortical layers, we compute local domains, which contain their potentially pre- and postsynaptic excitatory partners. These domains have columnar shapes with laminar specific radii and are roughly of the size of the ocular dominance column. Therefore, connections between most excitatory neurons in the ocular dominance column can be implemented by local synaptogenesis. Structural connectivity involving inhibitory basket cells is generally weaker than excitatory connectivity. Here, only nearby neurons are capable of establishing more than one potential synapse, implying that within the ocular dominance column these connections have more limited potential for circuit remodeling.},
  language = {en},
  number = {1},
  journal = {Cerebral Cortex},
  author = {Stepanyants, Armen and Hirsch, Judith A. and Martinez, Luis M. and Kisv\'arday, Zolt\'an F. and Ferecsk\'o, Alex S. and Chklovskii, Dmitri B.},
  month = jan,
  year = {2008},
  keywords = {excitatory,inhibitory,interlaminar connectivity,morphology,neurogeometry},
  pages = {13-28},
  file = {articles/Stepanyants2008.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/87JUCGC3/13.html},
  pmid = {17420172}
}

@techreport{zotero-93,
  title = {Overpic},
  file = {manuals/latex/overpic.pdf},
  note = {manuals/latex}
}

@unpublished{Hoffmann2009d,
  title = {Algebra},
  author = {Hoffmann, Felix},
  year = {2009},
  file = {manuscripts/Hoffmann2009_Algebra.pdf}
}

@article{Baudot2013,
  title = {Animation of Natural Scene by Virtual Eye-Movements Evokes High Precision and Low Noise in {{V1}} Neurons},
  volume = {7},
  doi = {10.3389/fncir.2013.00206},
  abstract = {Synaptic noise is thought to be a limiting factor for computational efficiency in the brain. In visual cortex (V1), ongoing activity is present in vivo, and spiking responses to simple stimuli are highly unreliable across trials. Stimulus statistics used to plot receptive fields, however, are quite different from those experienced during natural visuomotor exploration. We recorded V1 neurons intracellularly in the anaesthetized and paralyzed cat and compared their spiking and synaptic responses to full field natural images animated by simulated eye-movements to those evoked by simpler (grating) or higher dimensionality statistics (dense noise). In most cells, natural scene animation was the only condition where high temporal precision (in the 10\textendash{}20 ms range) was maintained during sparse and reliable activity. At the subthreshold level, irregular but highly reproducible membrane potential dynamics were observed, even during long (several 100 ms) ``spike-less'' periods. We showed that both the spatial structure of natural scenes and the temporal dynamics of eye-movements increase the signal-to-noise ratio by a non-linear amplification of the signal combined with a reduction of the subthreshold contextual noise. These data support the view that the sparsening and the time precision of the neural code in V1 may depend primarily on three factors: (1) broadband input spectrum: the bandwidth must be rich enough for recruiting optimally the diversity of spatial and time constants during recurrent processing; (2) tight temporal interplay of excitation and inhibition: conductance measurements demonstrate that natural scene statistics narrow selectively the duration of the spiking opportunity window during which the balance between excitation and inhibition changes transiently and reversibly; (3) signal energy in the lower frequency band: a minimal level of power is needed below 10 Hz to reach consistently the spiking threshold, a situation rarely reached with visual dense noise.},
  journal = {Frontiers in Neural Circuits},
  author = {Baudot, Pierre and Levy, Manuel and Marre, Olivier and Monier, Cyril and Pananceau, Marc and Fr\'egnac, Yves},
  year = {2013},
  keywords = {eye movements,intracellular membrane potential dynamics,natural visual statistics,reliability,sensory coding,visual cortex},
  pages = {206},
  file = {articles/Baudot2013.pdf}
}

@techreport{zotero-96,
  title = {Biblatex {{Package}} 1.7},
  file = {manuals/latex/biblatex_package_1.7.pdf},
  note = {manuals/latex}
}

@article{Lu2015,
  title = {Neuroscience: {{Forgetfulness}} Illuminated},
  volume = {525},
  issn = {0028-0836, 1476-4687},
  shorttitle = {Neuroscience},
  doi = {10.1038/nature15211},
  number = {7569},
  journal = {Nature},
  author = {Lu, Ju and Zuo, Yi},
  month = sep,
  year = {2015},
  pages = {324-325},
  file = {articles/Lu2015.pdf}
}

@book{Drongelen2007,
  address = {Amsterdam ; Burlington, MA},
  title = {Signal Processing for Neuroscientists: Introduction to the Analysis of Physiological Signals},
  isbn = {978-0-12-370867-0},
  lccn = {R857.D47 .D755 2007},
  shorttitle = {Signal Processing for Neuroscientists},
  publisher = {{Elsevier/Academic Press}},
  author = {van Drongelen, Wim},
  year = {2007},
  keywords = {Data processing,Digital techniques,Mathematical models,Neurology,Neurosciences,Physiology,Signal processing},
  file = {books/Drongelen2007_Signal-processing-for-neuroscientists-introduction-to-the-analysis-of-physiological-signals.pdf}
}

@article{Wilson1972,
  title = {Excitatory and {{Inhibitory Interactions}} in {{Localized Populations}} of {{Model Neurons}}},
  volume = {12},
  issn = {0006-3495},
  doi = {10.1016/S0006-3495(72)86068-5},
  abstract = {Coupled nonlinear differential equations are derived for the dynamics of spatially localized populations containing both excitatory and inhibitory model neurons. Phase plane methods and numerical solutions are then used to investigate population responses to various types of stimuli. The results obtained show simple and multiple hysteresis phenomena and limit cycle activity. The latter is particularly interesting since the frequency of the limit cycle oscillation is found to be a monotonic function of stimulus intensity. Finally, it is proved that the existence of limit cycle dynamics in response to one class of stimuli implies the existence of multiple stable states and hysteresis in response to a different class of stimuli. The relation between these findings and a number of experiments is discussed.},
  language = {English},
  number = {1},
  journal = {Biophysical Journal},
  author = {Wilson, Hugh R. and Cowan, Jack D.},
  month = jan,
  year = {1972},
  pages = {1-24},
  file = {articles/Wilson1972.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/24GTEUF5/S0006-3495(72)86068-5.html},
  pmid = {4332108}
}

@article{Zheng2013,
  title = {Network {{Self}}-{{Organization Explains}} the {{Statistics}} and {{Dynamics}} of {{Synaptic Connection Strengths}} in {{Cortex}}},
  volume = {9},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002848},
  abstract = {Author Summary  The computations that brain circuits can perform depend on their wiring. While a wiring diagram is still out of reach for major brain structures such as the neocortex and hippocampus, data on the overall distribution of synaptic connection strengths and the temporal fluctuations of individual synapses have recently become available. Specifically, there exists a small population of very strong and stable synaptic connections, which may form the physiological substrate of life-long memories. This population coexists with a big and ever changing population of much smaller and strongly fluctuating synaptic connections. So far it has remained unclear how these properties of networks in neocortex and hippocampus arise. Here we present a computational model that explains these fundamental properties of neural circuits as a consequence of network self-organization resulting from the combined action of different forms of neuronal plasticity. This self-organization is driven by a rich-get-richer effect induced by an associative synaptic learning mechanism which is kept in check by several homeostatic plasticity mechanisms stabilizing the network. The model highlights the role of self-organization in the formation of brain circuits and parsimoniously explains a range of recent findings about their fundamental properties.},
  number = {1},
  journal = {PLOS Comput Biol},
  author = {Zheng, Pengsheng and Dimitrakakis, Christos and Triesch, Jochen},
  month = jan,
  year = {2013},
  keywords = {Hippocampus,Homeostatic mechanisms,Neural pathways,Neuronal plasticity,Neurons,Synapses,neural networks,synaptic plasticity},
  pages = {e1002848},
  file = {articles/Zheng2013_2.pdf;articles/Zheng2013_3.pdf;articles/Zheng2013.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/6DV2FIZD/article.html}
}

@book{Bang-Jensen2002,
  title = {Digraphs},
  isbn = {1-85233-611-0},
  language = {English},
  publisher = {{Springer}},
  author = {{Bang-Jensen}, Jorgen and Gutin, Gregory Z. and Gutin, Gregory},
  month = aug,
  year = {2002},
  file = {books/Bang-Jensen2002_Digraphs.pdf}
}

@article{Roxin2011a,
  title = {The {{Role}} of {{Degree Distribution}} in {{Shaping}} the {{Dynamics}} in {{Networks}} of {{Sparsely Connected Spiking Neurons}}},
  volume = {5},
  issn = {1662-5188},
  doi = {10.3389/fncom.2011.00008},
  abstract = {Neuronal network models often assume a fixed probability of connection between neurons. This assumption leads to random networks with binomial in-degree and out-degree distributions which are relatively narrow. Here I study the effect of broad degree distributions on network dynamics by interpolating between a binomial and a truncated power-law distribution for the in-degree and out-degree independently. This is done both for an inhibitory network (I network) as well as for the recurrent excitatory connections in a network of excitatory and inhibitory neurons (EI network). In both cases increasing the width of the in-degree distribution affects the global state of the network by driving transitions between asynchronous behavior and oscillations. This effect is reproduced in a simplified rate model which includes the heterogeneity in neuronal input due to the in-degree of cells. On the other hand, broadening the out-degree distribution is shown to increase the fraction of common inputs to pairs of neurons. This leads to increases in the amplitude of the cross-correlation (CC) of synaptic currents. In the case of the I network, despite strong oscillatory CCs in the currents, CCs of the membrane potential are low due to filtering and reset effects, leading to very weak CCs of the spike-count. In the asynchronous regime of the EI network, broadening the out-degree increases the amplitude of CCs in the recurrent excitatory currents, while CC of the total current is essentially unaffected as are pairwise spiking correlations. This is due to a dynamic balance between excitatory and inhibitory synaptic currents. In the oscillatory regime, changes in the out-degree can have a large effect on spiking correlations and even on the qualitative dynamical state of the network.},
  journal = {Frontiers in Computational Neuroscience},
  author = {Roxin, Alex},
  month = mar,
  year = {2011},
  file = {articles/Roxin2011.pdf},
  pmid = {21556129},
  pmcid = {PMC3058136}
}

@unpublished{Hoffmann2015a,
  title = {Markovketten - {{Zusammenfassung}}},
  author = {Hoffmann, Felix},
  year = {2015},
  file = {manuscripts/Hoffmann2015_Markovketten---Zusammenfassung.pdf}
}

@article{Fino2011,
  title = {Dense {{Inhibitory Connectivity}} in {{Neocortex}}},
  volume = {69},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2011.02.025},
  abstract = {The connectivity diagram of neocortical circuits is still unknown, and there are conflicting data as to whether cortical neurons are wired specifically or not. To investigate the basic structure of cortical microcircuits, we use a two-photon photostimulation technique that enables the systematic mapping of synaptic connections with single-cell resolution. We map the inhibitory connectivity between upper layers somatostatin-positive GABAergic interneurons and pyramidal cells in mouse frontal cortex. Most, and sometimes all, inhibitory neurons are locally connected to every sampled pyramidal cell. This dense inhibitory connectivity is found at both young and mature developmental ages. Inhibitory innervation of neighboring pyramidal cells is similar, regardless of whether they are connected among themselves or not. We conclude that local inhibitory connectivity is promiscuous, does not form subnetworks, and can approach the theoretical limit of a completely connected synaptic matrix.},
  language = {English},
  number = {6},
  journal = {Neuron},
  author = {Fino, Elodie and Yuste, Rafael},
  month = mar,
  year = {2011},
  pages = {1188-1203},
  file = {articles/Fino2011_2.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/4AF7GN5F/S0896-6273(11)00123-1.html},
  pmid = {21435562, 21435562}
}

@article{Babadi2016,
  title = {Stability and {{Competition}} in {{Multi}}-Spike {{Models}} of {{Spike}}-{{Timing Dependent Plasticity}}},
  volume = {12},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004750},
  language = {en},
  number = {3},
  journal = {PLOS Computational Biology},
  author = {Babadi, Baktash and Abbott, L. F.},
  editor = {Gutkin, Boris S.},
  month = mar,
  year = {2016},
  pages = {e1004750},
  file = {articles/Babadi2016.pdf}
}

@book{Fisher1987,
  address = {Cambridge [Cambridgeshire] ; New York},
  title = {Statistical Analysis of Spherical Data},
  isbn = {978-0-521-24273-8},
  lccn = {QA276 .F489 1987},
  publisher = {{Cambridge University Press}},
  author = {Fisher, N. I. and Lewis, Toby and Embleton, B. J. J.},
  year = {1987},
  keywords = {Mathematical statistics,Spherical data},
  file = {books/Fisher1987_Statistical-analysis-of-spherical-data.pdf}
}

@book{Smola2008,
  title = {Introduction to {{Machine Learning}}},
  publisher = {{Cambridge University Press}},
  author = {Smola, Alex and Vishwanathan, S.V.N.},
  year = {2008},
  file = {books/Smola2008_Introduction-to-Machine-Learning.pdf}
}

@article{Ramaswamy2012,
  title = {Intrinsic Morphological Diversity of Thick-Tufted Layer 5 Pyramidal Neurons Ensures Robust and Invariant Properties of in Silico Synaptic Connections},
  volume = {590},
  issn = {0022-3751},
  doi = {10.1113/jphysiol.2011.219576},
  abstract = {Non-technical summary
Pyramidal neurons are output neurons of the neocortex. The thick-tufted layer 5 (TTL5) pyramidal neurons are one of the most extensively studied neocortical cell types and are characterized by an exquisite morphological structure. Despite their characteristic morphology, TTL5 neurons in the neocortical microcircuit display an intrinsic diversity that renders each neuron morphologically unique. In order to investigate the functional significance of this intrinsic morphological diversity, we reconstructed networks of TTL5 neurons through a detailed computer model and compared the properties of modelled synaptic connections against experimental data. We found that the average synaptic properties of modelled connections between TTL5 neurons closely matched experimental observations and remained unaltered by changes to several parameters at the local network level. These results show that the intrinsic morphological diversity of TTL5 neurons is a mechanism to ensure that the average synaptic properties are robust to changes at the local network level.

Abstract
The morphology of neocortical pyramidal neurons is not only highly characteristic but also displays an intrinsic diversity that renders each neuron morphologically unique. We investigated the significance of this intrinsic morphological diversity in in silico networks composed of thick-tufted layer 5 (TTL5) pyramidal neurons, by comparing the in silico and in vitro properties of TTL5 synaptic connections. The synaptic locations of in silico connections were determined by placing 3D reconstructed TTL5 neurons randomly in a volume equivalent to that of layer 5 in the juvenile rat somatosensory cortex and using a `collision-detection' algorithm to identify the incidental loci of axo-dendritic overlap. The activation time of the modelled synapses and their biophysical properties were characterized based on experimental measurements. We found that the anatomical loci of synapses and the physiological properties of the somatically recorded EPSPs closely matched those recorded experimentally without the need for any fine-tuning. Furthermore, perturbations to both the physiological or anatomical parameters of the model did not alter the average physiological properties of the population of modelled synaptic connections. This microcircuit-level robust behaviour was due to the intrinsic diversity of the morphology of pyramidal neurons in the microcircuit. We conclude that synaptic transmission in a network of TTL5 neurons is highly invariant across microcircuits suggesting that intrinsic diversity is a mechanism to ensure the same average synaptic properties in different animals of the same species. Finally, we show that the average physiological properties of the TTL5 microcircuit are surprisingly robust to anatomical and physiological perturbations also partly due to the intrinsic diversity of pyramidal neuron morphology.},
  number = {Pt 4},
  journal = {The Journal of Physiology},
  author = {Ramaswamy, Srikanth and Hill, Sean L and King, James G and Sch\"urmann, Felix and Wang, Yun and Markram, Henry},
  month = feb,
  year = {2012},
  pages = {737-752},
  file = {articles/Ramaswamy2012.pdf},
  pmid = {22083599},
  pmcid = {PMC3381307}
}

@book{Mardia2000a,
  address = {Chichester; New York},
  title = {Directional Statistics},
  isbn = {0-471-95333-4 978-0-471-95333-3},
  language = {English},
  publisher = {{J. Wiley}},
  author = {Mardia, K. V and Jupp, Peter E},
  year = {2000},
  file = {books/Mardia2000_Directional-statistics.pdf}
}

@incollection{Braitenberg1992,
  title = {Manifesto of {{Brain Science}}},
  copyright = {\textcopyright{}1992 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-49969-2 978-3-642-49967-8},
  language = {en},
  booktitle = {Information {{Processing}} in the {{Cortex}}},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Braitenberg, Valentino},
  editor = {Aertsen, Dr Ad and Braitenberg, Professor Dr Valentino},
  year = {1992},
  keywords = {Artificial Intelligence (incl. Robotics),Biophysics and Biological Physics,Mathematical and Computational Biology,Neurosciences,Simulation and Modeling,Statistics for Life Sciences; Medicine; Health Sciences},
  pages = {473-477},
  file = {book_sections/Braitenberg1992.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/8PNUXR8A/10.html},
  doi = {10.1007/978-3-642-49967-8_29}
}

@article{Brunel2000,
  title = {Dynamics of {{Sparsely Connected Networks}} of {{Excitatory}} and {{Inhibitory Spiking Neurons}}},
  volume = {8},
  issn = {0929-5313, 1573-6873},
  doi = {10.1023/A:1008925309027},
  abstract = {The dynamics of networks of sparsely connected excitatory and inhibitory integrate-and-fire neurons are studied analytically. The analysis reveals a rich repertoire of states, including synchronous states in which neurons fire regularly; asynchronous states with stationary global activity and very irregular individual cell activity; and states in which the global activity oscillates but individual cells fire irregularly, typically at rates lower than the global oscillation frequency. The network can switch between these states, provided the external frequency, or the balance between excitation and inhibition, is varied. Two types of network oscillations are observed. In the fast oscillatory state, the network frequency is almost fully controlled by the synaptic time scale. In the slow oscillatory state, the network frequency depends mostly on the membrane time constant. Finite size effects in the asynchronous state are also discussed.},
  language = {en},
  number = {3},
  journal = {Journal of Computational Neuroscience},
  author = {Brunel, Nicolas},
  month = may,
  year = {2000},
  keywords = {Human Genetics,Neurology,Neurosciences,Recurrent network,Synchronization,Theory of Computation},
  pages = {183-208},
  file = {articles/Brunel2000.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/ZZAZTIN8/A1008925309027.html}
}

@unpublished{Hoffmann2015e,
  title = {Funktional {{Analysis}} ({{Kuwert}})},
  author = {Hoffmann},
  year = {2015},
  file = {manuscripts/Hoffmann2015_Funktional-Analysis-(Kuwert).pdf}
}

@article{Rosenbaum2014a,
  title = {Balanced {{Networks}} of {{Spiking Neurons}} with {{Spatially Dependent Recurrent Connections}}},
  volume = {4},
  doi = {10.1103/PhysRevX.4.021039},
  abstract = {Networks of model neurons with balanced recurrent excitation and inhibition capture the irregular and asynchronous spiking activity reported in cortex. While mean-field theories of spatially homogeneous balanced networks are well understood, a mean-field analysis of spatially heterogeneous balanced networks has not been fully developed. We extend the analysis of balanced networks to include a connection probability that depends on the spatial separation between neurons. In the continuum limit, we derive that stable, balanced firing rate solutions require that the spatial spread of external inputs be broader than that of recurrent excitation, which in turn must be broader than or equal to that of recurrent inhibition. Notably, this implies that network models with broad recurrent inhibition are inconsistent with the balanced state. For finite size networks, we investigate the pattern-forming dynamics arising when balanced conditions are not satisfied. Our study highlights the new challenges that balanced networks pose for the spatiotemporal dynamics of complex systems.},
  number = {2},
  journal = {Physical Review X},
  author = {Rosenbaum, Robert and Doiron, Brent},
  month = may,
  year = {2014},
  pages = {021039},
  file = {articles/Rosenbaum2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/C3SXHQ76/PhysRevX.4.html}
}

@article{Stepanyants2005,
  title = {Neurogeometry and Potential Synaptic Connectivity},
  volume = {28},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2005.05.006},
  abstract = {The advent of high-quality 3D reconstructions of neuronal arbors has revived the hope of inferring synaptic connectivity from the geometric shapes of axons and dendrites, or `neurogeometry'. A quantitative description of connectivity must be built on a sound theoretical framework. Here, we review recent developments in neurogeometry that can provide such a framework. We base the geometric description of connectivity on the concept of a `potential synapse' \textendash{} the close apposition between axons and dendrites necessary to form an actual synapse. In addition to describing potential synaptic connectivity in neuronal circuits, neurogeometry provides insight into basic features of functional connectivity, such as specificity and plasticity.},
  number = {7},
  journal = {Trends in Neurosciences},
  author = {Stepanyants, Armen and Chklovskii, Dmitri B.},
  month = jul,
  year = {2005},
  pages = {387-394},
  file = {articles/Stepanyants2005.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/HPBWA3MW/S0166223605001311.html}
}

@techreport{zotero-116,
  title = {{{TikZ}} - {{Commutative Diagrams}}},
  file = {manuals/latex/tikz_-_commutative_diagrams.pdf},
  note = {manuals/latex}
}

@article{Martin2014,
  title = {Superficial Layer Pyramidal Cells Communicate Heterogeneously between Multiple Functional Domains of Cat Primary Visual Cortex},
  volume = {5},
  copyright = {\textcopyright{} 2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  doi = {10.1038/ncomms6252},
  abstract = {The axons of pyramidal neurons in the superficial layers of the neocortex of higher mammals form lateral networks of discrete clusters of synaptic boutons. In primary visual cortex the clusters are reported to link domains that share the same orientation preferences, but how individual neurons contribute to this network is unknown. Here we performed optical imaging to record the intrinsic signal, which is an indirect measure of neuronal firing, and determined the global map of orientation preferences in the cat primary visual system. In the same experiment, single cells were recorded and labelled intracellularly. We found that individual axons arborise within the retinotopic representation of the classical receptive field, but their bouton clusters were not aligned along their preferred axis of orientation along the retinotopic map. Axon clusters formed in a variety of different orientation domains, not just the like-orientation domains. This topography and heterogeneity of single-cell connectivity provides circuits for normalization and context-dependent feature processing of visual scenes.},
  language = {en},
  journal = {Nature Communications},
  author = {Martin, Kevan A. C. and Roth, Stephan and Rusch, Elisha S.},
  month = oct,
  year = {2014},
  keywords = {Biological sciences,Neuroscience},
  file = {articles/Martin2014.pdf}
}

@article{Fishell2013,
  title = {The {{Neuron Identity Problem}}: {{Form Meets Function}}},
  volume = {80},
  issn = {08966273},
  shorttitle = {The {{Neuron Identity Problem}}},
  doi = {10.1016/j.neuron.2013.10.035},
  language = {en},
  number = {3},
  journal = {Neuron},
  author = {Fishell, Gord and Heintz, Nathaniel},
  month = oct,
  year = {2013},
  pages = {602-612},
  file = {articles/Fishell2013.pdf}
}

@techreport{zotero-119,
  title = {Standalone {{Package}} 1.1b},
  file = {manuals/latex/standalone_package_1.1b.pdf},
  note = {manuals/latex}
}

@article{Song2005,
  title = {Highly {{Nonrandom Features}} of {{Synaptic Connectivity}} in {{Local Cortical Circuits}}},
  volume = {3},
  doi = {10.1371/journal.pbio.0030068},
  abstract = {A dataset of hundreds of recordings in which four neurons were simultaneously monitored reveals clustered connectivity patterns among cortical neurons.},
  number = {3},
  journal = {PLoS Biol},
  author = {Song, Sen and Sj\"ostr\"om, Per Jesper and Reigl, Markus and Nelson, Sacha and Chklovskii, Dmitri B},
  month = mar,
  year = {2005},
  keywords = {bidirs},
  pages = {e68},
  file = {articles/Song2005.pdf}
}

@article{delaRocha2007,
  title = {Correlation between Neural Spike Trains Increases with Firing Rate},
  volume = {448},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature06028},
  number = {7155},
  journal = {Nature},
  author = {{de la Rocha}, Jaime and Doiron, Brent and {Shea-Brown}, Eric and Josi\'c, Kre{\v s}imir and Reyes, Alex},
  month = aug,
  year = {2007},
  pages = {802-806},
  file = {articles/de la Rocha2007.pdf}
}

@misc{zotero-122,
  title = {Python, Extract File Name from Path, No Matter What the Os/Path Format},
  abstract = {Which Python library can I use to extract file names from paths, no matter what the operating system or path format could be ?

For example, I'd like all of these paths to return me "c" : 

a/b/c/
...},
  howpublished = {http://stackoverflow.com/questions/8384737/python-extract-file-name-from-path-no-matter-what-the-os-path-format}
}

@article{Ko2011,
  title = {Functional Specificity of Local Synaptic Connections in Neocortical Networks},
  volume = {473},
  copyright = {\textcopyright{} 2011 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {0028-0836},
  doi = {10.1038/nature09880},
  abstract = {Neuronal connectivity is fundamental to information processing in the brain. Therefore, understanding the mechanisms of sensory processing requires uncovering how connection patterns between neurons relate to their function. On a coarse scale, long-range projections can preferentially link cortical regions with similar responses to sensory stimuli. But on the local scale, where dendrites and axons overlap substantially, the functional specificity of connections remains unknown. Here we determine synaptic connectivity between nearby layer 2/3 pyramidal neurons in vitro, the response properties of which were first characterized in mouse visual cortex in vivo. We found that connection probability was related to the similarity of visually driven neuronal activity. Neurons with the same preference for oriented stimuli connected at twice the rate of neurons with orthogonal orientation preferences. Neurons responding similarly to naturalistic stimuli formed connections at much higher rates than those with uncorrelated responses. Bidirectional synaptic connections were found more frequently between neuronal pairs with strongly correlated visual responses. Our results reveal the degree of functional specificity of local synaptic connections in the visual cortex, and point to the existence of fine-scale subnetworks dedicated to processing related sensory information.},
  language = {en},
  number = {7345},
  journal = {Nature},
  author = {Ko, Ho and Hofer, Sonja B. and Pichler, Bruno and Buchanan, Katherine A. and Sj\"ostr\"om, P. Jesper and {Mrsic-Flogel}, Thomas D.},
  month = may,
  year = {2011},
  keywords = {Neuroscience},
  pages = {87-91},
  file = {articles/Ko2011.pdf}
}

@article{Tartaglia2009,
  title = {Perceptual Learning and Roving: {{Stimulus}} Types and Overlapping Neural Populations},
  volume = {49},
  issn = {0042-6989},
  shorttitle = {Perceptual Learning and Roving},
  doi = {10.1016/j.visres.2009.02.013},
  abstract = {In perceptual learning, performance usually improves when observers train with one type of stimulus, for example, a bisection stimulus. Roving denotes the situation when, instead of one, two or more types of stimuli are presented randomly interleaved, for example, a bisection stimulus and a vernier. For some combinations of stimulus types, performance improves in roving situations whereas for others it does not. To investigate when roving impedes perceptual learning, we conducted four experiments. Performance improved, for example, when we roved a bisection stimulus and a vernier but not when we roved certain types of bisection stimuli. We propose that roving hinders perceptual learning when the stimulus types are clearly distinct from each other but still excite overlapping but not identical neural populations.},
  number = {11},
  journal = {Vision Research},
  author = {Tartaglia, Elisa M. and Aberg, Kristoffer C. and Herzog, Michael H.},
  month = jun,
  year = {2009},
  keywords = {Bisection task,Hyperacuity,Spatial vision},
  pages = {1420-1427},
  file = {articles/Tartaglia2009.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/UPETUW22/S0042698909000583.html}
}

@book{Switzer2002,
  address = {Berlin},
  series = {Classics in mathematics},
  title = {Algebraic Topology - Homotopy and Homology},
  isbn = {978-3-540-42750-6},
  language = {eng},
  publisher = {{Springer}},
  author = {Switzer, Robert M.},
  year = {2002},
  keywords = {\#aAlgebraic topology\#zAutomatisch aus GBV_2011-10 2012-06-07,\#aHomology theory\#zAutomatisch aus GBV_2011-10 2012-06-07,\#aHomotopy theory\#zAutomatisch aus GBV_2011-10 2012-06-07,28,Algebraic topology,Algebraische Topologie,Homology theory,Homotopy theory},
  file = {books/Switzer2002_Algebraic-topology---homotopy-and-homology.djvu;books/Switzer2002_Algebraic-topology---homotopy-and-homology.pdf}
}

@book{Rojas1996,
  address = {Berlin ; New York},
  edition = {1 edition},
  title = {Neural {{Networks}}: {{A Systematic Introduction}}},
  isbn = {978-3-540-60505-8},
  shorttitle = {Neural {{Networks}}},
  abstract = {Neural networks are a computing paradigm that is finding increasing attention among computer scientists. In this book, theoretical laws and models previously scattered in the literature are brought together into a general theory of artificial neural nets. Always with a view to biology and starting with the simplest nets, it is shown how the properties of models change when more general computing elements and net topologies are introduced. Each chapter contains examples, numerous illustrations, and a bibliography. The book is aimed at readers who seek an overview of the field or who wish to deepen their knowledge. It is suitable as a basis for university courses in neurocomputing.},
  language = {English},
  publisher = {{Springer}},
  author = {Rojas, Raul},
  month = jan,
  year = {1996},
  file = {books/Rojas1996_Neural-Networks-A-Systematic-Introduction.pdf}
}

@article{Sporns2007,
  title = {Brain Connectivity},
  volume = {2},
  issn = {1941-6016},
  doi = {10.4249/scholarpedia.4695},
  number = {10},
  journal = {Scholarpedia},
  author = {Sporns, Olaf},
  year = {2007},
  pages = {4695}
}

@article{Spruston2008,
  title = {Pyramidal Neurons: Dendritic Structure and Synaptic Integration},
  volume = {9},
  copyright = {\textcopyright{} 2008 Nature Publishing Group},
  issn = {1471-003X},
  shorttitle = {Pyramidal Neurons},
  doi = {10.1038/nrn2286},
  abstract = {Pyramidal neurons are characterized by their distinct apical and basal dendritic trees and the pyramidal shape of their soma. They are found in several regions of the CNS and, although the reasons for their abundance remain unclear, functional studies \textemdash{} especially of CA1 hippocampal and layer V neocortical pyramidal neurons \textemdash{} have offered insights into the functions of their unique cellular architecture. Pyramidal neurons are not all identical, but some shared functional principles can be identified. In particular, the existence of dendritic domains with distinct synaptic inputs, excitability, modulation and plasticity appears to be a common feature that allows synapses throughout the dendritic tree to contribute to action-potential generation. These properties support a variety of coincidence-detection mechanisms, which are likely to be crucial for synaptic integration and plasticity.},
  language = {en},
  number = {3},
  journal = {Nature Reviews Neuroscience},
  author = {Spruston, Nelson},
  month = mar,
  year = {2008},
  pages = {206-221},
  file = {articles/Spruston2008.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/JHPUPAXD/nrn2286.html}
}

@article{Effenberger2015,
  title = {Self-Organization in {{Balanced State Networks}} by {{STDP}} and {{Homeostatic Plasticity}}},
  volume = {11},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004420},
  language = {en},
  number = {9},
  journal = {PLOS Computational Biology},
  author = {Effenberger, Felix and Jost, J\"urgen and Levina, Anna},
  editor = {Morrison, Abigail},
  month = sep,
  year = {2015},
  keywords = {_tablet},
  pages = {e1004420},
  file = {articles/Effenberger2015.PDF}
}

@book{Schleif1993,
  address = {Baltimore},
  edition = {2nd ed},
  title = {Genetics and Molecular Biology},
  isbn = {978-0-8018-4673-1 978-0-8018-4674-8},
  lccn = {QH442 .S34 1993},
  publisher = {{Johns Hopkins University Press}},
  author = {Schleif, Robert F.},
  year = {1993},
  keywords = {Molecular genetics},
  file = {books/Schleif1993_Genetics-and-molecular-biology.pdf}
}

@article{Briggman2011,
  title = {Wiring Specificity in the Direction-Selectivity Circuit of the Retina},
  volume = {471},
  copyright = {\textcopyright{} 2011 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {0028-0836},
  doi = {10.1038/nature09818},
  abstract = {The proper connectivity between neurons is essential for the implementation of the algorithms used in neural computations, such as the detection of directed motion by the retina. The analysis of neuronal connectivity is possible with electron microscopy, but technological limitations have impeded the acquisition of high-resolution data on a large enough scale. Here we show, using serial block-face electron microscopy and two-photon calcium imaging, that the dendrites of mouse starburst amacrine cells make highly specific synapses with direction-selective ganglion cells depending on the ganglion cell's preferred direction. Our findings indicate that a structural (wiring) asymmetry contributes to the computation of direction selectivity. The nature of this asymmetry supports some models of direction selectivity and rules out others. It also puts constraints on the developmental mechanisms behind the formation of synaptic connections. Our study demonstrates how otherwise intractable neurobiological questions can be addressed by combining functional imaging with the analysis of neuronal connectivity using large-scale electron microscopy.
View full text},
  language = {en},
  number = {7337},
  journal = {Nature},
  author = {Briggman, Kevin L. and Helmstaedter, Moritz and Denk, Winfried},
  month = mar,
  year = {2011},
  keywords = {Neuroscience},
  pages = {183-188},
  file = {articles/Briggman2011.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/QJ2KW5S4/nature09818.html}
}

@article{Kalisman2005,
  title = {The Neocortical Microcircuit as a Tabula Rasa},
  volume = {102},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0407088102},
  abstract = {The neocortex has a high capacity for plasticity. To understand the full scope of this capacity, it is essential to know how neurons choose particular partners to form synaptic connections. By using multineuron whole-cell recordings and confocal microscopy we found that axons of layer V neocortical pyramidal neurons do not preferentially project toward the dendrites of particular neighboring pyramidal neurons; instead, axons promiscuously touch all neighboring dendrites without any bias. Functional synaptic coupling of a small fraction of these neurons is, however, correlated with the existence of synaptic boutons at existing touch sites. These data provide the first direct experimental evidence for a tabula rasa-like structural matrix between neocortical pyramidal neurons and suggests that pre- and postsynaptic interactions shape the conversion between touches and synapses to form specific functional microcircuits. These data also indicate that the local neocortical microcircuit has the potential to be differently rewired without the need for remodeling axonal or dendritic arbors.},
  language = {en},
  number = {3},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  author = {Kalisman, Nir and Silberberg, Gilad and Markram, Henry},
  month = jan,
  year = {2005},
  keywords = {connectivity,neocortex,spines},
  pages = {880-885},
  file = {articles/Kalisman2005.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/QGFN4MDX/880.html},
  pmid = {15630093}
}

@unpublished{Lebiedz2011,
  title = {Optimierung},
  author = {Lebiedz},
  year = {2011},
  file = {manuscripts/Lebiedz2011_Optimierung.pdf}
}

@article{Waelti2001,
  title = {Dopamine Responses Comply with Basic Assumptions of Formal Learning Theory},
  volume = {412},
  copyright = {\textcopyright{} 2001 Nature Publishing Group},
  issn = {0028-0836},
  doi = {10.1038/35083500},
  abstract = {According to contemporary learning theories, the discrepancy, or error, between the actual and predicted reward determines whether learning occurs when a stimulus is paired with a reward. The role of prediction errors is directly demonstrated by the observation that learning is blocked when the stimulus is paired with a fully predicted reward. By using this blocking procedure, we show that the responses of dopamine neurons to conditioned stimuli was governed differentially by the occurrence of reward prediction errors rather than stimulus\textendash{}reward associations alone, as was the learning of behavioural reactions. Both behavioural and neuronal learning occurred predominantly when dopamine neurons registered a reward prediction error at the time of the reward. Our data indicate that the use of analytical tests derived from formal behavioural learning theory provides a powerful approach for studying the role of single neurons in learning.},
  language = {en},
  number = {6842},
  journal = {Nature},
  author = {Waelti, Pascale and Dickinson, Anthony and Schultz, Wolfram},
  month = jul,
  year = {2001},
  pages = {43-48},
  file = {articles/Waelti2001.pdf;articles/Waelti20012.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/B82UH3IC/412043a0.html}
}

@article{Borst1999,
  title = {Information Theory and Neural Coding},
  volume = {2},
  copyright = {\textcopyright{} 1999 Nature Publishing Group},
  issn = {1097-6256},
  doi = {10.1038/14731},
  abstract = {Information theory quantifies how much information a neural response carries about the stimulus. This can be compared to the information transferred in particular models of the stimulus-response function and to maximum possible information transfer. Such comparisons are crucial because they validate assumptions present in any neurophysiological analysis. Here we review information-theory basics before demonstrating its use in neural coding. We show how to use information theory to validate simple stimulus-response models of neural coding of dynamic stimuli. Because these models require specification of spike timing precision, they can reveal which time scales contain information in neural coding. This approach shows that dynamic stimuli can be encoded efficiently by single neurons and that each spike contributes to information transmission. We argue, however, that the data obtained so far do not suggest a temporal code, in which the placement of spikes relative to each other yields additional information.},
  language = {en},
  number = {11},
  journal = {Nature Neuroscience},
  author = {Borst, Alexander and Theunissen, Fr\'ed\'eric E.},
  month = nov,
  year = {1999},
  pages = {947-957},
  file = {articles/Borst1999.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/IU66V685/nn1199_947.html}
}

@book{Sarachik1997,
  address = {Cambridge},
  title = {Principles of Linear Systems},
  isbn = {978-0-521-57057-2 978-0-521-57606-2},
  publisher = {{Cambridge Univ. Press}},
  author = {Sarachik, Philip E.},
  year = {1997},
  keywords = {Linear systems,State-space methods,System analysis},
  file = {books/Sarachik1997_Principles-of-linear-systems.pdf;books/Sarachik1997_Principles-of-linear-systems2.pdf}
}

@article{Noguchi2011,
  title = {In Vivo Two-Photon Uncaging of Glutamate Revealing the Structure\textendash{}Function Relationships of Dendritic Spines in the Neocortex of Adult Mice},
  volume = {589},
  issn = {1469-7793},
  doi = {10.1113/jphysiol.2011.207100},
  abstract = {Non-technical summary\hspace{0.6em} Neurons communicate with each other with synapses using chemical messengers. The major synapses in the cerebral cortex utilize glutamate as a messenger and are made on special submicron structures, called dendritic spines. Dendritic spines are diverse in their size and densely packed in the cortex. Therefore, an optical technique for application of glutamate to single spines (two-photon (TP) uncaging) has been intensively used to clarify their functions in vitro. We have here extended 2P uncaging to living adult brain, and found that spine sizes display tight correlations with their functions, such as rapid glutamate sensing and an increase in cytosolic Ca2+ concentrations, even in vivo, as they were reported for in vitro preparations. Our data suggest that the structure and motility of dendritic spines play a key role in the adult brain function.},
  language = {en},
  number = {10},
  journal = {The Journal of Physiology},
  author = {Noguchi, Jun and Nagaoka, Akira and Watanabe, Satoshi and {Ellis-Davies}, Graham C. R. and Kitamura, Kazuo and Kano, Masanobu and Matsuzaki, Masanori and Kasai, Haruo},
  month = may,
  year = {2011},
  pages = {2447-2457},
  file = {articles/Noguchi2011.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/WF425SE2/abstract.html}
}

@phdthesis{Perin2010,
  title = {Emergent {{Dynamics}} in {{Neocortical Microcircuits}}},
  author = {Perin, Rodrigo},
  year = {2010},
  file = {thesis/Perin2010.pdf}
}

@article{Miller1999,
  title = {Is the Development of Orientation Selectivity Instructed by Activity?},
  volume = {41},
  copyright = {Copyright \textcopyright{} 1999 John Wiley \& Sons, Inc.},
  issn = {1097-4695},
  doi = {10.1002/(SICI)1097-4695(199910)41:1<44::AID-NEU7>3.0.CO;2-V},
  abstract = {Is the development of orientation selectivity in visual cortex instructed by the patterns of neural activity of input neurons? We review evidence as to the role of activity, review models of activity-instructed development, and discuss how these models can be tested. The models can explain the normal development of simple cells with binocularly matched orientation preferences, the effects of monocular deprivation and reverse suture on the orientation map, and the development of a full intracortical circuit sufficient to explain mature response properties including the contrast-invariance of orientation tuning. Existing experiments are consistent with the models, in that (a) selective blockade of ON-center ganglion cells, which will degrade or eliminate the information predicted to drive development of orientation selectivity, in fact prevents development of orientation selectivity; and (b) the spontaneous activities of inputs serving the two eyes are correlated in the lateral geniculate nucleus at appropriate developmental times, as was predicted to be required to achieve binocular matching of preferred orientations. However, definitive tests remain to be done to firmly establish the instructive rather than simply permissive role of activity and determine whether the retinotopically and center type\textendash{}specific patterns of activity predicted by the models actually exist. We conclude by critically examining alternative scenarios for the development of orientation selectivity and maps, including the idea that maps are genetically prespecified. \textcopyright{} 1999 John Wiley \& Sons, Inc. J Neurobiol 41: 44\textendash{}57, 1999},
  language = {en},
  number = {1},
  journal = {Journal of Neurobiology},
  author = {Miller, Kenneth D. and Erwin, Ed and Kayser, Andrew},
  month = oct,
  year = {1999},
  keywords = {Hebbian synaptic plasticity,LGN spontaneous activity,orientation maps,simple cells,visual cortex},
  pages = {44-57},
  file = {articles/Miller1999.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/ABFTHQ33/abstract\;jsessionid=B828DEC5AD129ED6573D3C3F5DF13D82.html}
}

@article{Hennig2013,
  title = {Theoretical Models of Synaptic Short Term Plasticity},
  volume = {7},
  doi = {10.3389/fncom.2013.00045},
  abstract = {Short term plasticity is a highly abundant form of rapid, activity-dependent modulation of synaptic efficacy. A shared set of mechanisms can cause both depression and enhancement of the postsynaptic response at different synapses, with important consequences for information processing. Mathematical models have been extensively used to study the mechanisms and roles of short term plasticity. This review provides an overview of existing models and their biological basis, and of their main properties. Special attention will be given to slow processes such as calcium channel inactivation and the effect of activation of presynaptic autoreceptors.},
  journal = {Frontiers in Computational Neuroscience},
  author = {Hennig, Matthias H.},
  year = {2013},
  keywords = {Synaptic Transmission,mathematical model,short term plasticity,synaptic depression,synaptic facilitation},
  pages = {45},
  file = {articles/Hennig2013.pdf}
}

@article{Lee2016,
  title = {Topology of {{ON}} and {{OFF}} Inputs in Visual Cortex Enables an Invariant Columnar Architecture},
  volume = {533},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature17941},
  number = {7601},
  journal = {Nature},
  author = {Lee, Kuo-Sheng and Huang, Xiaoying and Fitzpatrick, David},
  month = apr,
  year = {2016},
  pages = {90-94},
  file = {articles/Lee2016_2.pdf}
}

@book{Trappenberg2010,
  address = {Oxford ; New York},
  edition = {2nd ed},
  title = {Fundamentals of Computational Neuroscience},
  isbn = {978-0-19-956841-3},
  lccn = {QP357.5 .T746 2010},
  abstract = {"Computational neuroscience is the theoretical study of the brain to uncover the principles and mechanisms that guide the development, organization, information processing, and mental functions of the nervous system. Although not a new area, it is only recently that enough knowledge has been gathered to establish computational neuroscience as a scientific discipline in its own right. Given the complexity of the field, and its increasing importance in progressing our understanding of how the brain works, there has long been a need for an introductory text on what is often assumed to be an impenetrable topic. The new edition of Fundamentals of Computational Neuroscience build on the success and strengths of the first edition. It introduces the theoretical foundations of neuroscience with a focus on the nature of information processing in the brain. The book covers the introduction and motivation of simplified models of neurons that are suitable for exploring information processing in large brain-like networks. Additionally, it introduces several fundamental network architectures and discusses their relevance for information processing in the brain, giving some examples of models of higher-order cognitive functions to demonstrate the advanced insight that can be gained with such studies. Each chapter starts by introducing its topic with experimental facts and conceptual questions related to the study of brain function. An additional feature is the inclusion of simple Matlab programs that can be used to explore many of the mechanisms explained in the book. An accompanying webpage includes programs for download. The book is aimed at those within the brain and cognitive sciences, from graduate level and upwards"--Provided by publisher},
  publisher = {{Oxford University Press}},
  author = {Trappenberg, Thomas P.},
  year = {2010},
  keywords = {Brain,Computational Biology,Models; Neurological,Nerve Net,Neurons,Neurosciences,Physiology,computational neuroscience,methods},
  file = {books/Trappenberg2010_Fundamentals-of-computational-neuroscience.pdf}
}

@article{Tetzlaff2011,
  title = {Synaptic Scaling in Combination with Many Generic Plasticity Mechanisms Stabilizes Circuit Connectivity},
  volume = {5},
  doi = {10.3389/fncom.2011.00047},
  abstract = {Synaptic scaling is a slow process that modifies synapses, keeping the firing rate of neural circuits in specific regimes. Together with other processes, such as conventional synaptic plasticity in the form of long term depression and potentiation, synaptic scaling changes the synaptic patterns in a network, ensuring diverse, functionally relevant, stable, and input-dependent connectivity. How synaptic patterns are generated and stabilized, however, is largely unknown. Here we formally describe and analyze synaptic scaling based on results from experimental studies and demonstrate that the combination of different conventional plasticity mechanisms and synaptic scaling provides a powerful general framework for regulating network connectivity. In addition, we design several simple models that reproduce experimentally observed synaptic distributions as well as the observed synaptic modifications during sustained activity changes. These models predict that the combination of plasticity with scaling generates globally stable, input-controlled synaptic patterns, also in recurrent networks. Thus, in combination with other forms of plasticity, synaptic scaling can robustly yield neuronal circuits with high synaptic diversity, which potentially enables robust dynamic storage of complex activation patterns. This mechanism is even more pronounced when considering networks with a realistic degree of inhibition. Synaptic scaling combined with plasticity could thus be the basis for learning structured behavior even in initially random networks.},
  journal = {Frontiers in Computational Neuroscience},
  author = {Tetzlaff, Christian and Kolodziejski, Christoph and Timme, Marc and W\"org\"otter, Florentin},
  year = {2011},
  keywords = {homeostasis,neural network,plasticity,synapse},
  pages = {47},
  file = {articles/Tetzlaff2011.pdf}
}

@article{Hagihara2015,
  title = {Neuronal Activity Is Not Required for the Initial Formation and Maturation of Visual Selectivity},
  volume = {18},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4155},
  number = {12},
  journal = {Nature Neuroscience},
  author = {Hagihara, Kenta M and Murakami, Tomonari and Yoshida, Takashi and Tagawa, Yoshiaki and Ohki, Kenichi},
  month = nov,
  year = {2015},
  pages = {1780-1788},
  file = {articles/Hagihara20152.pdf}
}

@article{Hiratani2016a,
  title = {Hebbian {{Wiring Plasticity Generates Efficient Network Structures}} for {{Robust Inference}} with {{Synaptic Weight Plasticity}}},
  doi = {10.3389/fncir.2016.00041},
  abstract = {In the adult mammalian cortex, a small fraction of spines are created and eliminated every day, and the resultant synaptic connection structure is highly nonrandom, even in local circuits. However, it remains unknown whether a particular synaptic connection structure is functionally advantageous in local circuits, and why creation and elimination of synaptic connections is necessary in addition to rich synaptic weight plasticity. To answer these questions, we studied an inference task model through theoretical and numerical analyses. We demonstrate that a robustly beneficial network structure naturally emerges by combining Hebbian-type synaptic weight plasticity and wiring plasticity. Especially in a sparsely connected network, wiring plasticity achieves reliable computation by enabling efficient information transmission. Furthermore, the proposed rule reproduces experimental observed correlation between spine dynamics and task performance.},
  journal = {Frontiers in Neural Circuits},
  author = {Hiratani, Naoki and Fukai, Tomoki},
  year = {2016},
  keywords = {computational model,connectomics,neural decoding,synaptic plasticity,synaptogenesis},
  pages = {41},
  file = {articles/Hiratani2016.pdf}
}

@article{Jovanovic2015,
  title = {Cumulants of {{Hawkes}} Point Processes},
  volume = {91},
  doi = {10.1103/PhysRevE.91.042802},
  abstract = {We derive explicit, closed-form expressions for the cumulant densities of a multivariate, self-exciting Hawkes point process, generalizing a result of Hawkes in his earlier work on the covariance density and Bartlett spectrum of such processes. To do this, we represent the Hawkes process in terms of a Poisson cluster process and show how the cumulant density formulas can be derived by enumerating all possible ``family trees,'' representing complex interactions between point events. We also consider the problem of computing the integrated cumulants, characterizing the average measure of correlated activity between events of different types, and derive the relevant equations.},
  number = {4},
  journal = {Physical Review E},
  author = {Jovanovi\'c, Stojan and Hertz, John and Rotter, Stefan},
  month = apr,
  year = {2015},
  pages = {042802},
  file = {articles/Jovanović2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/48C3H7JM/PhysRevE.91.html}
}

@article{Gilson2009b,
  title = {Emergence of Network Structure Due to Spike-Timing-Dependent Plasticity in Recurrent Neuronal Networks. {{II}}. {{Input}} Selectivity\textemdash{}Symmetry Breaking},
  volume = {101},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-009-0320-y},
  abstract = {Spike-timing-dependent plasticity (STDP) is believed to structure neuronal networks by slowly changing the strengths (or weights) of the synaptic connections between neurons depending upon their spiking activity, which in turn modifies the neuronal firing dynamics. In this paper, we investigate the change in synaptic weights induced by STDP in a recurrently connected network in which the input weights are plastic but the recurrent weights are fixed. The inputs are divided into two pools with identical constant firing rates and equal within-pool spike-time correlations, but with no between-pool correlations. Our analysis uses the Poisson neuron model in order to predict the evolution of the input synaptic weights and focuses on the asymptotic weight distribution that emerges due to STDP. The learning dynamics induces a symmetry breaking for the individual neurons, namely for sufficiently strong within-pool spike-time correlation each neuron specializes to one of the input pools. We show that the presence of fixed excitatory recurrent connections between neurons induces a group symmetry-breaking effect, in which neurons tend to specialize to the same input pool. Consequently STDP generates a functional structure on the input connections of the network.},
  language = {en},
  number = {2},
  journal = {Biological Cybernetics},
  author = {Gilson, Matthieu and Burkitt, Anthony N. and Grayden, David B. and Thomas, Doreen A. and van Hemmen, J. Leo},
  month = jun,
  year = {2009},
  keywords = {Bioinformatics,Complexity,Computer Appl. in Life Sciences,Neurobiology,Neurosciences,Recurrent neuronal network,STDP,Spike-time correlation,Symmetry breaking,learning},
  pages = {103-114},
  file = {articles/Gilson2009_2.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/IM5DF9GS/s00422-009-0320-y.html}
}

@book{Bear2006,
  address = {Philadelphia, PA},
  edition = {3rd edition},
  title = {Neuroscience: {{Exploring}} the {{Brain}}, 3rd {{Edition}}},
  isbn = {978-0-7817-6003-4},
  shorttitle = {Neuroscience},
  abstract = {Widely praised for its student-friendly style and exceptional artwork and pedagogy, Neuroscience: Exploring the Brain is a leading undergraduate textbook on the biology of the brain and the systems that underlie behavior. This edition provides increased coverage of taste and smell, circadian rhythms, brain development, and developmental disorders and includes new information on molecular mechanisms and functional brain imaging. Path of Discovery boxes, written by leading researchers, highlight major current discoveries. In addition, readers will be able to assess their knowledge of neuroanatomy with the Illustrated Guide to Human Neuroanatomy, which includes a perforated self-testing workbook.This edition's robust ancillary package includes a bound-in student CD-ROM, an Instructor's Resource CD-ROM, and resources online.},
  language = {English},
  publisher = {{Lippincott Williams and Wilkins}},
  author = {Bear, Mark F. and Connors, Barry W. and Paradiso, Michael A.},
  month = feb,
  year = {2006},
  file = {books/Bear2006_Neuroscience-Exploring-the-Brain,-3rd-Edition.pdf}
}

@article{Okun2008,
  title = {Instantaneous Correlation of Excitation and Inhibition during Ongoing and Sensory-Evoked Activities},
  volume = {11},
  copyright = {\textcopyright{} 2008 Nature Publishing Group},
  issn = {1097-6256},
  doi = {10.1038/nn.2105},
  abstract = {Temporal and quantitative relations between excitatory and inhibitory inputs in the cortex are central to its activity, yet they remain poorly understood. In particular, a controversy exists regarding the extent of correlation between cortical excitation and inhibition. Using simultaneous intracellular recordings in pairs of nearby neurons in vivo, we found that excitatory and inhibitory inputs are continuously synchronized and correlated in strength during spontaneous and sensory-evoked activities in the rat somatosensory cortex.},
  language = {en},
  number = {5},
  journal = {Nature Neuroscience},
  author = {Okun, Michael and Lampl, Ilan},
  month = may,
  year = {2008},
  pages = {535-537},
  file = {articles/Okun2008.pdf}
}

@article{Liley1994,
  title = {Intracortical Connectivity of Pyramidal and Stellate Cells: Estimates of Synaptic Densities and Coupling Symmetry},
  volume = {5},
  issn = {0954-898X},
  shorttitle = {Intracortical Connectivity of Pyramidal and Stellate Cells},
  doi = {10.1088/0954-898X_5_2_004},
  abstract = {A method is outlined for estimating the the average number of synapses forming between cortical neurons as a function of their intercellular separation and the geometry of their dendritic and axonal arborization. Consideration is confined to the formation of local intracortical connections and to the case where the distribution of axonal and dendritic fibres has spherical symmetry. Parameters are deduced from quantitative anatomical studies in murine cortex. It is demonstrated that the majority of local connections forming within a given volume of isotropic cortex can be accounted for on the assumption that local connections between neurons form randomly. From these computations the symmetry of connection between neurons, the likely position for synapse formation on the dendritic tree and the relative synaptic densities attributable to long-and short-range interaction between excitatory and inhibitory neural subsets is determined. Local intracortical couplings appear to be highly asymmetric, and account for about 3200 synapses forming on pyramidal and stellate cells.},
  number = {2},
  journal = {Network: Computation in Neural Systems},
  author = {Liley, D T J and Wright, J J},
  month = jan,
  year = {1994},
  pages = {175-189},
  file = {articles/Liley1994.pdf}
}

@book{Buzsaki2006,
  address = {Oxford ; New York},
  title = {Rhythms of the Brain},
  isbn = {978-0-19-530106-9 978-0-19-982823-4},
  lccn = {QP376 .B88 2006},
  publisher = {{Oxford University Press}},
  author = {Buzs\'aki, G.},
  year = {2006},
  keywords = {Biological rhythms,Brain,Cortical Synchronization,Periodicity,Physiology,oscillations},
  file = {books/Buzsáki2006_Rhythms-of-the-brain.pdf},
  note = {OCLC: ocm63279497}
}

@unpublished{Hoffmann2015d,
  title = {Optimierung - {{Zusammenfassung}}},
  author = {Hoffmann, Felix},
  year = {2015},
  file = {manuscripts/Hoffmann2015_Optimierung---Zusammenfassung.pdf}
}

@article{Hong2012,
  title = {Single {{Neuron Firing Properties Impact Correlation}}-{{Based Population Coding}}},
  volume = {32},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3735-11.2012},
  language = {en},
  number = {4},
  journal = {Journal of Neuroscience},
  author = {Hong, S. and Ratte, S. and Prescott, S. A. and De Schutter, E.},
  month = jan,
  year = {2012},
  pages = {1413-1428},
  file = {articles/Hong2012.pdf}
}

@book{Rudin1991,
  address = {New York},
  edition = {2Rev Ed edition},
  title = {Functional {{Analysis}}},
  isbn = {978-0-07-100944-7},
  language = {English},
  publisher = {{McGraw-Hill Science/Engineerin}},
  author = {Rudin, Walter},
  year = {1991},
  file = {books/Rudin1991_Functional-Analysis.pdf}
}

@article{Petersen2003,
  title = {Spatiotemporal {{Dynamics}} of {{Sensory Responses}} in {{Layer}} 2/3 of {{Rat Barrel Cortex Measured In Vivo}} by {{Voltage}}-{{Sensitive Dye Imaging Combined}} with {{Whole}}-{{Cell Voltage Recordings}} and {{Neuron Reconstructions}}},
  volume = {23},
  issn = {0270-6474, 1529-2401},
  abstract = {The spatiotemporal dynamics of the sensory response in layer 2/3 of primary somatosensory cortex evoked by a single brief whisker deflection was investigated by simultaneous voltage-sensitive dye (VSD) imaging and whole-cell (WC) voltage recordings in the anesthetized rat combined with reconstructions of dendritic and axonal arbors of L2/3 pyramids. Single and dual WC recordings from pyramidal cells indicated a strong correlation between the local VSD population response and the simultaneously measured subthreshold postsynaptic potential changes in both amplitude and time course. The earliest VSD response was detected 10\textendash{}12 msec after whisker deflection centered above the barrel isomorphic to the stimulated principal whisker. It was restricted horizontally to the size of a single barrel-column coextensive with the dendritic arbor of barrel-column-related pyramids in L2/3. The horizontal spread of excitation remained confined to a single barrel-column with weak whisker deflection. With intermediate deflections, excitation spread into adjacent barrel-columns, propagating twofold more rapidly along the rows of the barrel field than across the arcs, consistent with the preferred axonal arborizations in L2/3 of reconstructed pyramidal neurons. Finally, larger whisker deflections evoked excitation spreading over the entire barrel field within $\sim$50 msec before subsiding over the next $\sim$250 msec. Thus the subthreshold cortical map representing a whisker deflection is dynamic on the millisecond time scale and strongly depends on stimulus strength. The sequential spatiotemporal activation of the excitatory neuronal network in L2/3 by a simple sensory stimulus can thus be accounted for primarily by the columnar restriction of L4 to L2/3 excitatory connections and the axonal field of barrel-related pyramids.},
  language = {en},
  number = {4},
  journal = {The Journal of Neuroscience},
  author = {Petersen, Carl C. H. and Grinvald, Amiram and Sakmann, Bert},
  month = feb,
  year = {2003},
  keywords = {barrel cortex,imaging,in vivo,layer 2/3,sensory response,voltage-sensitive dye},
  pages = {1298-1309},
  file = {articles/Petersen2003.pdf},
  pmid = {12598618}
}

@article{Caze2015,
  title = {Synaptic Clustering or Scattering? {{Insight}} from a Model of Synaptic Plasticity in Dendrites},
  copyright = {\textcopyright{} 2015, Published by Cold Spring Harbor Laboratory Press. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  shorttitle = {Synaptic Clustering or Scattering?},
  doi = {10.1101/029330},
  abstract = {A large body of theoretical work has shown how dendrites can increase the computational capacity of the neuron. This work predicted that synapses active together should be close together in space, a phenomenon called synaptic clustering. Experimental evidence has shown that, in the absence of sensory stimulation, synapses nearby on the same dendrite tend to be active together more than expected by chance. Synaptic clustering, however, does not seem to be ubiquitous: other groups have reported that nearby synapses can respond to different features of a stimulus during sensory evoked activity. In other words, synapses that are active together during sensory evoked activity can be far apart in space, a phenomenon we term synaptic scattering. To unify these apparently inconsistent experimental results, we use a computational framework to study the formation of a synaptic architecture -- a set of synaptic weights -- displaying both synaptic clustering and scattering. We present three conditions under which a neuron can learn such synaptic architecture: (i) presynaptic inputs are organized into correlated groups of neurons; (ii) the postsynaptic neuron is compartmentalized in subunits representing dendrites; and (iii) the synaptic plasticity rule is local within a subunit. Importantly, we show that given the same synaptic architecture, synaptic clustering is expressed during spontaneous activity, i.e. in the absence of sensory evoked activity, whereas synaptic scattering is present under evoked activity. Interestingly, reduced dendritic morphology in our model leads to a pathological hyper-excitability, as observed for instance in Alzheimer's Disease. This work therefore unifies a seemingly contradictory set of experimental observations: we demonstrate that the same synaptic architecture can lead to synaptic clustering and scattering depending on the input structure.},
  language = {en},
  journal = {bioRxiv},
  author = {Caze, Romain Daniel and Clopath, Claudia and Schultz, Simon R.},
  month = oct,
  year = {2015},
  pages = {029330},
  file = {articles/Caze2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/5JV6KQCM/029330.html}
}

@article{Trousdale2012,
  title = {Impact of {{Network Structure}} and {{Cellular Response}} on {{Spike Time Correlations}}},
  volume = {8},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002408},
  abstract = {Author Summary          Is neural activity more than the sum of its individual parts? What is the impact of cooperative, or  correlated , spiking among multiple cells? We can start addressing these questions, as rapid advances in experimental techniques allow simultaneous recordings from ever-increasing populations. However, we still lack a general understanding of the origin and consequences of the joint activity that is revealed. The challenge is compounded by the fact that both the intrinsic dynamics of single cells and the correlations among then vary depending on the overall state of the network. Here, we develop a toolbox that addresses this issue. Specifically, we show how  linear response theory  allows for the expression of correlations explicitly in terms of the underlying network connectivity and known single-cell properties \textendash{} and that the predictions of this theory accurately match simulations of a touchstone, nonlinear model in computational neuroscience, the general integrate-and-fire cell. Thus, our theory should help unlock the relationship between network architecture, single-cell dynamics, and correlated activity in diverse neural circuits.},
  number = {3},
  journal = {PLOS Comput Biol},
  author = {Trousdale, James and Hu, Yu and {Shea-Brown}, Eric and Josi\'c, Kre{\v s}imir},
  month = mar,
  year = {2012},
  keywords = {Action potentials,Convolution,Fourier analysis,Membrane potential,Neurons,Synapses,White noise,neural networks},
  pages = {e1002408},
  file = {articles/Trousdale2012.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/AWS46G94/article.html}
}

@article{Gidon2012,
  title = {Principles {{Governing}} the {{Operation}} of {{Synaptic Inhibition}} in {{Dendrites}}},
  volume = {75},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2012.05.015},
  abstract = {Summary
Synaptic inhibition plays a key role in shaping the dynamics of neuronal networks and selecting cell assemblies. Typically, an inhibitory axon contacts a particular dendritic subdomain of its target neuron, where it often makes 10\textendash{}20 synapses, sometimes on very distal branches. The functional implications of such a connectivity pattern are not well understood. Our experimentally based theoretical study highlights several new and counterintuitive principles for dendritic inhibition. We show that distal ``off-path'' rather than proximal ``on-path'' inhibition effectively dampens proximal excitable dendritic ``hotspots,'' thus powerfully controlling the neuron's output. Additionally, with multiple synaptic contacts, inhibition operates globally, spreading centripetally hundreds of micrometers from the inhibitory synapses. Consequently, inhibition in regions lacking inhibitory synapses may exceed that at the synaptic sites themselves. These results offer new insights into the synergetic effect of dendritic inhibition in controlling dendritic excitability and plasticity and in dynamically molding functional dendritic subdomains and their output.},
  number = {2},
  journal = {Neuron},
  author = {Gidon, Albert and Segev, Idan},
  month = jul,
  year = {2012},
  pages = {330-341},
  file = {articles/Gidon2012.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/TRNR34ZS/S0896627312004813.html}
}

@book{Neuroscience2002,
  address = {Washington, D.C},
  edition = {Fourth Edition edition},
  title = {Brain {{Facts}}: {{A Primer}} on the {{Brain}} and {{Nervous System}}},
  isbn = {978-0-916110-00-0},
  shorttitle = {Brain {{Facts}}},
  abstract = {Book by Society for Neuroscience},
  language = {English},
  publisher = {{The Society For Neuroscience}},
  author = {for Neuroscience, The Society},
  year = {2002},
  file = {books/Neuroscience2002_Brain-Facts-A-Primer-on-the-Brain-and-Nervous-System.pdf}
}

@article{Sadeh2015a,
  title = {Emergence of {{Functional Specificity}} in {{Balanced Networks}} with {{Synaptic Plasticity}}},
  volume = {11},
  doi = {10.1371/journal.pcbi.1004307},
  abstract = {Author Summary In primary visual cortex of mammals, neurons are selective to the orientation of contrast edges. In some species, as cats and monkeys, neurons preferring similar orientations are adjacent on the cortical surface, leading to smooth orientation maps. In rodents, in contrast, such spatial orientation maps do not exist, and neurons of different specificities are mixed in a salt-and-pepper fashion. During development, however, a ``functional'' map of orientation selectivity emerges, where connections between neurons of similar preferred orientations are selectively enhanced. Here we show how such feature-specific connectivity can arise in realistic neocortical networks of excitatory and inhibitory neurons. Our results demonstrate how recurrent dynamics can work in cooperation with synaptic plasticity to form networks where neurons preferring similar stimulus features connect more strongly together. Such networks, in turn, are known to enhance the specificity of neuronal responses to a stimulus. Our study thus reveals how self-organizing connectivity in neuronal networks enable them to achieve new or enhanced functions, and it underlines the essential role of recurrent inhibition and plasticity in this process.},
  number = {6},
  journal = {PLoS Comput Biol},
  author = {Sadeh, Sadra and Clopath, Claudia and Rotter, Stefan},
  month = jun,
  year = {2015},
  pages = {e1004307},
  file = {articles/Sadeh2015.pdf}
}

@article{Markram1998,
  title = {Differential Signaling via the Same Axon of Neocortical Pyramidal Neurons},
  volume = {95},
  issn = {0027-8424, 1091-6490},
  abstract = {The nature of information stemming from a single neuron and conveyed simultaneously to several hundred target neurons is not known. Triple and quadruple neuron recordings revealed that each synaptic connection established by neocortical pyramidal neurons is potentially unique. Specifically, synaptic connections onto the same morphological class differed in the numbers and dendritic locations of synaptic contacts, their absolute synaptic strengths, as well as their rates of synaptic depression and recovery from depression. The same axon of a pyramidal neuron innervating another pyramidal neuron and an interneuron mediated frequency-dependent depression and facilitation, respectively, during high frequency discharges of presynaptic action potentials, suggesting that the different natures of the target neurons underlie qualitative differences in synaptic properties. Facilitating-type synaptic connections established by three pyramidal neurons of the same class onto a single interneuron, were all qualitatively similar with a combination of facilitation and depression mechanisms. The time courses of facilitation and depression, however, differed for these convergent connections, suggesting that different pre-postsynaptic interactions underlie quantitative differences in synaptic properties. Mathematical analysis of the transfer functions of frequency-dependent synapses revealed supra-linear, linear, and sub-linear signaling regimes in which mixtures of presynaptic rates, integrals of rates, and derivatives of rates are transferred to targets depending on the precise values of the synaptic parameters and the history of presynaptic action potential activity. Heterogeneity of synaptic transfer functions therefore allows multiple synaptic representations of the same presynaptic action potential train and suggests that these synaptic representations are regulated in a complex manner. It is therefore proposed that differential signaling is a key mechanism in neocortical information processing, which can be regulated by selective synaptic modifications.},
  language = {en},
  number = {9},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Markram, Henry and Wang, Yun and Tsodyks, Misha},
  month = apr,
  year = {1998},
  pages = {5323-5328},
  file = {articles/Markram1998.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/JZAV2BJ6/5323.html},
  pmid = {9560274}
}

@book{Kandel2012,
  address = {New York},
  edition = {5th edition},
  title = {Principles of {{Neural Science}}, {{Fifth Edition}}},
  isbn = {978-0-07-139011-8},
  abstract = {Now updated: the definitive neuroscience resource\textemdash{}from Eric R. Kandel, MD (winner of the Nobel Prize in 2000); James H. Schwartz, MD, PhD; Thomas M. Jessell, PhD; Steven A. Siegelbaum, PhD; and A. J. Hudspeth, PhD 900 full-color illustrations Deciphering the link between the human brain and behavior has always been one of the most intriguing\textemdash{}and often challenging\textemdash{}aspects of scientific endeavor. The sequencing of the human genome, and advances in molecular biology, have illuminated the pathogenesis of many neurological diseases and have propelled our knowledge of how the brain controls behavior.  To grasp the wider implications of these developments and gain a fundamental understanding of this dynamic, fast-moving field, Principles of Neuroscience stands alone as the most authoritative and indispensible resource of its kind.  In this classic text, prominent researchers in the field expertly survey the entire spectrum of neural science, giving an up-to-date, unparalleled view of the discipline for anyone who studies brain and mind. Here, in one remarkable volume, is the current state of neural science knowledge\textemdash{}ranging from molecules and cells, to anatomic structures and systems, to the senses and cognitive functions\textemdash{}all supported by more than 900 precise, full-color illustrations. In addition to clarifying complex topics, the book also benefits from a cohesive organization, beginning with an insightful overview of the interrelationships between the brain, nervous system, genes, and behavior. Principles of Neural Science then proceeds with an in-depth examination of the molecular and cellular biology of nerve cells, synaptic transmission, and the neural basis of cognition. The remaining sections illuminate how cells, molecules, and systems give us sight, hearing, touch, movement, thought, learning, memories, and emotions.  The new fifth edition of Principles of Neural Science is thoroughly updated to reflect the tremendous amount of research, and the very latest clinical perspectives, that have significantly transformed the field within the last decade.  Ultimately, Principles of Neural Science affirms that all behavior is an expression of neural activity, and that the future of clinical neurology and psychiatry hinges on the progress of neural science. Far exceeding the scope and scholarship of similar texts, this unmatched guide offers a commanding, scientifically rigorous perspective on the molecular mechanisms of neural function and disease\textemdash{}one that you'll continually rely on to advance your comprehension of brain, mind, and behavior.  FEATURES   The cornerstone reference in the field of neuroscience that explains how the nerves, brain, and mind function  Clear emphasis on how behavior can be examined through the electrical activity of both individual neurons and systems of nerve cells  Current focus on molecular biology as a tool for probing the pathogenesis of many neurological diseases, including muscular dystrophy, Huntington disease, and certain forms of Alzheimer's disease  More than 900 engaging full-color illustrations\textemdash{}including line drawings, radiographs, micrographs, and medical photographs clarify often-complex neuroscience concepts  Outstanding section on the development and emergence of behavior, including important coverage of brain damage repair, the sexual differentiation of the nervous system,  and the aging brain  NEW! More detailed discussions of cognitive and behavioral functions, and an expanded review of cognitive processes  NEW! A focus on the increasing importance of computational neural science, which enhances our ability to record the brain's electrical activity and study cognitive processes more directly  NEW! Chapter-opening Key Concepts provide a convenient, study-enhancing introduction to the material covered in each chapter  Selected Readings and full reference citations at the close of each chapter facilitate further study and research  Helpful appendices highlight basic circuit theory; the neurological examination of the patient; circulation of the brain; the blood-brain barrier, choroid plexus, and cerebrospinal fluid; neural networks; and theoretical approaches to neuroscience/ul$>$},
  language = {English},
  publisher = {{McGraw-Hill Professional}},
  editor = {Kandel, Eric R. and Schwartz, James H. and Jessell, Thomas M. and Siegelbaum, Steven A. and Hudspeth, A. J.},
  month = oct,
  year = {2012},
  file = {books/Kandel2012_Principles-of-Neural-Science,-Fifth-Edition.epub;books/Kandel2012_Principles-of-Neural-Science,-Fifth-Edition.pdf}
}

@article{Perin2011,
  title = {A Synaptic Organizing Principle for Cortical Neuronal Groups},
  volume = {108},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1016051108},
  abstract = {Neuronal circuitry is often considered a clean slate that can be dynamically and arbitrarily molded by experience. However, when we investigated synaptic connectivity in groups of pyramidal neurons in the neocortex, we found that both connectivity and synaptic weights were surprisingly predictable. Synaptic weights follow very closely the number of connections in a group of neurons, saturating after only 20\% of possible connections are formed between neurons in a group. When we examined the network topology of connectivity between neurons, we found that the neurons cluster into small world networks that are not scale-free, with less than 2 degrees of separation. We found a simple clustering rule where connectivity is directly proportional to the number of common neighbors, which accounts for these small world networks and accurately predicts the connection probability between any two neurons. This pyramidal neuron network clusters into multiple groups of a few dozen neurons each. The neurons composing each group are surprisingly distributed, typically more than 100 $\mu$m apart, allowing for multiple groups to be interlaced in the same space. In summary, we discovered a synaptic organizing principle that groups neurons in a manner that is common across animals and hence, independent of individual experiences. We speculate that these elementary neuronal groups are prescribed Lego-like building blocks of perception and that acquired memory relies more on combining these elementary assemblies into higher-order constructs.},
  language = {en},
  number = {13},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Perin, Rodrigo and Berger, Thomas K. and Markram, Henry},
  month = mar,
  year = {2011},
  keywords = {Edelman,Hebb,bidirs,brain development,cell assemblies,learning},
  pages = {5419-5424},
  file = {articles/Perin2011_commentary.pdf;articles/Perin2011_SI.pdf;articles/Perin2011.pdf},
  pmid = {21383177}
}

@article{Romand2011,
  title = {Morphological {{Development}} of {{Thick}}-{{Tufted Layer V Pyramidal Cells}} in the {{Rat Somatosensory Cortex}}},
  volume = {5},
  issn = {1662-5129},
  doi = {10.3389/fnana.2011.00005},
  abstract = {The thick-tufted layer V pyramidal (TTL5) neuron is a key neuron providing output from the neocortex. Although it has been extensively studied, principles governing its dendritic and axonal arborization during development are still not fully quantified. Using 3-D model neurons reconstructed from biocytin-labeled cells in the rat somatosensory cortex, this study provides a detailed morphological analysis of TTL5 cells at postnatal day (P) 7, 14, 21, 36, and 60. Three developmental periods were revealed, which were characterized by distinct growing rates and properties of alterations in different compartments. From P7 to P14, almost all compartments grew fast, and filopodia-like segments along apical dendrite disappeared; From P14 to P21, the growth was localized on specified segments of each compartment, and the densities of spines and boutons were significantly increased; From P21 to P60, the number of basal dendritic segments was significantly increased at specified branch orders, and some basal and oblique dendritic segments were lengthened or thickened. Development changes were therefore seen in two modes: the fast overall growth during the first period and the slow localized growth (thickening mainly on intermediates or lengthening mainly on terminals) at the subsequent stages. The lengthening may be accompanied by the retraction on different segments. These results reveal a differential regulation in the arborization of neuronal compartments during development, supporting the notion of functional compartmental development. This quantification provides new insight into the potential value of the TTL5 morphology for information processing, and for other purposes as well.},
  journal = {Frontiers in Neuroanatomy},
  author = {Romand, Sandrine and Wang, Yun and {Toledo-Rodriguez}, Maria and Markram, Henry},
  month = feb,
  year = {2011},
  file = {articles/Romand2011.pdf},
  pmid = {21369363},
  pmcid = {PMC3043270}
}

@article{Sporns2004,
  title = {Motifs in {{Brain Networks}}},
  volume = {2},
  doi = {10.1371/journal.pbio.0020369},
  abstract = {Analysis of characteristic patterns of connectivity in neuroanatomical datasets suggests that nervous systems evolved to maximize functional repertoires and support highly efficient integration of information.},
  number = {11},
  journal = {PLoS Biol},
  author = {Sporns, Olaf and K\"otter, Rolf},
  month = oct,
  year = {2004},
  pages = {e369},
  file = {articles/Sporns2004.pdf}
}

@article{Watt2010,
  title = {Homeostatic {{Plasticity}} and {{STDP}}: {{Keeping}} a {{Neuron}}'s {{Cool}} in a {{Fluctuating World}}},
  volume = {2},
  issn = {1663-3563},
  shorttitle = {Homeostatic {{Plasticity}} and {{STDP}}},
  doi = {10.3389/fnsyn.2010.00005},
  abstract = {Spike-timing-dependent plasticity (STDP) offers a powerful means of forming and modifying neural circuits. Experimental and theoretical studies have demonstrated its potential usefulness for functions as varied as cortical map development, sharpening of sensory receptive fields, working memory, and associative learning. Even so, it is unlikely that STDP works alone. Unless changes in synaptic strength are coordinated across multiple synapses and with other neuronal properties, it is difficult to maintain the stability and functionality of neural circuits. Moreover, there are certain features of early postnatal development (e.g., rapid changes in sensory input) that threaten neural circuit stability in ways that STDP may not be well placed to counter. These considerations have led researchers to investigate additional types of plasticity, complementary to STDP, that may serve to constrain synaptic weights and/or neuronal firing. These are collectively known as ``homeostatic plasticity'' and include schemes that control the total synaptic strength of a neuron, that modulate its intrinsic excitability as a function of average activity, or that make the ability of synapses to undergo Hebbian modification depend upon their history of use. In this article, we will review the experimental evidence for homeostatic forms of plasticity and consider how they might interact with STDP during development, and learning and memory.},
  journal = {Frontiers in Synaptic Neuroscience},
  author = {Watt, Alanna J. and Desai, Niraj S.},
  month = jun,
  year = {2010},
  file = {articles/Watt2010.pdf},
  pmid = {21423491},
  pmcid = {PMC3059670}
}

@article{Lerchner2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.00411},
  primaryClass = {q-bio},
  title = {A Unifying Framework for Understanding State-Dependent Network Dynamics in Cortex},
  abstract = {Activity in neocortex exhibits a range of behaviors, from irregular to temporally precise, and from weakly to strongly correlated. So far there has been no single theoretical framework that could explain all these behaviors, leaving open the possibility that they are a signature of radically different mechanisms. Here, we suggest that this is not the case. Instead, we show that a single theory can account for a broad spectrum of experimental observations, including specifics such as the fine temporal details of subthreshold cross-correlations. For the model underlying our theory, we need only assume a small number of well-established properties common to all local cortical networks. When these assumptions are combined with realistically structured input, they produce exactly the repertoire of behaviors that is observed experimentally, and lead to a number of testable predictions.},
  journal = {arXiv:1511.00411 [q-bio]},
  author = {Lerchner, Alexander and Latham, Peter E.},
  month = nov,
  year = {2015},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {articles/Lerchner2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/XVKVFFQ8/1511.html}
}

@article{Davison2012,
  title = {Automated {{Capture}} of {{Experiment Context}} for {{Easier Reproducibility}} in {{Computational Research}}},
  volume = {14},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2012.41},
  abstract = {Published scientific research that relies on numerical computations is too often not reproducible. For computational research to become consistently and reliably reproducible, the process must become easier to achieve, as part of day-to-day research. A combination of best practices and automated tools can make it easier to create reproducible research.},
  number = {4},
  journal = {Computing in Science \& Engineering},
  author = {Davison, Andrew},
  month = jul,
  year = {2012},
  pages = {48-56},
  file = {articles/Davison2012.pdf}
}

@article{Feldman2009,
  title = {Synaptic {{Mechanisms}} for {{Plasticity}} in {{Neocortex}}},
  volume = {32},
  issn = {0147-006X},
  doi = {10.1146/annurev.neuro.051508.135516},
  abstract = {Sensory experience and learning alter sensory representations in cerebral cortex. The synaptic mechanisms underlying sensory cortical plasticity have long been sought. Recent work indicates that long-term cortical plasticity is a complex, multicomponent process involving multiple synaptic and cellular mechanisms. Sensory use, disuse, and training drive long-term potentiation and depression (LTP and LTD), homeostatic synaptic plasticity and plasticity of intrinsic excitability, and structural changes including formation, removal, and morphological remodeling of cortical synapses and dendritic spines. Both excitatory and inhibitory circuits are strongly regulated by experience. This review summarizes these findings and proposes that these mechanisms map onto specific functional components of plasticity, which occur in common across the primary somatosensory, visual, and auditory cortices.},
  journal = {Annual review of neuroscience},
  author = {Feldman, Daniel E.},
  year = {2009},
  pages = {33-55},
  file = {articles/Feldman2009.pdf},
  pmid = {19400721},
  pmcid = {PMC3071739}
}

@article{Vreeswijk1996,
  title = {Chaos in {{Neuronal Networks}} with {{Balanced Excitatory}} and {{Inhibitory Activity}}},
  volume = {274},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.274.5293.1724},
  abstract = {Neurons in the cortex of behaving animals show temporally irregular spiking patterns. The origin of this irregularity and its implications for neural processing are unknown. The hypothesis that the temporal variability in the firing of a neuron results from an approximate balance between its excitatory and inhibitory inputs was investigated theoretically. Such a balance emerges naturally in large networks of excitatory and inhibitory neuronal populations that are sparsely connected by relatively strong synapses. The resulting state is characterized by strongly chaotic dynamics, even when the external inputs to the network are constant in time. Such a network exhibits a linear response, despite the highly nonlinear dynamics of single neurons, and reacts to changing external stimuli on time scales much smaller than the integration time constant of a single neuron.},
  language = {en},
  number = {5293},
  journal = {Science},
  author = {van Vreeswijk, C. and Sompolinsky, H.},
  month = jun,
  year = {1996},
  pages = {1724-1726},
  file = {articles/Vreeswijk1996.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/D7S6AC8N/1724.html},
  pmid = {8939866}
}

@book{Gerstner2014,
  address = {Cambridge, United Kingdom},
  title = {Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition},
  isbn = {978-1-107-06083-8 978-1-107-63519-7},
  lccn = {QP363 .G474 2014},
  shorttitle = {Neuronal Dynamics},
  publisher = {{Cambridge University Press}},
  author = {Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam},
  year = {2014},
  keywords = {Cognitive neuroscience,Neural networks (Neurobiology),Neurobiology},
  file = {books/Gerstner2014_Neuronal-dynamics-from-single-neurons-to-networks-and-models-of-cognition.pdf}
}

@article{Bi2001,
  title = {Synaptic Modification by Correlated Activity : {{Hebb}}'s {{Postulate}} Revisited},
  volume = {24},
  shorttitle = {{{SYNAPTIC MODIFICATION BY CORRELATED ACTIVITY}}},
  doi = {10.1146/annurev.neuro.24.1.139},
  abstract = {Correlated spiking of pre- and postsynaptic neurons can result in strengthening or weakening of synapses, depending on the temporal order of spiking. Recent findings indicate that there are narrow and cell type\textendash{}specific temporal windows for such synaptic modification and that the generally accepted input- (or synapse-) specific rule for modification appears not to be strictly adhered to. Spike timing\textendash{}dependent modifications, together with selective spread of synaptic changes, provide a set of cellular mechanisms that are likely to be important for the development and functioning of neural networks. When an axon of cell A is near enough to excite cell B or repeatedly or consistently takes part in firing it, some growth or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased. Donald Hebb (1949)},
  number = {1},
  journal = {Annual Review of Neuroscience},
  author = {Bi, Guo-qiang and Poo, Mu-ming},
  year = {2001},
  keywords = {Hebbian synapse,LTD,LTP,input specificity,spike timing},
  pages = {139-166},
  file = {articles/Bi2001.pdf},
  pmid = {11283308}
}

@article{George2015,
  title = {Modeling the Interplay between Structural Plasticity and Spike-Timing-Dependent Plasticity},
  volume = {16},
  issn = {1471-2202},
  doi = {10.1186/1471-2202-16-S1-P107},
  number = {1},
  journal = {BMC Neuroscience},
  author = {George, Richard M. and Diehl, Peter U. and Cook, Matthew and Mayr, Christian and Indiveri, Giacomo},
  year = {2015},
  pages = {1-2},
  file = {articles/George2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/EFIMD745/1471-2202-16-S1-P107.html}
}

@book{Sporns2010,
  address = {Cambridge, Mass},
  edition = {1 edition},
  title = {Networks of the {{Brain}}},
  isbn = {978-0-262-01469-4},
  abstract = {Over the last decade, the study of complex networks has expanded across                 diverse scientific fields. Increasingly, science is concerned with the structure,                 behavior, and evolution of complex systems ranging from cells to ecosystems. Modern                 network approaches are beginning to reveal fundamental principles of brain                 architecture and function, and in Networks of the Brain, Olaf                 Sporns describes how the integrative nature of brain function can be illuminated                 from a complex network perspective. Highlighting the many emerging points of contact                 between neuroscience and network science, the book serves to introduce network                 theory to neuroscientists and neuroscience to those working on theoretical network                 models.Brain networks span the microscale of individual cells and                 synapses and the macroscale of cognitive systems and embodied cognition. Sporns                 emphasizes how networks connect levels of organization in the brain and how they                 link structure to function. In order to keep the book accessible and focused on the                 relevance to neuroscience of network approaches, he offers an informal and                 nonmathematical treatment of the subject. After describing the basic concepts of                 network theory and the fundamentals of brain connectivity, Sporns discusses how                 network approaches can reveal principles of brain architecture. He describes new                 links between network anatomy and function and investigates how networks shape                 complex brain dynamics and enable adaptive neural computation. The book documents                 the rapid pace of discovery and innovation while tracing the historical roots of the                 field. The study of brain connectivity has already opened new                 avenues of study in neuroscience. Networks of the Brain offers a                 synthesis of the sciences of complex networks and the brain that will be an                 essential foundation for future research.},
  language = {English},
  publisher = {{The MIT Press}},
  author = {Sporns, Olaf},
  month = oct,
  year = {2010},
  file = {books/Sporns2010_Networks-of-the-Brain.pdf}
}

@unpublished{Goette2011,
  title = {Topologie \& {{Algebraische Topologie}}},
  author = {Goette, Sebastian},
  year = {2011},
  file = {manuscripts/Goette2011_Topologie-&-Algebraische-Topologie.pdf}
}

@article{Bourjaily2011,
  title = {Excitatory, Inhibitory, and Structural Plasticity Produce Correlated Connectivity in Random Networks Trained to Solve Paired-Stimulus Tasks},
  volume = {5},
  doi = {10.3389/fncom.2011.00037},
  abstract = {The pattern of connections among cortical excitatory cells with overlapping arbors is non-random. In particular, correlations among connections produce clustering \textendash{} cells in cliques connect to each other with high probability, but with lower probability to cells in other spatially intertwined cliques. In this study, we model initially randomly connected sparse recurrent networks of spiking neurons with random, overlapping inputs, to investigate what functional and structural synaptic plasticity mechanisms sculpt network connections into the patterns measured in vitro. Our Hebbian implementation of structural plasticity causes a removal of connections between uncorrelated excitatory cells, followed by their random replacement. To model a biconditional discrimination task, we stimulate the network via pairs (A + B, C + D, A + D, and C + B) of four inputs (A, B, C, and D). We find networks that produce neurons most responsive to specific paired inputs \textendash{} a building block of computation and essential role for cortex \textendash{} contain the excessive clustering of excitatory synaptic connections observed in cortical slices. The same networks produce the best performance in a behavioral readout of the networks' ability to complete the task. A plasticity mechanism operating on inhibitory connections, long-term potentiation of inhibition, when combined with structural plasticity, indirectly enhances clustering of excitatory cells via excitatory connections. A rate-dependent (triplet) form of spike-timing-dependent plasticity (STDP) between excitatory cells is less effective and basic STDP is detrimental. Clustering also arises in networks stimulated with single stimuli and in networks undergoing raised levels of spontaneous activity when structural plasticity is combined with functional plasticity. In conclusion, spatially intertwined clusters or cliques of connected excitatory cells can arise via a Hebbian form of structural plasticity operating in initially randomly connected networks.},
  journal = {Frontiers in Computational Neuroscience},
  author = {Bourjaily, Mark A. and Miller, Paul},
  year = {2011},
  keywords = {Hebbian learning,STDP,connectivity,correlations,inhibitory plasticity,network,simulation,structural plasticity},
  pages = {37},
  file = {articles/Bourjaily2011.pdf}
}

@article{Jahn2012,
  title = {Molecular Machines Governing Exocytosis of Synaptic Vesicles},
  volume = {490},
  copyright = {\textcopyright{} 2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {0028-0836},
  doi = {10.1038/nature11320},
  abstract = {Calcium-dependent exocytosis of synaptic vesicles mediates the release of neurotransmitters. Important proteins in this process have been identified such as the SNAREs, synaptotagmins, complexins, Munc18 and Munc13. Structural and functional studies have yielded a wealth of information about the physiological role of these proteins. However, it has been surprisingly difficult to arrive at a unified picture of the molecular sequence of events from vesicle docking to calcium-triggered membrane fusion. Using mainly a biochemical and biophysical perspective, we briefly survey the molecular mechanisms in an attempt to functionally integrate the key proteins into the emerging picture of the neuronal fusion machine.},
  language = {en},
  number = {7419},
  journal = {Nature},
  author = {Jahn, Reinhard and Fasshauer, Dirk},
  month = oct,
  year = {2012},
  keywords = {Biochemistry,Cell biology,Neuroscience},
  pages = {201-207},
  file = {articles/Jahn2012.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/4VMKXNSM/nature11320.html}
}

@article{Sjostrom2008,
  title = {Dendritic {{Excitability}} and {{Synaptic Plasticity}}},
  volume = {88},
  copyright = {Copyright \textcopyright{} 2008 the American Physiological Society},
  issn = {0031-9333, 1522-1210},
  doi = {10.1152/physrev.00016.2007},
  abstract = {Most synaptic inputs are made onto the dendritic tree. Recent work has shown that dendrites play an active role in transforming synaptic input into neuronal output and in defining the relationships between active synapses. In this review, we discuss how these dendritic properties influence the rules governing the induction of synaptic plasticity. We argue that the location of synapses in the dendritic tree, and the type of dendritic excitability associated with each synapse, play decisive roles in determining the plastic properties of that synapse. Furthermore, since the electrical properties of the dendritic tree are not static, but can be altered by neuromodulators and by synaptic activity itself, we discuss how learning rules may be dynamically shaped by tuning dendritic function. We conclude by describing how this reciprocal relationship between plasticity of dendritic excitability and synaptic plasticity has changed our view of information processing and memory storage in neuronal networks.},
  language = {en},
  number = {2},
  journal = {Physiological Reviews},
  author = {Sj\"ostr\"om, P. Jesper and Rancz, Ede A. and Roth, Arnd and H\"ausser, Michael},
  month = apr,
  year = {2008},
  pages = {769-840},
  file = {articles/Sjöström2008.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/V2SGFE8S/769.html},
  pmid = {18391179},
  note = {Most synaptic inputs are made onto the dendritic tree. Recent work has shown that dendrites play an active role in transforming synaptic input into neuronal output and in defining the relationships between active synapses. In this review, we discuss how these dendritic properties influence the rules governing the induction of synaptic plasticity. We argue that the location of synapses in the dendritic tree, and the type of dendritic excitability associated with each synapse, play decisive roles in determining the plastic properties of that synapse. Furthermore, since the electrical properties of the dendritic tree are not static, but can be altered by neuromodulators and by synaptic activity itself, we discuss how learning rules may be dynamically shaped by tuning dendritic function. We conclude by describing how this reciprocal relationship between plasticity of dendritic excitability and synaptic plasticity has changed our view of information processing and memory storage in neuronal networks.}
}

@book{Batschelet1981,
  address = {London; New York},
  title = {Circular Statistics in Biology},
  isbn = {0-12-081050-6 978-0-12-081050-5},
  language = {English},
  publisher = {{Academic Press}},
  author = {Batschelet, Edward},
  year = {1981}
}

@article{Gilson2009a,
  title = {Emergence of Network Structure Due to Spike-Timing-Dependent Plasticity in Recurrent Neuronal Networks {{III}}: {{Partially}} Connected Neurons Driven by Spontaneous Activity},
  volume = {101},
  issn = {0340-1200, 1432-0770},
  shorttitle = {Emergence of Network Structure Due to Spike-Timing-Dependent Plasticity in Recurrent Neuronal Networks {{III}}},
  doi = {10.1007/s00422-009-0343-4},
  abstract = {In contrast to a feed-forward architecture, the weight dynamics induced by spike-timing-dependent plasticity (STDP) in a recurrent neuronal network is not yet well understood. In this article, we extend a previous study of the impact of additive STDP in a recurrent network that is driven by spontaneous activity (no external stimulating inputs) from a fully connected network to one that is only partially connected. The asymptotic state of the network is analyzed, and it is found that the equilibrium and stability conditions for the firing rates are similar for both full and partial connectivity: STDP causes the firing rates to converge toward the same value and remain quasi-homogeneous. However, when STDP induces strong weight competition, the connectivity affects the weight dynamics in that the distribution of the weights disperses more quickly for lower density than for higher density. The asymptotic weight distribution strongly depends upon that at the beginning of the learning epoch; consequently, homogeneous connectivity alone is not sufficient to obtain homogeneous neuronal activity. In the absence of external inputs, STDP can nevertheless generate structure in the network through autocorrelation effects, for example, by introducing asymmetry in network topology.},
  language = {en},
  number = {5-6},
  journal = {Biological Cybernetics},
  author = {Gilson, Matthieu and Burkitt, Anthony N. and Grayden, David B. and Thomas, Doreen A. and van Hemmen, J. Leo},
  month = nov,
  year = {2009},
  keywords = {Bioinformatics,Computer Appl. in Life Sciences,Neurobiology,Neurosciences,Partial connectivity,Recurrent neuronal network,STDP,Statistical Physics; Dynamical Systems and Complexity,learning},
  pages = {411-426},
  file = {articles/Gilson2009_3.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/MDSWEWJT/s00422-009-0343-4.html}
}

@techreport{zotero-184,
  title = {Caption},
  file = {manuals/latex/caption.pdf},
  note = {manuals/latex}
}

@unpublished{Hoffmann2015,
  title = {Topologie \& {{Algebraische Topologie}}},
  author = {Hoffmann, Felix},
  year = {2015},
  file = {manuscripts/Hoffmann2015_Topologie-&-Algebraische-Topologie.pdf}
}

@unpublished{Hoffmann2015f,
  title = {Operatornorm},
  author = {Hoffmann, Felix},
  year = {2015},
  file = {manuscripts/Hoffmann2015_Operatornorm.pdf}
}

@article{Zalesky2012,
  title = {On the Use of Correlation as a Measure of Network Connectivity},
  volume = {60},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2012.02.001},
  abstract = {Numerous studies have demonstrated that brain networks derived from neuroimaging data have nontrivial topological features, such as small-world organization, modular structure and highly connected hubs. In these studies, the extent of connectivity between pairs of brain regions has often been measured using some form of statistical correlation. This article demonstrates that correlation as a measure of connectivity in and of itself gives rise to networks with non-random topological features. In particular, networks in which connectivity is measured using correlation are inherently more clustered than random networks, and as such are more likely to be small-world networks. Partial correlation as a measure of connectivity also gives rise to networks with non-random topological features. Partial correlation networks are inherently less clustered than random networks. Network measures in correlation networks should be benchmarked against null networks that respect the topological structure induced by correlation measurements. Prevalently used random rewiring algorithms do not yield appropriate null networks for some network measures. Null networks are proposed to explicitly normalize for the inherent topological structure found in correlation networks, resulting in more conservative estimates of small-world organization. A number of steps may be needed to normalize each network measure individually and control for distinct features (e.g. degree distribution). The main conclusion of this article is that correlation can and should be used to measure connectivity, however appropriate null networks should be used to benchmark network measures in correlation networks.},
  number = {4},
  journal = {NeuroImage},
  author = {Zalesky, Andrew and Fornito, Alex and Bullmore, Ed},
  month = may,
  year = {2012},
  keywords = {Brain connectivity,Connectome,Correlation,Partial correlation,Small-world network,Transitivity},
  pages = {2096-2106},
  file = {articles/Zalesky2012.pdf}
}

@article{Borst2015,
  title = {Common Circuit Design in Fly and Mammalian Motion Vision},
  volume = {18},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4050},
  number = {8},
  journal = {Nature Neuroscience},
  author = {Borst, Alexander and Helmstaedter, Moritz},
  month = jun,
  year = {2015},
  pages = {1067-1076},
  file = {articles/Borst2015.pdf}
}

@article{Ostojic2011,
  title = {From {{Spiking Neuron Models}} to {{Linear}}-{{Nonlinear Models}}},
  volume = {7},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1001056},
  language = {en},
  number = {1},
  journal = {PLoS Computational Biology},
  author = {Ostojic, Srdjan and Brunel, Nicolas},
  editor = {Latham, Peter E.},
  month = jan,
  year = {2011},
  pages = {e1001056},
  file = {articles/Ostojic2011.pdf}
}

@article{Cuntz2012,
  title = {A Scaling Law Derived from Optimal Dendritic Wiring},
  volume = {109},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1200430109},
  abstract = {The wide diversity of dendritic trees is one of the most striking features of neural circuits. Here we develop a general quantitative theory relating the total length of dendritic wiring to the number of branch points and synapses. We show that optimal wiring predicts a 2/3 power law between these measures. We demonstrate that the theory is consistent with data from a wide variety of neurons across many different species and helps define the computational compartments in dendritic trees. Our results imply fundamentally distinct design principles for dendritic arbors compared with vascular, bronchial, and botanical trees.},
  language = {en},
  number = {27},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Cuntz, Hermann and Mathy, Alexandre and H\"ausser, Michael},
  month = mar,
  year = {2012},
  keywords = {_tablet,branching,computational neuroscience,dendrite,minimum spanning tree,morphology},
  pages = {11014-11018},
  file = {articles/Cuntz2012.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/GI5A83GX/11014.html},
  pmid = {22715290}
}

@article{Babadi2013,
  title = {Pairwise {{Analysis Can Account}} for {{Network Structures Arising}} from {{Spike}}-{{Timing Dependent Plasticity}}},
  volume = {9},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002906},
  language = {en},
  number = {2},
  journal = {PLoS Computational Biology},
  author = {Babadi, Baktash and Abbott, L. F.},
  editor = {Gutkin, Boris S.},
  month = feb,
  year = {2013},
  pages = {e1002906},
  file = {/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/MPS3AIF6/Babadi2013_S1.pdf;articles/Babadi2013.pdf}
}

@book{Janson2000,
  address = {New York},
  series = {Wiley-Interscience series in discrete mathematics and optimization},
  title = {Random Graphs},
  isbn = {978-0-471-17541-4},
  lccn = {QA166.17 .J36 2000},
  publisher = {{John Wiley}},
  author = {Janson, Svante and \L{}uczak, Tomasz and Ruci\'nski, Andrzej},
  year = {2000},
  keywords = {Random graphs},
  file = {books/Janson2000_Random-graphs.djvu}
}

@article{Megias2001,
  title = {Total Number and Distribution of Inhibitory and Excitatory Synapses on Hippocampal {{CA1}} Pyramidal Cells},
  volume = {102},
  issn = {0306-4522},
  doi = {10.1016/S0306-4522(00)00496-6},
  abstract = {The integrative properties of neurons depend strongly on the number, proportions and distribution of excitatory and inhibitory synaptic inputs they receive. In this study the three-dimensional geometry of dendritic trees and the density of symmetrical and asymmetrical synapses on different cellular compartments of rat hippocampal CA1 area pyramidal cells was measured to calculate the total number and distribution of excitatory and inhibitory inputs on a single cell.

A single pyramidal cell has $\sim$12,000 $\mu$m dendrites and receives around 30,000 excitatory and 1700 inhibitory inputs, of which 40\% are concentrated in the perisomatic region and 20\% on dendrites in the stratum lacunosum-moleculare. The pre- and post-synaptic features suggest that CA1 pyramidal cell dendrites are heterogeneous. Strata radiatum and oriens dendrites are similar and differ from stratum lacunosum-moleculare dendrites. Proximal apical and basal strata radiatum and oriens dendrites are spine-free or sparsely spiny. Distal strata radiatum and oriens dendrites (forming 68.5\% of the pyramidal cells' dendritic tree) are densely spiny; their excitatory inputs terminate exclusively on dendritic spines, while inhibitory inputs target only dendritic shafts. The proportion of inhibitory inputs on distal spiny strata radiatum and oriens dendrites is low ($\sim$3\%). In contrast, proximal dendritic segments receive mostly (70\textendash{}100\%) inhibitory inputs. Only inhibitory inputs innervate the somata (77\textendash{}103 per cell) and axon initial segments. Dendrites in the stratum lacunosum-moleculare possess moderate to small amounts of spines. Excitatory synapses on stratum lacunosum-moleculare dendrites are larger than the synapses in other layers, are frequently perforated ($\sim$40\%) and can be located on dendritic shafts. Inhibitory inputs, whose percentage is relatively high ($\sim$14\textendash{}17\%), also terminate on dendritic spines.

Our results indicate that: (i) the highly convergent excitation arriving onto the distal dendrites of pyramidal cells is primarily controlled by proximally located inhibition; (ii) the organization of excitatory and inhibitory inputs in layers receiving Schaffer collateral input (radiatum/oriens) versus perforant path input (lacunosum-moleculare) is significantly different.},
  number = {3},
  journal = {Neuroscience},
  author = {Meg\i\'as, M and Emri, Zs and Freund, T. F and Guly\'as, A. I},
  month = feb,
  year = {2001},
  keywords = {3D,database for modeling,dendrite geometry,electron microscopy,serial reconstruction,synaptic convergence},
  pages = {527-540},
  file = {articles/Megı́as2001.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/SUJWNAI4/S0306452200004966.html}
}

@article{Heeger2000,
  title = {Poisson Model of Spike Generation},
  volume = {5},
  journal = {Handout, University of Standford},
  author = {Heeger, David},
  year = {2000},
  file = {articles/Heeger2000.pdf}
}

@article{Schneidman2006,
  title = {Weak Pairwise Correlations Imply Strongly Correlated Network States in a Neural Population},
  volume = {440},
  issn = {0028-0836, 1476-4679},
  doi = {10.1038/nature04701},
  number = {7087},
  journal = {Nature},
  author = {Schneidman, Elad and Berry, Michael J. and Segev, Ronen and Bialek, William},
  month = apr,
  year = {2006},
  keywords = {_tablet},
  pages = {1007-1012},
  file = {articles/Schneidman2006.pdf}
}

@article{Legenstein2008,
  title = {A {{Learning Theory}} for {{Reward}}-{{Modulated Spike}}-{{Timing}}-{{Dependent Plasticity}} with {{Application}} to {{Biofeedback}}},
  volume = {4},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000180},
  language = {en},
  number = {10},
  journal = {PLoS Computational Biology},
  author = {Legenstein, Robert and Pecevski, Dejan and Maass, Wolfgang},
  editor = {Graham, Lyle J.},
  month = oct,
  year = {2008},
  pages = {e1000180},
  file = {articles/Legenstein2008.pdf}
}

@book{Lathi1998,
  address = {Carmichael, Calif},
  title = {Signal Processing and Linear Systems},
  isbn = {978-0-941413-35-0},
  lccn = {TK5102.9 .L38 1998},
  publisher = {{Berkeley Cambridge Press}},
  author = {Lathi, B. P.},
  year = {1998},
  keywords = {Linear systems,Signal processing,System analysis},
  file = {books/Lathi1998_Signal-processing-and-linear-systems.djvu}
}

@book{Hatcher2002,
  address = {Cambridge ; New York},
  title = {Algebraic Topology},
  isbn = {978-0-521-79160-1 978-0-521-79540-1},
  lccn = {QA612 .H42 2002},
  publisher = {{Cambridge University Press}},
  author = {Hatcher, Allen},
  year = {2002},
  keywords = {Algebraic topology},
  file = {books/Hatcher2002_Algebraic-topology.pdf}
}

@article{Takemura2013,
  title = {A Visual Motion Detection Circuit Suggested by {{Drosophila}} Connectomics},
  volume = {500},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature12450},
  number = {7461},
  journal = {Nature},
  author = {Takemura, Shin-ya and Bharioke, Arjun and Lu, Zhiyuan and Nern, Aljoscha and Vitaladevuni, Shiv and Rivlin, Patricia K. and Katz, William T. and Olbris, Donald J. and Plaza, Stephen M. and Winston, Philip and Zhao, Ting and Horne, Jane Anne and Fetter, Richard D. and Takemura, Satoko and Blazek, Katerina and Chang, Lei-Ann and Ogundeyi, Omotara and Saunders, Mathew A. and Shapiro, Victor and Sigmund, Christopher and Rubin, Gerald M. and Scheffer, Louis K. and Meinertzhagen, Ian A. and Chklovskii, Dmitri B.},
  month = aug,
  year = {2013},
  keywords = {_tablet},
  pages = {175-181},
  file = {articles/Takemura2013.pdf}
}

@article{Bacque-Cazenave2015,
  title = {The Effect of Sensory Feedback on Crayfish Posture and Locomotion: {{II}}. {{Neuromechanical}} Simulation of Closing the Loop},
  volume = {113},
  issn = {0022-3077, 1522-1598},
  shorttitle = {The Effect of Sensory Feedback on Crayfish Posture and Locomotion},
  doi = {10.1152/jn.00870.2014},
  language = {en},
  number = {6},
  journal = {Journal of Neurophysiology},
  author = {{Bacqu\'e-Cazenave}, Julien and Chung, Bryce and Cofer, David W. and Cattaert, Daniel and Edwards, Donald H.},
  month = mar,
  year = {2015},
  pages = {1772-1783},
  file = {articles/Bacqué-Cazenave2015.pdf}
}

@article{Bienenstock1982,
  title = {Theory for the Development of Neuron Selectivity: Orientation Specificity and Binocular Interaction in Visual Cortex},
  volume = {2},
  issn = {0270-6474, 1529-2401},
  shorttitle = {Theory for the Development of Neuron Selectivity},
  abstract = {The development of stimulus selectivity in the primary sensory cortex of higher vertebrates is considered in a general mathematical framework. A synaptic evolution scheme of a new kind is proposed in which incoming patterns rather than converging afferents compete. The change in the efficacy of a given synapse depends not only on instantaneous pre- and postsynaptic activities but also on a slowly varying time-averaged value of the postsynaptic activity. Assuming an appropriate nonlinear form for this dependence, development of selectivity is obtained under quite general conditions on the sensory environment. One does not require nonlinearity of the neuron's integrative power nor does one need to assume any particular form for intracortical circuitry. This is first illustrated in simple cases, e.g., when the environment consists of only two different stimuli presented alternately in a random manner. The following formal statement then holds: the state of the system converges with probability 1 to points of maximum selectivity in the state space. We next consider the problem of early development of orientation selectivity and binocular interaction in primary visual cortex. Giving the environment an appropriate form, we obtain orientation tuning curves and ocular dominance comparable to what is observed in normally reared adult cats or monkeys. Simulations with binocular input and various types of normal or altered environments show good agreement with the relevant experimental data. Experiments are suggested that could test our theory further.},
  language = {en},
  number = {1},
  journal = {The Journal of Neuroscience},
  author = {Bienenstock, E. L. and Cooper, L. N. and Munro, P. W.},
  month = jan,
  year = {1982},
  pages = {32-48},
  file = {articles/Bienenstock1982.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/4U4QA72I/32.html},
  pmid = {7054394}
}

@article{Mertz2007,
  title = {Graphics with {{TikZ}}.},
  volume = {1},
  journal = {The PracTEX Journal},
  author = {Mertz, Andrew and Slough, William},
  year = {2007},
  file = {articles/Mertz2007.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/VM4AAGRS/3.html}
}

@article{Hires2015,
  title = {Low-Noise Encoding of Active Touch by Layer 4 in the Somatosensory Cortex},
  volume = {4},
  issn = {2050-084X},
  doi = {10.7554/eLife.06619},
  language = {en},
  journal = {eLife},
  author = {Hires, Andrew Samuel and Gutnisky, Diego A and Yu, Jianing and O'Connor, Daniel H and Svoboda, Karel},
  month = aug,
  year = {2015},
  file = {articles/Hires2015.pdf}
}

@article{Sigurdsson2015,
  title = {Neural Circuit Dysfunction in Schizophrenia: {{Insights}} from Animal Models},
  issn = {03064522},
  shorttitle = {Neural Circuit Dysfunction in Schizophrenia},
  doi = {10.1016/j.neuroscience.2015.06.059},
  language = {en},
  journal = {Neuroscience},
  author = {Sigurdsson, T.},
  month = jul,
  year = {2015},
  file = {articles/Sigurdsson2015.pdf}
}

@book{Solomon1978,
  address = {Philadelphia, Pa},
  series = {CBMS-NSF regional conference series in applied mathematics},
  title = {Geometric Probability},
  isbn = {978-0-89871-025-0},
  language = {eng},
  number = {28},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Solomon, Herbert},
  collaborator = {{Society for Industrial and Applied Mathematics}},
  year = {1978},
  file = {books/Solomon1978_Geometric-probability.pdf}
}

@article{Erdi2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.5909},
  title = {Teaching {{Computational Neuroscience}}},
  issn = {1871-4080, 1871-4099},
  doi = {10.1007/s11571-015-9340-6.},
  abstract = {The problems and beauty of teaching computational neuroscience are discussed by reviewing three new textbooks.},
  journal = {Cognitive Neurodynamics},
  author = {\'Erdi, P\'eter},
  month = mar,
  year = {2015},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {articles/Érdi2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/UI68KA8S/1412.html}
}

@article{Petreanu2009,
  title = {The Subcellular Organization of Neocortical Excitatory Connections},
  volume = {457},
  copyright = {\textcopyright{} 2009 Nature Publishing Group},
  issn = {0028-0836},
  doi = {10.1038/nature07709},
  abstract = {Understanding cortical circuits will require mapping the connections between specific populations of neurons, as well as determining the dendritic locations where the synapses occur. The dendrites of individual cortical neurons overlap with numerous types of local and long-range excitatory axons, but axodendritic overlap is not always a good predictor of actual connection strength. Here we developed an efficient channelrhodopsin-2 (ChR2)-assisted method to map the spatial distribution of synaptic inputs, defined by presynaptic ChR2 expression, within the dendritic arborizations of recorded neurons. We expressed ChR2 in two thalamic nuclei, the whisker motor cortex and local excitatory neurons and mapped their synapses with pyramidal neurons in layers 3, 5A and 5B (L3, L5A and L5B) in the mouse barrel cortex. Within the dendritic arborizations of L3 cells, individual inputs impinged onto distinct single domains. These domains were arrayed in an orderly, monotonic pattern along the apical axis: axons from more central origins targeted progressively higher regions of the apical dendrites. In L5 arborizations, different inputs targeted separate basal and apical domains. Input to L3 and L5 dendrites in L1 was related to whisker movement and position, suggesting that these signals have a role in controlling the gain of their target neurons. Our experiments reveal high specificity in the subcellular organization of excitatory circuits.},
  language = {en},
  number = {7233},
  journal = {Nature},
  author = {Petreanu, Leopoldo and Mao, Tianyi and Sternson, Scott M. and Svoboda, Karel},
  month = feb,
  year = {2009},
  pages = {1142-1145},
  file = {articles/Petreanu2009.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/FEND9KR6/nature07709.html}
}

@book{Rannacher2006,
  title = {Einf\"uhrung in Die {{Numerische Mathematik}}},
  author = {Rannacher, Rolf},
  year = {2006},
  file = {books/Rannacher2006_Einführung-in-die-Numerische-Mathematik.pdf}
}

@article{Softky1993,
  title = {The Highly Irregular Firing of Cortical Cells Is Inconsistent with Temporal Integration of Random {{EPSPs}}},
  volume = {13},
  issn = {0270-6474, 1529-2401},
  language = {en},
  number = {1},
  journal = {The Journal of Neuroscience},
  author = {Softky, W. R. and Koch, C.},
  month = jan,
  year = {1993},
  pages = {334-350},
  file = {articles/Softky1993.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/KW2DQJ7Z/334.html},
  pmid = {8423479}
}

@article{Brunel2004,
  title = {Optimal {{Information Storage}} and the {{Distribution}} of {{Synaptic Weights}}: {{Perceptron}} versus {{Purkinje Cell}}},
  volume = {43},
  issn = {0896-6273},
  shorttitle = {Optimal {{Information Storage}} and the {{Distribution}} of {{Synaptic Weights}}},
  doi = {10.1016/j.neuron.2004.08.023},
  abstract = {It is widely believed that synaptic modifications underlie learning and memory. However, few studies have examined what can be deduced about the learning process from the distribution of synaptic weights. We analyze the perceptron, a prototypical feedforward neural network, and obtain the optimal synaptic weight distribution for a perceptron with excitatory synapses. It contains more than 50\% silent synapses, and this fraction increases with storage reliability: silent synapses are therefore a necessary byproduct of optimizing learning and reliability. Exploiting the classical analogy between the perceptron and the cerebellar Purkinje cell, we fitted the optimal weight distribution to that measured for granule cell-Purkinje cell synapses. The two distributions agreed well, suggesting that the Purkinje cell can learn up to 5 kilobytes of information, in the form of 40,000 input-output associations.},
  number = {5},
  journal = {Neuron},
  author = {Brunel, Nicolas and Hakim, Vincent and Isope, Philippe and Nadal, Jean-Pierre and Barbour, Boris},
  month = sep,
  year = {2004},
  pages = {745-757},
  file = {articles/Brunel22.pdf}
}

@article{Herzog2011,
  series = {Perceptual Learning - mechanisms and manifestations},
  title = {Perceptual Learning, Roving and the Unsupervised Bias},
  volume = {61},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2011.11.001},
  abstract = {Perceptual learning improves perception through training. Perceptual learning improves with most stimulus types but fails when certain stimulus types are mixed during training (roving). This result is surprising because classical supervised and unsupervised neural network models can cope easily with roving conditions. What makes humans so inferior compared to these models? As experimental and conceptual work has shown, human perceptual learning is neither supervised nor unsupervised but reward-based learning. Reward-based learning suffers from the so-called unsupervised bias, i.e., to prevent synaptic ``drift'', the average reward has to be exactly estimated. However, this is impossible when two or more stimulus types with different rewards are presented during training (and the reward is estimated by a running average). For this reason, we propose no learning occurs in roving conditions. However, roving hinders perceptual learning only for combinations of similar stimulus types but not for dissimilar ones. In this latter case, we propose that a critic can estimate the reward for each stimulus type separately. One implication of our analysis is that the critic cannot be located in the visual system.},
  journal = {Vision Research},
  author = {Herzog, Michael H. and Aberg, Kristoffer C. and Fr\'emaux, Nicolas and Gerstner, Wulfram and Sprekeler, Henning},
  month = nov,
  year = {2011},
  keywords = {Bisection stimuli,Perceptual learning,Roving,neural networks},
  pages = {95-99},
  file = {articles/Herzog2011.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/VAQMV5FV/S0042698911003816.html}
}

@article{Klinshov2014,
  title = {Dense {{Neuron Clustering Explains Connectivity Statistics}} in {{Cortical Microcircuits}}},
  volume = {9},
  doi = {10.1371/journal.pone.0094292},
  abstract = {Local cortical circuits appear highly non-random, but the underlying connectivity rule remains elusive. Here, we analyze experimental data observed in layer 5 of rat neocortex and suggest a model for connectivity from which emerge essential observed non-random features of both wiring and weighting. These features include lognormal distributions of synaptic connection strength, anatomical clustering, and strong correlations between clustering and connection strength. Our model predicts that cortical microcircuits contain large groups of densely connected neurons which we call clusters. We show that such a cluster contains about one fifth of all excitatory neurons of a circuit which are very densely connected with stronger than average synapses. We demonstrate that such clustering plays an important role in the network dynamics, namely, it creates bistable neural spiking in small cortical circuits. Furthermore, introducing local clustering in large-scale networks leads to the emergence of various patterns of persistent local activity in an ongoing network activity. Thus, our results may bridge a gap between anatomical structure and persistent activity observed during working memory and other cognitive processes.},
  number = {4},
  journal = {PLoS ONE},
  author = {Klinshov, Vladimir V. and Teramae, Jun-nosuke and Nekorkin, Vladimir I. and Fukai, Tomoki},
  month = apr,
  year = {2014},
  pages = {e94292},
  file = {articles/Klinshov2014.pdf}
}

@article{Kleindienst2011a,
  title = {Activity-{{Dependent Clustering}} of {{Functional Synaptic Inputs}} on {{Developing Hippocampal Dendrites}}},
  volume = {72},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2011.10.015},
  abstract = {Summary
During brain development, before sensory systems become functional, neuronal networks spontaneously generate repetitive bursts of neuronal activity, which are typically synchronized across many neurons. Such activity patterns have been described on the level of networks and cells, but the fine-structure of inputs received by an individual neuron during spontaneous network activity has not been studied. Here, we used calcium imaging to record activity at many synapses of hippocampal pyramidal neurons simultaneously to establish the activity patterns in the majority of synapses of an entire cell. Analysis of the spatiotemporal patterns of synaptic activity revealed a fine-scale connectivity rule: neighboring synapses (\&lt;16~$\mu$m intersynapse distance) are more likely to be coactive than synapses that are farther away from each other. Blocking spiking activity or NMDA receptor activation revealed that the clustering of synaptic inputs required neuronal activity, demonstrating a role of developmentally expressed spontaneous activity for connecting neurons with subcellular precision.},
  number = {6},
  journal = {Neuron},
  author = {Kleindienst, Thomas and Winnubst, Johan and {Roth-Alpermann}, Claudia and Bonhoeffer, Tobias and Lohmann, Christian},
  month = dec,
  year = {2011},
  pages = {1012-1024},
  file = {articles/Kleindienst2011.pdf}
}

@article{Tkacik2015,
  title = {Thermodynamics and Signatures of Criticality in a Network of Neurons},
  volume = {112},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1514188112},
  abstract = {The activity of a neural network is defined by patterns of spiking and silence from the individual neurons. Because spikes are (relatively) sparse, patterns of activity with increasing numbers of spikes are less probable, but, with more spikes, the number of possible patterns increases. This tradeoff between probability and numerosity is mathematically equivalent to the relationship between entropy and energy in statistical physics. We construct this relationship for populations of up to N = 160 neurons in a small patch of the vertebrate retina, using a combination of direct and model-based analyses of experiments on the response of this network to naturalistic movies. We see signs of a thermodynamic limit, where the entropy per neuron approaches a smooth function of the energy per neuron as N increases. The form of this function corresponds to the distribution of activity being poised near an unusual kind of critical point. We suggest further tests of criticality, and give a brief discussion of its functional significance.},
  language = {en},
  number = {37},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Tka{\v c}ik, Ga{\v s}per and Mora, Thierry and Marre, Olivier and Amodei, Dario and Palmer, Stephanie E. and Berry, Michael J. and Bialek, William},
  month = sep,
  year = {2015},
  keywords = {Correlation,Monte Carlo,entropy,information,neural networks},
  pages = {11508-11513},
  file = {articles/Tkačik2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/26688B5E/11508.html},
  pmid = {26330611}
}

@book{West2000,
  address = {Upper Saddle River, N.J},
  edition = {2 edition},
  title = {Introduction to {{Graph Theory}}},
  isbn = {978-0-13-014400-3},
  language = {English},
  publisher = {{Pearson}},
  author = {West, Douglas},
  month = sep,
  year = {2000},
  file = {books/West2000_Introduction-to-Graph-Theory.pdf}
}

@article{Zhao2011,
  title = {Synchronization from {{Second Order Network Connectivity Statistics}}},
  volume = {5},
  issn = {1662-5188},
  doi = {10.3389/fncom.2011.00028},
  abstract = {We investigate how network structure can influence the tendency for a neuronal network to synchronize, or its synchronizability, independent of the dynamical model for each neuron. The synchrony analysis takes advantage of the framework of second order networks, which defines four second order connectivity statistics based on the relative frequency of two-connection network motifs. The analysis identifies two of these statistics, convergent connections, and chain connections, as highly influencing the synchrony. Simulations verify that synchrony decreases with the frequency of convergent connections and increases with the frequency of chain connections. These trends persist with simulations of multiple models for the neuron dynamics and for different types of networks. Surprisingly, divergent connections, which determine the fraction of shared inputs, do not strongly influence the synchrony. The critical role of chains, rather than divergent connections, in influencing synchrony can be explained by their increasing the effective coupling strength. The decrease of synchrony with convergent connections is primarily due to the resulting heterogeneity in firing rates.},
  journal = {Frontiers in Computational Neuroscience},
  author = {Zhao, Liqiong and Beverlin, Bryce and Netoff, Theoden and Nykamp, Duane Q.},
  month = jul,
  year = {2011},
  file = {articles/Zhao2011.pdf},
  pmid = {21779239},
  pmcid = {PMC3134837}
}

@techreport{zotero-217,
  title = {Pro {{Git}}},
  file = {manuals/git/pro_git.pdf},
  note = {manuals/git}
}

@techreport{zotero-218,
  title = {Algorithms {{Bundle}}},
  file = {manuals/latex/algorithms_bundle.pdf},
  note = {manuals/latex}
}

@article{Kalisman2003,
  title = {Deriving Physical Connectivity from Neuronal Morphology},
  volume = {88},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-002-0377-3},
  abstract = {A model is presented that allows prediction of the probability for the formation of appositions between the axons and dendrites of any two neurons based only on their morphological statistics and relative separation. Statistics of axonal and dendritic morphologies of single neurons are obtained from 3D reconstructions of biocytin-filled cells, and a statistical representation of the same cell type is obtained by averaging across neurons according to the model. A simple mathematical formulation is applied to the axonal and dendritic statistical representations to yield the probability for close appositions. The model is validated by a mathematical proof and by comparison of predicted appositions made by layer 5 pyramidal neurons in the rat somatosensory cortex with real anatomical data. The model could be useful for studying microcircuit connectivity and for designing artificial neural networks.},
  language = {en},
  number = {3},
  journal = {Biological Cybernetics},
  author = {Kalisman, Nir and Silberberg, Gilad and Markram, Henry},
  month = mar,
  year = {2003},
  pages = {210-218},
  file = {articles/Kalisman2003.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/5N835XB9/10.html}
}

@techreport{zotero-220,
  title = {{{FreeNAS}} 9.3},
  file = {manuals/server/freenas_9.3.pdf},
  note = {manuals/server}
}

@article{Miner2016,
  title = {Plasticity-{{Driven Self}}-{{Organization}} under {{Topological Constraints Accounts}} for {{Non}}-Random {{Features}} of {{Cortical Synaptic Wiring}}},
  volume = {12},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004759},
  language = {en},
  number = {2},
  journal = {PLOS Computational Biology},
  author = {Miner, Daniel and Triesch, Jochen},
  editor = {Sporns, Olaf},
  month = feb,
  year = {2016},
  keywords = {structural_plasticity},
  pages = {e1004759},
  file = {articles/Miner2016.pdf}
}

@book{Kailath1980,
  address = {Englewood Cliffs, N.J},
  series = {Prentice-Hall information and system science series},
  title = {Linear Systems},
  isbn = {978-0-13-536961-6},
  lccn = {QA402 .K295 1980},
  publisher = {{Prentice-Hall}},
  author = {Kailath, Thomas},
  year = {1980},
  keywords = {Linear systems,System analysis},
  file = {books/Kailath1980_Linear-systems.pdf}
}

@article{Gilson2009,
  title = {Emergence of Network Structure Due to Spike-Timing-Dependent Plasticity in Recurrent Neuronal Networks {{IV}}},
  volume = {101},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-009-0346-1},
  abstract = {In neuronal networks, the changes of synaptic strength (or weight) performed by spike-timing-dependent plasticity (STDP) are hypothesized to give rise to functional network structure. This article investigates how this phenomenon occurs for the excitatory recurrent connections of a network with fixed input weights that is stimulated by external spike trains. We develop a theoretical framework based on the Poisson neuron model to analyze the interplay between the neuronal activity (firing rates and the spike-time correlations) and the learning dynamics, when the network is stimulated by correlated pools of homogeneous Poisson spike trains. STDP can lead to both a stabilization of all the neuron firing rates (homeostatic equilibrium) and a robust weight specialization. The pattern of specialization for the recurrent weights is determined by a relationship between the input firing-rate and correlation structures, the network topology, the STDP parameters and the synaptic response properties. We find conditions for feed-forward pathways or areas with strengthened self-feedback to emerge in an initially homogeneous recurrent network.},
  language = {en},
  number = {5-6},
  journal = {Biological Cybernetics},
  author = {Gilson, Matthieu and Burkitt, Anthony N. and Grayden, David B. and Thomas, Doreen A. and van Hemmen, J. Leo},
  month = nov,
  year = {2009},
  keywords = {Bioinformatics,Computer Appl. in Life Sciences,Neurobiology,Neurosciences,Recurrent neuronal network,STDP,Spike-time correlation,Statistical Physics; Dynamical Systems and Complexity,learning},
  pages = {427-444},
  file = {articles/Gilson2009_4.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/FQA9BMDT/s00422-009-0346-1.html}
}

@book{Daley2003,
  address = {New York},
  edition = {2nd edition},
  title = {An {{Introduction}} to the {{Theory}} of {{Point Processes}}, {{Volume}} 1},
  isbn = {978-0-387-95541-4},
  abstract = {Point processes and random measures find wide applicability in telecommunications, earthquakes, image analysis, spatial point patterns, and stereology, to name but a few areas. The authors have made a major reshaping of their work in their first edition of 1988 and now present their Introduction to the Theory of Point Processes in two volumes with sub-titles Elementary Theory and Models and General Theory and Structure.Volume One contains the introductory chapters from the first edition, together with an informal treatment of some of the later material intended to make it more accessible to readers primarily interested in models and applications. The main new material in this volume relates to marked point processes and to processes evolving in time, where the conditional intensity methodology provides a basis for model building, inference, and prediction. There are abundant examples whose purpose is both didactic and to illustrate further applications of the ideas and models that are the main substance of the text.},
  language = {English},
  publisher = {{Springer}},
  author = {Daley, D. J. and {Vere-Jones}, D.},
  month = nov,
  year = {2003},
  file = {books/Daley2003_An-Introduction-to-the-Theory-of-Point-Processes,-Volume-1.pdf}
}

@book{MacKay2003,
  address = {Cambridge, UK ; New York},
  title = {Information Theory, Inference, and Learning Algorithms},
  isbn = {978-0-521-64298-9},
  lccn = {Q360 .M23 2003},
  publisher = {{Cambridge University Press}},
  author = {MacKay, David J. C.},
  year = {2003},
  keywords = {Information theory},
  file = {books/MacKay2003_Information-theory,-inference,-and-learning-algorithms.pdf}
}

@article{Seth2005,
  series = {Neurobiology of Animal Consciousness},
  title = {Neural {{Darwinism}} and Consciousness},
  volume = {14},
  issn = {1053-8100},
  doi = {10.1016/j.concog.2004.08.008},
  abstract = {Neural Darwinism (ND) is a large scale selectionist theory of brain development and function that has been hypothesized to relate to consciousness. According to ND, consciousness is entailed by reentrant interactions among neuronal populations in the thalamocortical system (the `dynamic core'). These interactions, which permit high-order discriminations among possible core states, confer selective advantages on organisms possessing them by linking current perceptual events to a past history of value-dependent learning. Here, we assess the consistency of ND with 16 widely recognized properties of consciousness, both physiological (for example, consciousness is associated with widespread, relatively fast, low amplitude interactions in the thalamocortical system), and phenomenal (for example, consciousness involves the existence of a private flow of events available only to the experiencing subject). While no theory accounts fully for all of these properties at present, we find that ND and its recent extensions fare well.},
  number = {1},
  journal = {Consciousness and Cognition},
  author = {Seth, Anil K. and Baars, Bernard J.},
  month = mar,
  year = {2005},
  keywords = {Complexity,Degeneracy,Dynamic core,Edelman,Neural Darwinism,Reentry,Thalamocortical system},
  pages = {140-168},
  file = {articles/Seth2005.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/R5HMSQ66/S1053810004000923.html}
}

@book{Godfrey-Smith2003,
  address = {Chicago},
  edition = {1 edition},
  title = {Theory and {{Reality}}: {{An Introduction}} to the {{Philosophy}} of {{Science}}},
  isbn = {978-0-226-30063-4},
  shorttitle = {Theory and {{Reality}}},
  abstract = {How does science work?  Does it tell us what the world is "really" like?  What makes it different from other ways of understanding the universe?  In Theory and Reality, Peter Godfrey-Smith addresses these questions by taking the reader on a grand tour of one hundred years of debate about science. The result is a completely accessible introduction to the main themes of the philosophy of science.  Intended for undergraduates and general readers with no prior background in philosophy, Theory and Reality covers logical positivism; the problems of induction and confirmation; Karl Popper's theory of science; Thomas Kuhn and "scientific revolutions"; the views of Imre Lakatos, Larry Laudan, and Paul Feyerabend; and challenges to the field from sociology of science, feminism, and science studies. The book then looks in more detail at some specific problems and theories, including scientific realism, the theory-ladeness of observation, scientific explanation, and Bayesianism. Finally, Godfrey-Smith defends a form of philosophical naturalism as the best way to solve the main problems in the field. Throughout the text he points out connections between philosophical debates and wider discussions about science in recent decades, such as the infamous "science wars." Examples and asides engage the beginning student; a glossary of terms explains key concepts; and suggestions for further reading are included at the end of each chapter. However, this is a textbook that doesn't feel like a textbook because it captures the historical drama of changes in how science has been conceived over the last one hundred years. Like no other text in this field, Theory and Reality combines a survey of recent history of the philosophy of science with current key debates in language that any beginning scholar or critical reader can follow.},
  language = {English},
  publisher = {{University of Chicago Press}},
  author = {{Godfrey-Smith}, Peter},
  month = aug,
  year = {2003},
  file = {books/Godfrey-Smith2003_Theory-and-Reality-An-Introduction-to-the-Philosophy-of-Science.pdf}
}

@article{Rall2009,
  title = {Rall Model},
  volume = {4},
  issn = {1941-6016},
  doi = {10.4249/scholarpedia.1369},
  language = {en},
  number = {4},
  journal = {Scholarpedia},
  author = {Rall, Wilfrid},
  year = {2009},
  pages = {1369}
}

@unpublished{Hoffmann2015i,
  title = {Markovketten},
  author = {Hoffmann},
  year = {2015},
  file = {manuscripts/Hoffmann2015_Markovketten.pdf}
}

@article{Geisler2010,
  title = {Temporal Delays among Place Cells Determine the Frequency of Population Theta Oscillations in the Hippocampus},
  volume = {107},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0912478107},
  abstract = {Driven either by external landmarks or by internal dynamics, hippocampal neurons form sequences of cell assemblies. The coordinated firing of these active cells is organized by the prominent ``theta'' oscillations in the local field potential (LFP): place cells discharge at progressively earlier theta phases as the rat crosses the respective place field (``phase precession''). The faster oscillation frequency of active neurons and the slower theta LFP, underlying phase precession, creates a paradox. How can faster oscillating neurons comprise a slower population oscillation, as reflected by the LFP? We built a mathematical model that allowed us to calculate the population activity analytically from experimentally derived parameters of the single neuron oscillation frequency, firing field size (duration), and the relationship between within-theta delays of place cell pairs and their distance representations (``compression''). The appropriate combination of these parameters generated a constant frequency population rhythm along the septo\textendash{}temporal axis of the hippocampus, while allowing individual neurons to vary their oscillation frequency and field size. Our results suggest that the faster-than-theta oscillations of pyramidal cells are inherent and that phase precession is a result of the coordinated activity of temporally shifted cell assemblies, relative to the population activity, reflected by the LFP.},
  language = {en},
  number = {17},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Geisler, Caroline and Diba, Kamran and Pastalkova, Eva and Mizuseki, Kenji and Royer, Sebastien and Buzs\'aki, Gy\"orgy},
  month = apr,
  year = {2010},
  keywords = {assembly sequence,cell assembly,phase coding,phase precession,ventral hippocampus},
  pages = {7957-7962},
  file = {articles/Geisler2010.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/5MVZUMGI/7957.html},
  pmid = {20375279}
}

@book{McFarland2009,
  address = {Beijing ; Sebastopol},
  edition = {2nd ed},
  series = {The missing manual},
  title = {{{CSS}}: The Missing Manual},
  isbn = {978-0-596-80244-8},
  lccn = {TK5105.888 .M3767 2009},
  shorttitle = {{{CSS}}},
  publisher = {{Pogue Press/O'Reilly}},
  author = {McFarland, David Sawyer},
  year = {2009},
  keywords = {Cascading style sheets,Computer graphics,Design,Web publishing,Web site development,Web sites},
  file = {books/McFarland2009_CSS-the-missing-manual.pdf},
  note = {manuals/html\_css}
}

@book{Sterratt2011,
  address = {Cambridge ; New York},
  title = {Principles of Computational Modelling in Neuroscience},
  isbn = {978-0-521-87795-4},
  lccn = {QP357.5 .P75 2011},
  publisher = {{Cambridge University Press}},
  editor = {Sterratt, David},
  year = {2011},
  keywords = {Computer Simulation,Models; Neurological,Neural Conduction,Synaptic Transmission,computational neuroscience},
  file = {books/Sterratt2011_Principles-of-computational-modelling-in-neuroscience.pdf}
}

@book{Geiger2002,
  address = {Berlin},
  title = {{Theorie und Numerik restringierter Optimierungsaufgaben: [mit 140 \"Ubungsaufgaben]}},
  isbn = {978-3-540-42790-2},
  shorttitle = {{Theorie und Numerik restringierter Optimierungsaufgaben}},
  language = {ger},
  publisher = {{Springer}},
  author = {Geiger, Carl and Kanzow, Christian},
  year = {2002},
  keywords = {29,Mathematical optimization,Nebenbedingung,Optimierung},
  file = {books/Geiger2002_Theorie-und-Numerik-restringierter-Optimierungsaufgaben-[mit-140-Übungsaufgaben].pdf}
}

@book{Kuhn1996,
  address = {Chicago, IL},
  edition = {3rd edition},
  title = {The {{Structure}} of {{Scientific Revolutions}}},
  isbn = {978-0-226-45808-3},
  abstract = {Thomas S. Kuhn's classic book is now available with a new index."A landmark in intellectual history which has attracted attention far beyond its own immediate field. . . . It is written with a combination of depth and clarity that make it an almost unbroken series of aphorisms. . . . Kuhn does not permit truth to be a criterion of scientific theories, he would presumably not claim his own theory to be true. But if causing a revolution is the hallmark of a superior paradigm, [this book] has been a resounding success." --Nicholas Wade, Science"Perhaps the best explanation of [the] process of discovery." --William Erwin Thompson, New York Times Book Review"Occasionally there emerges a book which has an influence far beyond its originally intended audience. . . . Thomas Kuhn's The Structure of Scientific Revolutions . . . has clearly emerged as just such a work." --Ron Johnston, Times Higher Education Supplement"Among the most influential academic books in this century." --Choice--One of "The Hundred Most Influential Books Since the Second World War," Times Literary SupplementThomas S. Kuhn was the Laurence Rockefeller Professor Emeritus of linguistics and philosophy at the Massachusetts Institute of Technology. His books include The Essential Tension; Black-Body Theory and the Quantum Discontinuity, 1894-1912; and The Copernican Revolution.},
  language = {English},
  publisher = {{University of Chicago Press}},
  author = {Kuhn, Thomas S.},
  month = dec,
  year = {1996},
  file = {books/Kuhn1996_The-Structure-of-Scientific-Revolutions.pdf}
}

@article{Hofer2011,
  title = {Differential Connectivity and Response Dynamics of Excitatory and Inhibitory Neurons in Visual Cortex},
  volume = {14},
  copyright = {\textcopyright{} 2011 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  doi = {10.1038/nn.2876},
  abstract = {Neuronal responses during sensory processing are influenced by both the organization of intracortical connections and the statistical features of sensory stimuli. How these intrinsic and extrinsic factors govern the activity of excitatory and inhibitory populations is unclear. Using two-photon calcium imaging in vivo and intracellular recordings in vitro, we investigated the dependencies between synaptic connectivity, feature selectivity and network activity in pyramidal cells and fast-spiking parvalbumin-expressing (PV) interneurons in mouse visual cortex. In pyramidal cell populations, patterns of neuronal correlations were largely stimulus-dependent, indicating that their responses were not strongly dominated by functionally biased recurrent connectivity. By contrast, visual stimulation only weakly modified co-activation patterns of fast-spiking PV cells, consistent with the observation that these broadly tuned interneurons received very dense and strong synaptic input from nearby pyramidal cells with diverse feature selectivities. Therefore, feedforward and recurrent network influences determine the activity of excitatory and inhibitory ensembles in fundamentally different ways.},
  language = {en},
  number = {8},
  journal = {Nature Neuroscience},
  author = {Hofer, Sonja B. and Ko, Ho and Pichler, Bruno and Vogelstein, Joshua and Ros, Hana and Zeng, Hongkui and Lein, Ed and Lesica, Nicholas A. and {Mrsic-Flogel}, Thomas D.},
  month = aug,
  year = {2011},
  keywords = {Neuronal physiology,Visual system},
  pages = {1045-1052},
  file = {articles/Hofer2011.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/9VGBDD7G/nn.2876.html}
}

@article{Feldmeyer2012,
  title = {Excitatory Neuronal Connectivity in the Barrel Cortex},
  volume = {6},
  issn = {1662-5129},
  doi = {10.3389/fnana.2012.00024},
  abstract = {Neocortical areas are believed to be organized into vertical modules, the cortical columns, and the horizontal layers 1\textendash{}6. In the somatosensory barrel cortex these columns are defined by the readily discernible barrel structure in layer 4. Information processing in the neocortex occurs along vertical and horizontal axes, thereby linking individual barrel-related columns via axons running through the different cortical layers of the barrel cortex. Long-range signaling occurs within the neocortical layers but also through axons projecting through the white matter to other neocortical areas and subcortical brain regions. Because of the ease of identification of barrel-related columns, the rodent barrel cortex has become a prototypical system to study the interactions between different neuronal connections within a sensory cortical area and between this area and other cortical as well subcortical regions. Such interactions will be discussed specifically for the feed-forward and feedback loops between the somatosensory and the somatomotor cortices as well as the different thalamic nuclei. In addition, recent advances concerning the morphological characteristics of excitatory neurons and their impact on the synaptic connectivity patterns and signaling properties of neuronal microcircuits in the whisker-related somatosensory cortex will be reviewed. In this context, their relationship between the structural properties of barrel-related columns and their function as a module in vertical synaptic signaling in the whisker-related cortical areas will be discussed.},
  journal = {Frontiers in Neuroanatomy},
  author = {Feldmeyer, Dirk},
  month = jul,
  year = {2012},
  keywords = {_tablet},
  file = {articles/Feldmeyer2012.pdf},
  pmid = {22798946},
  pmcid = {PMC3394394}
}

@article{Kempter1999,
  title = {Hebbian Learning and Spiking Neurons},
  volume = {59},
  doi = {10.1103/PhysRevE.59.4498},
  abstract = {A correlation-based (``Hebbian'') learning rule at a spike level with millisecond resolution is formulated, mathematically analyzed, and compared with learning in a firing-rate description. The relative timing of presynaptic and postsynaptic spikes influences synaptic weights via an asymmetric ``learning window.'' A differential equation for the learning dynamics is derived under the assumption that the time scales of learning and neuronal spike dynamics can be separated. The differential equation is solved for a Poissonian neuron model with stochastic spike arrival. It is shown that correlations between input and output spikes tend to stabilize structure formation. With an appropriate choice of parameters, learning leads to an intrinsic normalization of the average weight and the output firing rate. Noise generates diffusion-like spreading of synaptic weights.},
  number = {4},
  journal = {Physical Review E},
  author = {Kempter, Richard and Gerstner, Wulfram and {van Hemmen}, J. Leo},
  month = apr,
  year = {1999},
  pages = {4498-4514},
  file = {articles/Kempter1999.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/HEP9ASTP/PhysRevE.59.html}
}

@article{Romani2013,
  title = {Scaling {{Laws}} of {{Associative Memory Retrieval}}},
  volume = {25},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00499},
  abstract = {Most people have great difficulty in recalling unrelated items. For example, in free recall experiments, lists of more than a few randomly selected words cannot be accurately repeated. Here we introduce a phenomenological model of memory retrieval inspired by theories of neuronal population coding of information. The model predicts nontrivial scaling behaviors for the mean and standard deviation of the number of recalled words for lists of increasing length. Our results suggest that associative information retrieval is a dominating factor that limits the number of recalled items.},
  number = {10},
  journal = {Neural Computation},
  author = {Romani, Sandro and Pinkoviezky, Itai and Rubin, Alon and Tsodyks, Misha},
  month = jun,
  year = {2013},
  pages = {2523-2544},
  file = {articles/Romani2013.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/I4H8SGK5/NECO_a_00499.html}
}

@book{Gardiner2004,
  address = {Berlin},
  edition = {3. ed},
  series = {Springer series in synergetics},
  title = {Handbook of Stochastic Methods for Physics, Chemistry and the Natural Sciences},
  isbn = {978-3-540-20882-2},
  abstract = {This valuable and highly-praised reference collects and explains, in simple language and reasonably deductive form, those formulas and methods and their applications used in modern Statistical Physics, including the foundations of Markov systems, stochastic differential equations, Fokker-Planck equations, approximation methods, chemical master equations, and quantum- mechanical Markov processes. The practical orientation and broad coverage appeal to researchers and academics working in theoretical physics, physical chemistry, and related fields. In the third edition of this classic the chapter on quantum Marcov processes has been replaced by a chapter on numerical treatment of stochastic differential equations to make the book even more valuable for practitioners. From the reviews: "Extremely well written and informative... clear, complete, and fairly rigorous treatment of a larger number of very basic concepts in stochastic theory." (Journal of Quantum Electronics) "A first class book." (Optica Acta) "Ideal for people who need a clear introduction to stochastic mathematics and their applications in physical sciences? an excellent self study and reference book." (Quantnotes.com) "This well-established volume takes a supreme position [among the many books on the subject].. This extremely valuable contribution to the field of applied stochastic methods can be recommended to graduate students, researchers, and university teachers." (Optimization)},
  language = {eng},
  publisher = {{Springer}},
  author = {Gardiner, Crispin W.},
  year = {2004},
  keywords = {Markov-Prozess,Stochastic processes,Stochastischer Prozess,Stochastisches Integral},
  file = {books/Gardiner2004_Handbook-of-stochastic-methods-for-physics,-chemistry-and-the-natural-sciences.djvu},
  note = {OCLC: 249252869}
}

@unpublished{Hoffmann2015c,
  title = {Stochastische {{Prozesse}} - {{Zusammenfassung}}},
  author = {Hoffmann, Felix},
  year = {2015},
  file = {manuscripts/Hoffmann2015_Stochastische-Prozesse---Zusammenfassung.pdf}
}

@unpublished{Hoffmann2010,
  title = {Adjunct {{Functors}}},
  author = {Hoffmann, Felix},
  year = {2010},
  file = {manuscripts/Hoffmann2010_Adjunct-Functors.pdf}
}

@article{Sjostrom2001,
  title = {Rate, {{Timing}}, and {{Cooperativity Jointly Determine Cortical Synaptic Plasticity}}},
  volume = {32},
  issn = {0896-6273},
  doi = {10.1016/S0896-6273(01)00542-6},
  abstract = {Cortical long-term plasticity depends on firing rate, spike timing, and cooperativity among inputs, but how these factors interact during realistic patterns of activity is unknown. Here we monitored plasticity while systematically varying the rate, spike timing, and number of coincident afferents. These experiments demonstrate a novel form of cooperativity operating even when postsynaptic firing is evoked by current injection, and reveal a complex dependence of LTP and LTD on rate and timing. Based on these data, we constructed and tested three quantitative models of cortical plasticity. One of these models, in which spike-timing relationships causing LTP ``win'' out over those favoring LTD, closely fits the data and accurately predicts the build-up of plasticity during random firing. This provides a quantitative framework for predicting the impact of in vivo firing patterns on synaptic strength.},
  number = {6},
  journal = {Neuron},
  author = {Sj\"ostr\"om, Per Jesper and Turrigiano, Gina G and Nelson, Sacha B},
  month = dec,
  year = {2001},
  pages = {1149-1164},
  file = {articles/Sjöström2001.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/BBEEGW2W/S0896627301005426.html}
}

@article{Memmesheimer2014,
  title = {Learning {{Precisely Timed Spikes}}},
  volume = {82},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2014.03.026},
  abstract = {Summary
To signal the onset of salient sensory features or execute well-timed motor sequences, neuronal circuits must transform streams of incoming spike trains into precisely timed firing. To address the efficiency and fidelity with which neurons can perform such computations, we developed a theory to characterize the capacity of feedforward networks to generate desired spike sequences. We find the maximum number of desired output spikes a neuron can implement to be 0.1\textendash{}0.3 per synapse. We further present a biologically plausible learning rule that allows feedforward and recurrent networks to learn multiple mappings between inputs and desired spike sequences. We apply this framework to reconstruct synaptic weights from spiking activity and study the precision with which the temporal structure of ongoing behavior can be inferred from the spiking of premotor neurons. This work provides a powerful approach for characterizing the computational and learning capacities of single neurons and neuronal circuits.},
  number = {4},
  journal = {Neuron},
  author = {Memmesheimer, Raoul-Martin and Rubin, Ran and \"Olveczky, Bence P. and Sompolinsky, Haim},
  month = may,
  year = {2014},
  pages = {925-938},
  file = {articles/Memmesheimer2014.pdf}
}

@book{Hanke-Bourgeois2009,
  address = {Wiesbaden},
  edition = {3., aktualisierte Aufl},
  series = {Studium},
  title = {{Grundlagen der numerischen Mathematik und des wissenschaftlichen Rechnens}},
  isbn = {978-3-8348-0708-3},
  language = {ger},
  publisher = {{Vieweg + Teubner}},
  author = {{Hanke-Bourgeois}, Martin},
  year = {2009},
  keywords = {Differentialgleichung,Lehrbuch,Mathematisches Modell,Numerical analysis,Numerische Mathematik,Numerisches Verfahren},
  file = {articles/Book/Hanke-Bourgeois2009.pdf;articles/Book/Hanke-Bourgeois20092.pdf;articles/Book/Hanke-Bourgeois20093.pdf;books/Hanke-Bourgeois2009_Grundlagen-der-numerischen-Mathematik-und-des-wissenschaftlichen-Rechnens.pdf}
}

@article{Markram1997,
  title = {Physiology and Anatomy of Synaptic Connections between Thick Tufted Pyramidal Neurones in the Developing Rat Neocortex.},
  volume = {500},
  issn = {0022-3751},
  abstract = {1. Dual voltage recordings were made from pairs of adjacent, synaptically connected thick tufted layer 5 pyramidal neurones in brain slices of young rat (14-16 days) somatosensory cortex to examine the physiological properties of unitary EPSPs. Pre- and postsynaptic neurones were filled with biocytin and examined in the light and electron microscope to quantify the morphology of axonal and dendritic arbors and the number and location of synaptic contacts on the target neurone. 2. In 138 synaptic connections between pairs of pyramidal neurones 96 (70\%) were unidirectional and 42 (30\%) were bidirectional. The probability of finding a synaptic connection in dual recordings was 0.1. Unitary EPSPs evoked by a single presynaptic action potential (AP) had a mean peak amplitude ranging from 0.15 to 5.5 mV in different connections with a mean of 1.3 +/- 1.1 mV, a latency of 1.7 +/- 0.9 ms, a 20-80\% rise time of 2.9 +/- 2.3 ms and a decay time constant of 40 +/- 18 ms at 32-24 degrees C and -60 +/- 2 mV membrane potential. 3. Peak amplitudes of unitary EPSPs fluctuated randomly from trial to trial. The coefficient of variation (c.v.) of the unitary EPSP amplitudes ranged from 0.13 to 2.8 in different synaptic connections (mean, 0.52; median, 0.41). The percentage of failures of single APs to evoke a unitary EPSP ranged from 0 to 73\% (mean, 14\%; median, 7\%). Both c.v. and percentage of failures decreased with increasing mean EPSP amplitude. 4. Postsynaptic glutamate receptors which mediate unitary EPSPs at -60 mV were predominantly of the L-alpha-amino-3-hydroxy-5-methyl-4-isoxazolepropionate (AMPA) receptor type. Receptors of the N-methyl-D-aspartate (NMDA) type contributed only a small fraction ($<$ 20\%) to the voltage-time integral of the unitary EPSP at -60 mV, but their contribution increased at more positive membrane potentials. 5. Branching patterns of dendrites and axon collaterals of forty-five synaptically connected neurones, when examined in the light microscope, indicated that the axonal and dendritic anatomy of both projecting and target neurones and of uni- and bidirectionally connected neurones was uniform. 6. The number of potential synaptic contacts formed by a presynaptic neurone on a target neurone varied between four and eight (mean, 5.5 +/- 1.1 contacts; n = 19 connections). Synaptic contacts were preferentially located on basal dendrites (63\%, 82 +/- 35 microns from the soma, n = 67) and apical oblique dendrites (27\%, 145 +/- 59 microns, n = 29), and 35\% of all contacts were located on tertiary basal dendritic branches. The mean geometric distances (from the soma) of the contacts of a connection varied between 80 and 585 microns (mean, 147 microns; median, 105 microns). The correlation between EPSP amplitude and the number of morphologically determined synaptic contacts or the mean geometric distances from the soma was only weak (correlation coefficients were 0.2 and 0.26, respectively). 7. Compartmental models constructed from camera lucida drawings of eight target neurones showed that synaptic contacts were located at mean electrotonic distances between 0.07 and 0.33 from the soma (mean, 0.13). Simulations of unitary EPSPs, assuming quantal conductance changes with fast rise time and short duration, indicated that amplitudes of quantal EPSPs at the soma were attenuated, on average, to $<$ 10\% of dendritic EPSPs and varied in amplitude up to 10-fold depending on the dendritic location of synaptic contacts. The inferred quantal peak conductance increase varied between 1.5 and 5.5 nS (mean, 3 nS). 8. The combined physiological and morphological measurements in conjunction with EPSP simulations indicated that the 20-fold range in efficacy of the synaptic connections between thick tufted pyramidal neurones, which have their synaptic contacts preferentially located on basal and apical oblique dendrites, was due to differences in transmitter release probability of the projecting neurones and, to a lesser extent, to differenc},
  number = {Pt 2},
  journal = {The Journal of Physiology},
  author = {Markram, Henry and L\"ubke, Joachim and Frotscher, Michael and Roth, Arnd and Sakmann, Bert},
  month = apr,
  year = {1997},
  keywords = {bidirs},
  pages = {409-440},
  file = {articles/Markram1997.pdf},
  pmid = {9147328},
  pmcid = {PMC1159394}
}

@article{Chung2015,
  title = {The Effect of Sensory Feedback on Crayfish Posture and Locomotion: {{I}}. {{Experimental}} Analysis of Closing the Loop},
  volume = {113},
  issn = {0022-3077, 1522-1598},
  shorttitle = {The Effect of Sensory Feedback on Crayfish Posture and Locomotion},
  doi = {10.1152/jn.00248.2014},
  language = {en},
  number = {6},
  journal = {Journal of Neurophysiology},
  author = {Chung, Bryce and {Bacqu\'e-Cazenave}, Julien and Cofer, David W. and Cattaert, Daniel and Edwards, Donald H.},
  month = mar,
  year = {2015},
  pages = {1763-1771},
  file = {articles/Chung2015.pdf}
}

@article{Kriener2009,
  title = {Correlations in Spiking Neuronal Networks with Distance Dependent Connections},
  volume = {27},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-008-0135-1},
  abstract = {Can the topology of a recurrent spiking network be inferred from observed activity dynamics? Which statistical parameters of network connectivity can be extracted from firing rates, correlations and related measurable quantities? To approach these questions, we analyze distance dependent correlations of the activity in small-world networks of neurons with current-based synapses derived from a simple ring topology. We find that in particular the distribution of correlation coefficients of subthreshold activity can tell apart random networks from networks with distance dependent connectivity. Such distributions can be estimated by sampling from random pairs. We also demonstrate the crucial role of the weight distribution, most notably the compliance with Dales principle, for the activity dynamics in recurrent networks of different types.},
  language = {en},
  number = {2},
  journal = {Journal of Computational Neuroscience},
  author = {Kriener, Birgit and Helias, Moritz and Aertsen, Ad and Rotter, Stefan},
  month = oct,
  year = {2009},
  keywords = {Distribution of correlation coefficients,Human Genetics,Neurology,Neurosciences,Pairwise correlations,Small-world networks,Spiking neural networks,Theory of Computation},
  pages = {177-200},
  file = {articles/Kriener2009.pdf}
}

@article{Koster2014,
  title = {Modeling {{Higher}}-{{Order Correlations}} within {{Cortical Microcolumns}}},
  volume = {10},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003684},
  language = {en},
  number = {7},
  journal = {PLoS Computational Biology},
  author = {K\"oster, Urs and {Sohl-Dickstein}, Jascha and Gray, Charles M. and Olshausen, Bruno A.},
  editor = {Macke, Jakob H.},
  month = jul,
  year = {2014},
  pages = {e1003684},
  file = {articles/Köster2014.pdf}
}

@article{Froemke2007,
  title = {A Synaptic Memory Trace for Cortical Receptive Field Plasticity},
  volume = {450},
  copyright = {\textcopyright{} 2007 Nature Publishing Group},
  issn = {0028-0836},
  doi = {10.1038/nature06289},
  abstract = {Receptive fields of sensory cortical neurons are plastic, changing in response to alterations of neural activity or sensory experience. In this way, cortical representations of the sensory environment can incorporate new information about the world, depending on the relevance or value of particular stimuli. Neuromodulation is required for cortical plasticity, but it is uncertain how subcortical neuromodulatory systems, such as the cholinergic nucleus basalis, interact with and refine cortical circuits. Here we determine the dynamics of synaptic receptive field plasticity in the adult primary auditory cortex (also known as AI) using in vivo whole-cell recording. Pairing sensory stimulation with nucleus basalis activation shifted the preferred stimuli of cortical neurons by inducing a rapid reduction of synaptic inhibition within seconds, which was followed by a large increase in excitation, both specific to the paired stimulus. Although nucleus basalis was stimulated only for a few minutes, reorganization of synaptic tuning curves progressed for hours thereafter: inhibition slowly increased in an activity-dependent manner to rebalance the persistent enhancement of excitation, leading to a retuned receptive field with new preference for the paired stimulus. This restricted period of disinhibition may be a fundamental mechanism for receptive field plasticity, and could serve as a memory trace for stimuli or episodes that have acquired new behavioural significance.},
  language = {en},
  number = {7168},
  journal = {Nature},
  author = {Froemke, Robert C. and Merzenich, Michael M. and Schreiner, Christoph E.},
  month = nov,
  year = {2007},
  pages = {425-429},
  file = {articles/Froemke2007.pdf}
}

@article{Hawkes1971,
  title = {Point {{Spectra}} of {{Some Mutually Exciting Point Processes}}},
  volume = {33},
  issn = {0035-9246},
  abstract = {The point spectral matrix is obtained for a class of mutually exciting point processes. The solution makes use of methods similar to those used in solving Wiener-Hopf integral equations.},
  number = {3},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  author = {Hawkes, Alan G.},
  year = {1971},
  pages = {438-443},
  file = {articles/Hawkes1971_2.pdf}
}

@article{Naumann2010,
  title = {Monitoring Neural Activity with Bioluminescence during Natural Behavior},
  volume = {13},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.2518},
  number = {4},
  journal = {Nature Neuroscience},
  author = {Naumann, Eva A and Kampff, Adam R and Prober, David A and Schier, Alexander F and Engert, Florian},
  month = apr,
  year = {2010},
  pages = {513-520},
  file = {articles/Naumann2010.pdf}
}

@article{Cohen2012,
  title = {Neuron-Type-Specific Signals for Reward and Punishment in the Ventral Tegmental Area},
  volume = {482},
  copyright = {\textcopyright{} 2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {0028-0836},
  doi = {10.1038/nature10754},
  abstract = {Dopamine has a central role in motivation and reward. Dopaminergic neurons in the ventral tegmental area (VTA) signal the discrepancy between expected and actual rewards (that is, reward prediction error), but how they compute such signals is unknown. We recorded the activity of VTA neurons while mice associated different odour cues with appetitive and aversive outcomes. We found three types of neuron based on responses to odours and outcomes: approximately half of the neurons (type I, 52\%) showed phasic excitation after reward-predicting odours and rewards in a manner consistent with reward prediction error coding; the other half of neurons showed persistent activity during the delay between odour and outcome that was modulated positively (type II, 31\%) or negatively (type III, 18\%) by the value of outcomes. Whereas the activity of type I neurons was sensitive to actual outcomes (that is, when the reward was delivered as expected compared to when it was unexpectedly omitted), the activity of type II and type III neurons was determined predominantly by reward-predicting odours. We /‘tagged/' dopaminergic and GABAergic neurons with the light-sensitive protein channelrhodopsin-2 and identified them based on their responses to optical stimulation while recording. All identified dopaminergic neurons were of type I and all GABAergic neurons were of type II. These results show that VTA GABAergic neurons signal expected reward, a key variable for dopaminergic neurons to calculate reward prediction error.},
  language = {en},
  number = {7383},
  journal = {Nature},
  author = {Cohen, Jeremiah Y. and Haesler, Sebastian and Vong, Linh and Lowell, Bradford B. and Uchida, Naoshige},
  month = feb,
  year = {2012},
  keywords = {Animal behaviour,Neuroscience},
  pages = {85-88},
  file = {articles/Cohen2012.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/IIF3W3DZ/nature10754.html}
}

@article{Brunel2016,
  title = {Is Cortical Connectivity Optimized for Storing Information?},
  volume = {19},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4286},
  number = {5},
  journal = {Nature Neuroscience},
  author = {Brunel, Nicolas},
  month = apr,
  year = {2016},
  pages = {749-755},
  file = {/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/4BGJFC9Z/Brunel2016_S1.pdf;articles/Brunel2016.pdf}
}

@article{Turrigiano2008,
  title = {The {{Self}}-{{Tuning Neuron}}: {{Synaptic Scaling}} of {{Excitatory Synapses}}},
  volume = {135},
  issn = {0092-8674},
  shorttitle = {The {{Self}}-{{Tuning Neuron}}},
  doi = {10.1016/j.cell.2008.10.008},
  abstract = {Homeostatic synaptic scaling is a form of synaptic plasticity that adjusts the strength of all of a neuron's excitatory synapses up or down to stabilize firing. Current evidence suggests that neurons detect changes in their own firing rates through a set of calcium-dependent sensors that then regulate receptor trafficking to increase or decrease the accumulation of glutamate receptors at synaptic sites. Additional mechanisms may allow local or network-wide changes in activity to be sensed through parallel pathways, generating a nested set of homeostatic mechanisms that operate over different temporal and spatial scales.},
  number = {3},
  journal = {Cell},
  author = {Turrigiano, Gina G.},
  month = oct,
  year = {2008},
  pages = {422-435},
  file = {articles/Turrigiano2008.pdf},
  pmid = {18984155},
  pmcid = {PMC2834419}
}

@article{Jensen1906,
  title = {{Sur les fonctions convexes et les in\'egalit\'es entre les valeurs moyennes}},
  volume = {30},
  issn = {0001-5962, 1871-2509},
  doi = {10.1007/BF02418571},
  language = {fr},
  number = {1},
  journal = {Acta Mathematica},
  author = {Jensen, J. L. W. V.},
  month = dec,
  year = {1906},
  keywords = {Mathematics; general},
  pages = {175-193},
  file = {articles/Jensen1906.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/FK4A423H/10.html}
}

@unpublished{Waldmann2011,
  title = {Operator-{{Algebraic Methods}} in {{Quantum Mechanics}}},
  author = {Waldmann, Stefan},
  year = {2011},
  file = {manuscripts/Waldmann2011_Operator-Algebraic-Methods-in-Quantum-Mechanics_2.pdf}
}

@book{Haykin2002,
  address = {New York},
  edition = {2nd ed},
  title = {Signals and Systems},
  isbn = {978-0-471-16474-6},
  lccn = {TK5102.5 .H37 2002},
  publisher = {{Wiley}},
  author = {Haykin, Simon S. and Van Veen, Barry},
  year = {2002},
  keywords = {Linear time invariant systems,Signal processing,System analysis,Telecommunication systems},
  file = {books/Haykin2002_Signals-and-systems.djvu}
}

@article{Weckstrom2010,
  title = {Intracellular Recording},
  volume = {5},
  issn = {1941-6016},
  doi = {10.4249/scholarpedia.2224},
  number = {8},
  journal = {Scholarpedia},
  author = {Weckstrom, Matti},
  year = {2010},
  pages = {2224}
}

@article{Morante2008,
  title = {The Color Vision Circuit in the Medulla of {{Drosophila}}},
  volume = {18},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2008.02.075},
  abstract = {Background
Color vision requires comparison between photoreceptors that are sensitive to different wavelengths of light. In Drosophila, this is achieved by the inner photoreceptors (R7 and R8) that contain different rhodopsins. Two types of comparisons can occur in fly color vision: between the R7 (UV-sensitive) and R8 (blue or green-sensitive) photoreceptor cells within one ommatidium (unit eye); or between different ommatidia that contain spectrally distinct inner photoreceptors. Photoreceptors project to the optic lobes: R1-6, which are involved in motion detection, project to the lamina, while R7 and R8 reach deeper in the medulla. This paper analyzes the neural network underlying color vision in the medulla.

Results
We reconstruct the neural network in the medulla, focusing on neurons likely to be involved in processing color vision. We identify the full complement of neurons in the medulla, including second order neurons that contact both R7 and R8 from a single ommatidium, or contact R7 and/or R8 from different ommatidia. We also examine third order neurons and local neurons that likely modulate information from second order neurons. Finally, we present highly specific tools that will allow us to functionally manipulate the network and test both activity and behavior.

Conclusions
This precise characterization of the medulla circuitry will allow us to understand how color vision is processed in the optic lobe of Drosophila, providing a paradigm for more complex systems in vertebrates.},
  number = {8},
  journal = {Current biology : CB},
  author = {Morante, Javier and Desplan, Claude},
  month = apr,
  year = {2008},
  pages = {553-565},
  file = {articles/Morante2008.pdf;articles/Morante2008.pdf},
  pmid = {18403201},
  pmcid = {PMC2430089}
}

@article{Vasilaki2014,
  title = {Emergence of {{Connectivity Motifs}} in {{Networks}} of {{Model Neurons}} with {{Short}}- and {{Long}}-{{Term Plastic Synapses}}},
  volume = {9},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0084626},
  abstract = {Recent experimental data from the rodent cerebral cortex and olfactory bulb indicate that specific connectivity motifs are correlated with short-term dynamics of excitatory synaptic transmission. It was observed that neurons with short-term facilitating synapses form predominantly reciprocal pairwise connections, while neurons with short-term depressing synapses form predominantly unidirectional pairwise connections. The cause of these structural differences in excitatory synaptic microcircuits is unknown. We show that these connectivity motifs emerge in networks of model neurons, from the interactions between short-term synaptic dynamics (SD) and long-term spike-timing dependent plasticity (STDP). While the impact of STDP on SD was shown in simultaneous neuronal pair recordings  in vitro , the mutual interactions between STDP and SD in large networks are still the subject of intense research. Our approach combines an SD phenomenological model with an STDP model that faithfully captures long-term plasticity dependence on both spike times and frequency. As a proof of concept, we first simulate and analyze recurrent networks of spiking neurons with random initial connection efficacies and where synapses are either all short-term facilitating or all depressing. For identical external inputs to the network, and as a direct consequence of internally generated activity, we find that networks with depressing synapses evolve unidirectional connectivity motifs, while networks with facilitating synapses evolve reciprocal connectivity motifs. We then show that the same results hold for heterogeneous networks, including both facilitating and depressing synapses. This does not contradict a recent theory that proposes that motifs are shaped by external inputs, but rather complements it by examining the role of both the external inputs and the internally generated network activity. Our study highlights the conditions under which SD-STDP might explain the correlation between facilitation and reciprocal connectivity motifs, as well as between depression and unidirectional motifs.},
  number = {1},
  journal = {PLOS ONE},
  author = {Vasilaki, Eleni and Giugliano, Michele},
  month = jan,
  year = {2014},
  keywords = {Action potentials,Depression,Network motifs,Neuronal plasticity,Neurons,Synapses,neural networks,synaptic plasticity},
  pages = {e84626},
  file = {articles/Vasilaki2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/TZU9RP6D/article.html}
}

@article{Butz2013,
  title = {A {{Simple Rule}} for {{Dendritic Spine}} and {{Axonal Bouton Formation Can Account}} for {{Cortical Reorganization}} after {{Focal Retinal Lesions}}},
  volume = {9},
  doi = {10.1371/journal.pcbi.1003259},
  abstract = {Author SummaryThe adult brain is less hard-wired than traditionally thought. About ten percent of synapses in the mature visual cortex is continually replaced by new ones (structural plasticity). This percentage greatly increases after lasting changes in visual input. Due to the topographically organized nerve connections from the retina in the eye to the primary visual cortex in the brain, a small circumscribed lesion in the retina leads to a defined area in the cortex that is deprived of input. Recent experimental studies have revealed that axonal sprouting and dendritic spine turnover are massively increased in and around the cortical area that is deprived of input. However, the driving forces for this structural plasticity remain unclear. Using a novel computational model, we examine whether the need for activity homeostasis of individual neurons may drive cortical reorganization after lasting changes in input activity. We show that homeostatic growth rules indeed give rise to structural and functional reorganization of neuronal networks similar to the cortical reorganization observed experimentally. Understanding the principles of structural plasticity may eventually lead to novel treatment strategies for stimulating functional reorganization after brain damage and neurodegeneration.},
  number = {10},
  journal = {PLoS Comput Biol},
  author = {Butz, Markus and {van Ooyen}, Arjen},
  month = oct,
  year = {2013},
  pages = {e1003259},
  file = {articles/Butz2013.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/UIZ7AQAG/infodoi10.1371journal.pcbi.html}
}

@article{Ohki2007,
  series = {Sensory systems},
  title = {Specificity and Randomness in the Visual Cortex},
  volume = {17},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2007.07.007},
  abstract = {Research on the functional anatomy of visual cortical circuits has recently zoomed in from the macroscopic level to the microscopic. High-resolution functional imaging has revealed that the functional architecture of orientation maps in higher mammals is built with single-cell precision. By contrast, orientation selectivity in rodents is dispersed on visual cortex in a salt-and-pepper fashion, despite highly tuned visual responses. Recent studies of synaptic physiology indicate that there are disjoint subnetworks of interconnected cells in the rodent visual cortex. These intermingled subnetworks, described in vitro, may relate to the intermingled ensembles of cells tuned to different orientations, described in vivo. This hypothesis may soon be tested with new anatomic techniques that promise to reveal the detailed wiring diagram of cortical circuits.},
  number = {4},
  journal = {Current Opinion in Neurobiology},
  author = {Ohki, Kenichi and Reid, R Clay},
  month = aug,
  year = {2007},
  pages = {401-407},
  file = {articles/Ohki2007.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/CDZVKC83/S0959438807000906.html}
}

@article{Woolsey1970,
  title = {The Structural Organization of Layer {{IV}} in the Somatosensory Region ({{S I}}) of Mouse Cerebral Cortex: {{The}} Description of a Cortical Field Composed of Discrete Cytoarchitectonic Units},
  volume = {17},
  issn = {0006-8993},
  shorttitle = {The Structural Organization of Layer {{IV}} in the Somatosensory Region ({{S I}}) of Mouse Cerebral Cortex},
  doi = {10.1016/0006-8993(70)90079-X},
  number = {2},
  journal = {Brain Research},
  author = {Woolsey, Thomas A. and {Van der Loos}, Hendrik},
  month = jan,
  year = {1970},
  pages = {205-242},
  file = {articles/Woolsey1970.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/7AM2G6HW/000689937090079X.html}
}

@article{Heju2015,
  title = {Helmholtz {{Juniors PhD}} Survey 2014},
  author = {Heju},
  year = {2015},
  file = {articles/Heju2015.pdf}
}

@book{Abeles2011,
  address = {S.l.},
  edition = {Softcover reprint of the original 1st ed. 1982 edition},
  title = {Local {{Cortical Circuits}}: {{An Electrophysiological Study}}},
  isbn = {978-3-642-81710-6},
  shorttitle = {Local {{Cortical Circuits}}},
  abstract = {Neurophysiologists are often accused by colleagues in the physical sci$\-$ ences of designing experiments without any underlying hypothesis. This impression is attributable to the ease of getting lost in the ever-increasing sea of professional publications which do not state explicitly the ultimate goal of the research. On the other hand, many of the explicit models for brain function in the past were so far removed from experimental reality that they had very little impact on further research. It seems that one needs much intimate experience with the real nerv-. ous system before a reasonable model can be suggested. It would have been impossible for Copernicus to suggest his model of the solar system without the detailed observations and tabulations of star and planet motion accu$\-$ mulated by the preceeding generations. This need for intimate experience with the nervous system before daring to put forward some hypothesis about its mechanism of action is especially apparent when theorizing about cerebral cortex function. There is widespread agreement that processing of information in the cor$\-$ tex is associated with complex spatio-temporal patterns of activity. Yet the vast majority of experimental work is based on single neuron recordings or on recordings made with gross electrodes to which tens of thousands of neurons contribute in an unknown fashion. Although these experiments have taught us a great deal about the organization and function of the cor$\-$ tex, they have not enabled us to examine the spatio-temporal organization of neuronal activity in any detail.},
  language = {English},
  publisher = {{Springer}},
  author = {Abeles, Moshe},
  month = dec,
  year = {2011},
  file = {books/Abeles2011_Local-Cortical-Circuits-An-Electrophysiological-Study.pdf}
}

@article{Karube2004,
  title = {Axon {{Branching}} and {{Synaptic Bouton Phenotypes}} in {{GABAergic Nonpyramidal Cell Subtypes}}},
  volume = {24},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4814-03.2004},
  abstract = {GABAergic nonpyramidal cells, cortical interneurons, consist of heterogeneous subtypes differing in their axonal field and target selectivity. It remains to be investigated how the diverse innervation patterns are generated and how these spatially complicated, but synaptically specific wirings are achieved. Here, we asked whether a particular cell type obeys a specific branching and bouton arrangement principle or differs from others only in average morphometric values of the morphological template common to nonpyramidal cells. For this purpose, we subclassified nonpyramidal cells within each physiological class by quantitative parameters of somata, dendrites, and axons and characterized axon branching and bouton distribution patterns quantitatively. Each subtype showed a characteristic set of vertical and horizontal bouton spreads around the somata. Each parameter, such as branching angles, internode or interbouton intervals, followed its own characteristic distribution pattern irrespective of subtypes, suggesting that nonpyramidal cells have the common mechanism for formation of the axon branching pattern and bouton arrangement. Fitting of internode and interbouton interval distributions to the exponential indicated their apparent random occurrence. Decay constants of the fitted exponentials varied among nonpyramidal cells, but each subtype expressed a particular set of interbouton and internode interval averages. The distinctive combination of innervation field shape and local axon phenotypes suggests a marked functional difference in the laminar and columnar integration properties of different GABAergic subtypes, as well as the subtype-specific density of inhibited targets.},
  language = {en},
  number = {12},
  journal = {The Journal of Neuroscience},
  author = {Karube, Fuyuki and Kubota, Yoshiyuki and Kawaguchi, Yasuo},
  month = mar,
  year = {2004},
  keywords = {GABA,fast-spiking cell,frontal cortex,interneuron,late-spiking cell,synaptic bouton},
  pages = {2853-2865},
  file = {articles/Karube2004.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/HEX8BIAZ/2853.html},
  pmid = {15044524}
}

@book{Schuppar2015,
  address = {Berlin},
  series = {Mathematik Primar- und Sekundarstufe I + II},
  title = {{Elementare Numerik f\"ur die Sekundarstufe}},
  isbn = {978-3-662-43479-6 978-3-662-43478-9},
  language = {ger},
  publisher = {{Springer Spektrum}},
  author = {Schuppar, Berthold and Humenberger, Hans},
  year = {2015},
  keywords = {Einführung,Numerische Mathematik},
  file = {books/Schuppar2015_Elementare-Numerik-für-die-Sekundarstufe.pdf}
}

@unpublished{Hoffmann2015b,
  title = {Funktionalanalysis - {{Zusammenfassung}}},
  author = {Hoffmann, Felix},
  year = {2015},
  file = {manuscripts/Hoffmann2015_Funktionalanalysis---Zusammenfassung.pdf}
}

@book{Galizia2013,
  address = {Heidelberg ; New York},
  title = {Neurosciences: From Molecule to Behavior: A University Textbook},
  isbn = {978-3-642-10768-9},
  lccn = {RC341 .N4375 2013},
  shorttitle = {Neurosciences},
  publisher = {{Springer Spektrum}},
  editor = {Galizia, C. Giovanni and Lledo, Pierre-Marie},
  year = {2013},
  keywords = {Nervous System Physiological Phenomena,Neurosciences},
  file = {books/Galizia2013_Neurosciences-from-molecule-to-behavior-a-university-textbook.pdf}
}

@article{Hay2011,
  title = {Models of {{Neocortical Layer}} 5b {{Pyramidal Cells Capturing}} a {{Wide Range}} of {{Dendritic}} and {{Perisomatic Active Properties}}},
  volume = {7},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002107},
  language = {en},
  number = {7},
  journal = {PLoS Computational Biology},
  author = {Hay, Etay and Hill, Sean and Sch\"urmann, Felix and Markram, Henry and Segev, Idan},
  editor = {Graham, Lyle J.},
  month = jul,
  year = {2011},
  pages = {e1002107},
  file = {articles/Hay2011.pdf}
}

@article{Brunel2001,
  title = {Effects of {{Synaptic Noise}} and {{Filtering}} on the {{Frequency Response}} of {{Spiking Neurons}}},
  volume = {86},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.86.2186},
  language = {en},
  number = {10},
  journal = {Physical Review Letters},
  author = {Brunel, Nicolas and Chance, Frances S. and Fourcaud, Nicolas and Abbott, L. F.},
  month = mar,
  year = {2001},
  pages = {2186-2189},
  file = {articles/Brunel2001.pdf}
}

@article{Hagihara2015a,
  title = {Neuronal Activity Is Not Required for the Initial Formation and Maturation of Visual Selectivity},
  volume = {18},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4155},
  number = {12},
  journal = {Nature Neuroscience},
  author = {Hagihara, Kenta M and Murakami, Tomonari and Yoshida, Takashi and Tagawa, Yoshiaki and Ohki, Kenichi},
  month = nov,
  year = {2015},
  pages = {1780-1788},
  file = {articles/Hagihara2015.pdf}
}

@book{Spanier1981,
  address = {New York, NY},
  title = {Algebraic {{Topology}}},
  isbn = {978-0-387-94426-5 978-1-4684-9322-1},
  language = {en},
  publisher = {{Springer New York}},
  author = {Spanier, Edwin H.},
  year = {1981},
  file = {books/Spanier1981_Algebraic-Topology.djvu}
}

@article{Kleindienst2011,
  title = {Activity-{{Dependent Clustering}} of {{Functional Synaptic Inputs}} on {{Developing Hippocampal Dendrites}}},
  volume = {72},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2011.10.015},
  abstract = {Summary
During brain development, before sensory systems become functional, neuronal networks spontaneously generate repetitive bursts of neuronal activity, which are typically synchronized across many neurons. Such activity patterns have been described on the level of networks and cells, but the fine-structure of inputs received by an individual neuron during spontaneous network activity has not been studied. Here, we used calcium imaging to record activity at many synapses of hippocampal pyramidal neurons simultaneously to establish the activity patterns in the majority of synapses of an entire cell. Analysis of the spatiotemporal patterns of synaptic activity revealed a fine-scale connectivity rule: neighboring synapses (\&lt;16~$\mu$m intersynapse distance) are more likely to be coactive than synapses that are farther away from each other. Blocking spiking activity or NMDA receptor activation revealed that the clustering of synaptic inputs required neuronal activity, demonstrating a role of developmentally expressed spontaneous activity for connecting neurons with subcellular precision.},
  number = {6},
  journal = {Neuron},
  author = {Kleindienst, Thomas and Winnubst, Johan and {Roth-Alpermann}, Claudia and Bonhoeffer, Tobias and Lohmann, Christian},
  month = dec,
  year = {2011},
  pages = {1012-1024},
  file = {articles/Kleindienst20112.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/DTS7BTCK/S0896627311009263.html}
}

@techreport{zotero-276,
  title = {{{TikZ}} and {{PGF}} 1.18},
  file = {manuals/latex/tikz_and_pgf_1.18.pdf},
  note = {manuals/latex}
}

@unpublished{Hoffmann2015j,
  title = {Stochastische {{Prozesse}}},
  author = {Hoffmann, Felix},
  year = {2015},
  file = {manuscripts/Hoffmann2015_Stochastische-Prozesse.pdf}
}

@article{Hilgetag2004,
  title = {Clustered Organization of Cortical Connectivity},
  volume = {2},
  issn = {1539-2791, 1559-0089},
  doi = {10.1385/NI:2:3:353},
  abstract = {Long-range corticocortical connectivity in mammalian brains possesses an intricate, nonrandom organization. Specifically, projections are arranged in `small-world' networks, forming clusters of cortical areas, which are closely linked among each other, but less frequently with areas in other clusters. In order to delineate the structure of cortical clusters and identify their members, we developed a computational approach based on evolutionary optimization. In different compilations of connectivity data for the cat and macaque monkey brain, the algorithm identified a small number of clusters that broadly agreed with functional cortical subdivisions. We propose a simple spatial growth model for evolving clustered connectivity, and discuss structural and functional implications of the clustered, small-world organization of cortical networks.},
  language = {en},
  number = {3},
  journal = {Neuroinformatics},
  author = {Hilgetag, Claus C. and Kaiser, Marcus},
  month = sep,
  year = {2004},
  keywords = {Biotechnology,Engineering; general,Neurology,Rhesus macaque monkey,Small-world networks,cat,cluster analysis,cortical development,network function,neural networks,robustness,scale-free networks,spatial growth,vulnerability},
  pages = {353-360},
  file = {articles/Hilgetag2004.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/NNQ3KQ9C/NI23353.html}
}

@article{Maggio2014,
  title = {Synaptic Plasticity at the Interface of Health and Disease: {{New}} Insights on the Role of Endoplasmic Reticulum Intracellular Calcium Stores},
  volume = {281},
  issn = {03064522},
  shorttitle = {Synaptic Plasticity at the Interface of Health and Disease},
  doi = {10.1016/j.neuroscience.2014.09.041},
  language = {en},
  journal = {Neuroscience},
  author = {Maggio, N. and Vlachos, A.},
  month = dec,
  year = {2014},
  pages = {135-146},
  file = {articles/Maggio2014.pdf}
}

@article{Stepanyants2004,
  title = {Class-{{Specific Features}} of {{Neuronal Wiring}}},
  volume = {43},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2004.06.013},
  abstract = {Brain function relies on specificity of synaptic connectivity patterns among different classes of neurons. Yet, the substrates of specificity in complex neuropil remain largely unknown. We search for imprints of specificity in the layout of axonal and dendritic arbors from the rat neocortex. An analysis of 3D reconstructions of pairs consisting of pyramidal cells (PCs) and GABAergic interneurons (GIs) revealed that the layout of GI axons is specific. This specificity is manifested in a relatively high tortuosity, small branch length of these axons, and correlations of their trajectories with the positions of postsynaptic neuron dendrites. Axons of PCs show no such specificity, usually taking a relatively straight course through neuropil. However, wiring patterns among PCs hold a large potential for circuit remodeling and specificity through growth and retraction of dendritic spines. Our results define distinct class-specific rules in establishing synaptic connectivity, which could be crucial in formulating a canonical cortical circuit.},
  language = {English},
  number = {2},
  journal = {Neuron},
  author = {Stepanyants, Armen and Tam\'as, G\'abor and Chklovskii, Dmitri B.},
  month = jul,
  year = {2004},
  pages = {251-259},
  file = {articles/Stepanyants2004.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/XIDSEI9T/S0896-6273(04)00362-9.html},
  pmid = {15260960, 15260960}
}

@book{Janson2000a,
  address = {New York; Chichester},
  title = {Theory of Random Graphs},
  isbn = {0-471-17541-2 978-0-471-17541-4},
  language = {English},
  publisher = {{John Wiley \& Sons}},
  author = {Janson, Svante and \L{}uczak, Tomasz and Ruci\'nski, Andrzej},
  year = {2000}
}

@book{Gerstner2002,
  address = {Cambridge, U.K. ; New York},
  edition = {1 edition},
  title = {Spiking {{Neuron Models}}: {{Single Neurons}}, {{Populations}}, {{Plasticity}}},
  isbn = {978-0-521-89079-3},
  shorttitle = {Spiking {{Neuron Models}}},
  abstract = {This introduction to spiking neurons can be used in advanced-level courses in computational neuroscience, theoretical biology, neural modeling, biophysics, or neural networks. It focuses on phenomenological approaches rather than detailed models in order to provide the reader with a conceptual framework. The authors formulate the theoretical concepts clearly without many mathematical details. While the book contains standard material for courses in computational neuroscience, neural modeling, or neural networks, it also provides an entry to current research. No prior knowledge beyond undergraduate mathematics is required.},
  language = {English},
  publisher = {{Cambridge University Press}},
  author = {Gerstner, Wulfram and Kistler, Werner M.},
  month = aug,
  year = {2002},
  file = {books/Gerstner2002_Spiking-Neuron-Models-Single-Neurons,-Populations,-Plasticity.pdf}
}

@book{Ross2009,
  address = {Upper Saddle River, N.J},
  edition = {8th edition},
  title = {A {{First Course}} in {{Probability}}},
  isbn = {978-0-13-603313-4},
  abstract = {A First Course in Probability, Eighth Edition, features clear and intuitive explanations of the mathematics of probability theory, outstanding problem sets, and a variety of diverse examples and applications. This book is ideal for an upper-level undergraduate or graduate level introduction to probability for math, science, engineering and business students. It assumes a background in elementary calculus.},
  language = {English},
  publisher = {{Pearson Prentice Hall}},
  author = {Ross, Sheldon},
  month = jan,
  year = {2009},
  file = {books/Ross2009_A-First-Course-in-Probability.pdf}
}

@book{Cuntz2014,
  address = {New York},
  series = {Springer series in computational neuroscience},
  title = {The Computing Dendrite: From Structure to Function},
  isbn = {978-1-4614-8093-8},
  lccn = {QP363 .C58 2014},
  shorttitle = {The Computing Dendrite},
  number = {volume 11},
  publisher = {{Springer}},
  editor = {Cuntz, Hermann and Remme, Michiel W. H. and {Torben-Nielsen}, Benjamin},
  year = {2014},
  keywords = {Dendrites,computational neuroscience},
  file = {books/Cuntz2014_The-computing-dendrite-from-structure-to-function.pdf},
  note = {OCLC: ocn882609497}
}

@article{Kriener2008,
  title = {Correlations and {{Population Dynamics}} in {{Cortical Networks}}},
  volume = {20},
  issn = {0899-7667},
  doi = {10.1162/neco.2008.02-07-474},
  abstract = {The function of cortical networks depends on the collective interplay between neurons and neuronal populations, which is reflected in the correlation of signals that can be recorded at different levels. To correctly interpret these observations it is important to understand the origin of neuronal correlations. Here we study how cells in large recurrent networks of excitatory and inhibitory neurons interact and how the associated correlations affect stationary states of idle network activity. We demonstrate that the structure of the connectivity matrix of such networks induces considerable correlations between synaptic currents as well as between subthreshold membrane potentials, provided Dale's principle is respected. If, in contrast, synaptic weights are randomly distributed, input correlations can vanish, even for densely connected networks. Although correlations are strongly attenuated when proceeding from membrane potentials to action potentials (spikes), the resulting weak correlations in the spike output can cause substantial fluctuations in the population activity, even in highly diluted networks. We show that simple mean-field models that take the structure of the coupling matrix into account can adequately describe the power spectra of the population activity. The consequences of Dale's principle on correlations and rate fluctuations are discussed in the light of recent experimental findings.},
  number = {9},
  journal = {Neural Computation},
  author = {Kriener, Birgit and Tetzlaff, Tom and Aertsen, Ad and Diesmann, Markus and Rotter, Stefan},
  month = apr,
  year = {2008},
  pages = {2185-2226},
  file = {articles/Kriener2008.pdf}
}

@article{Larkum2009,
  title = {Synaptic {{Integration}} in {{Tuft Dendrites}} of {{Layer}} 5 {{Pyramidal Neurons}}: {{A New Unifying Principle}}},
  volume = {325},
  copyright = {Copyright \textcopyright{} 2009, American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  shorttitle = {Synaptic {{Integration}} in {{Tuft Dendrites}} of {{Layer}} 5 {{Pyramidal Neurons}}},
  doi = {10.1126/science.1171958},
  abstract = {Fine Dendrites Fire Differently
The pyramidal neuron is the basic computational unit in the brain cortex. Its distal tuft dendrite is heavily innervated by horizontal fibers coursing through cortical layer-I providing long-range corticocortical and thalamocortical associational input. Larkum et al. (p. 756) investigated whether the apical tuft dendrites of layer-5 neocortical pyramidal neurons, like basal dendrites, generate n-methyl-d-aspartate (NMDA) spikes using two-photon\textendash{}guided direct dendritic recording, glutamate uncaging, and modeling. NMDA spikes could be evoked in the distal tuft dendrites, while Ca2+ spikes could be triggered at the bifurcation points. Block of the hyperpolarization-activated current enhanced these NMDA spikes. Thus, the generation of NMDA spikes is a general principle of thin, basal, and tuft dendrites.
Tuft dendrites are the main target for feedback inputs innervating neocortical layer 5 pyramidal neurons, but their properties remain obscure. We report the existence of N-methyl-d-aspartate (NMDA) spikes in the fine distal tuft dendrites that otherwise did not support the initiation of calcium spikes. Both direct measurements and computer simulations showed that NMDA spikes are the dominant mechanism by which distal synaptic input leads to firing of the neuron and provide the substrate for complex parallel processing of top-down input arriving at the tuft. These data lead to a new unifying view of integration in pyramidal neurons in which all fine dendrites, basal and tuft, integrate inputs locally through the recruitment of NMDA receptor channels relative to the fixed apical calcium and axosomatic sodium integration points.
Thin tuft and basal dendrites of pyramidal neurons use N-methyl-d-aspartate spikes to sum up synaptic inputs in semi-independent compartments.
Thin tuft and basal dendrites of pyramidal neurons use N-methyl-d-aspartate spikes to sum up synaptic inputs in semi-independent compartments.},
  language = {en},
  number = {5941},
  journal = {Science},
  author = {Larkum, Matthew E. and Nevian, Thomas and Sandler, Maya and Polsky, Alon and Schiller, Jackie},
  month = aug,
  year = {2009},
  pages = {756-760},
  file = {articles/Larkum2009.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/8QSB5674/756.html},
  pmid = {19661433}
}

@article{Costa2013,
  title = {Probabilistic Inference of Short-Term Synaptic Plasticity in Neocortical Microcircuits},
  volume = {7},
  issn = {1662-5188},
  doi = {10.3389/fncom.2013.00075},
  abstract = {Short-term synaptic plasticity is highly diverse across brain area, cortical layer, cell type, and developmental stage. Since short-term plasticity (STP) strongly shapes neural dynamics, this diversity suggests a specific and essential role in neural information processing. Therefore, a correct characterization of short-term synaptic plasticity is an important step towards understanding and modeling neural systems. Phenomenological models have been developed, but they are usually fitted to experimental data using least-mean-square methods. We demonstrate that for typical synaptic dynamics such fitting may give unreliable results. As a solution, we introduce a Bayesian formulation, which yields the posterior distribution over the model parameters given the data. First, we show that common STP protocols yield broad distributions over some model parameters. Using our result we propose a experimental protocol to more accurately determine synaptic dynamics parameters. Next, we infer the model parameters using experimental data from three different neocortical excitatory connection types. This reveals connection-specific distributions, which we use to classify synaptic dynamics. Our approach to demarcate connection-specific synaptic dynamics is an important improvement on the state of the art and reveals novel features from existing data.},
  journal = {Frontiers in Computational Neuroscience},
  author = {Costa, Rui P. and Sjostrom, P. Jesper and {van Rossum}, Mark C. W.},
  month = jun,
  year = {2013},
  file = {articles/Costa2013.pdf},
  pmid = {23761760},
  pmcid = {PMC3674479}
}

@article{Hiratani2014,
  title = {Interplay between {{Short}}- and {{Long}}-{{Term Plasticity}} in {{Cell}}-{{Assembly Formation}}},
  volume = {9},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0101535},
  abstract = {Various hippocampal and neocortical synapses of mammalian brain show both short-term plasticity and long-term plasticity, which are considered to underlie learning and memory by the brain. According to Hebb's postulate, synaptic plasticity encodes memory traces of past experiences into cell assemblies in cortical circuits. However, it remains unclear how the various forms of long-term and short-term synaptic plasticity cooperatively create and reorganize such cell assemblies. Here, we investigate the mechanism in which the three forms of synaptic plasticity known in cortical circuits, i.e., spike-timing-dependent plasticity (STDP), short-term depression (STD) and homeostatic plasticity, cooperatively generate, retain and reorganize cell assemblies in a recurrent neuronal network model. We show that multiple cell assemblies generated by external stimuli can survive noisy spontaneous network activity for an adequate range of the strength of STD. Furthermore, our model predicts that a symmetric temporal window of STDP, such as observed in dopaminergic modulations on hippocampal neurons, is crucial for the retention and integration of multiple cell assemblies. These results may have implications for the understanding of cortical memory processes.},
  number = {7},
  journal = {PLOS ONE},
  author = {Hiratani, Naoki and Fukai, Tomoki},
  month = jul,
  year = {2014},
  keywords = {Action potentials,Memory,Network analysis,Neuronal plasticity,Neurons,Synapses,neural networks,synaptic plasticity},
  pages = {e101535},
  file = {articles/Hiratani2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/XXWQKQHX/article.html}
}

@article{Ko2013,
  title = {The Emergence of Functional Microcircuits in Visual Cortex},
  volume = {496},
  copyright = {\textcopyright{} 2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {0028-0836},
  doi = {10.1038/nature12015},
  abstract = {Sensory processing occurs in neocortical microcircuits in which synaptic connectivity is highly structured and excitatory neurons form subnetworks that process related sensory information. However, the developmental mechanisms underlying the formation of functionally organized connectivity in cortical microcircuits remain unknown. Here we directly relate patterns of excitatory synaptic connectivity to visual response properties of neighbouring layer 2/3 pyramidal neurons in mouse visual cortex at different postnatal ages, using two-photon calcium imaging in vivo and multiple whole-cell recordings in vitro. Although neural responses were already highly selective for visual stimuli at eye opening, neurons responding to similar visual features were not yet preferentially connected, indicating that the emergence of feature selectivity does not depend on the precise arrangement of local synaptic connections. After eye opening, local connectivity reorganized extensively: more connections formed selectively between neurons with similar visual responses and connections were eliminated between visually unresponsive neurons, but the overall connectivity rate did not change. We propose a sequential model of cortical microcircuit development based on activity-dependent mechanisms of plasticity whereby neurons first acquire feature preference by selecting feedforward inputs before the onset of sensory experience\textemdash{}a process that may be facilitated by early electrical coupling between neuronal subsets\textemdash{}and then patterned input drives the formation of functional subnetworks through a redistribution of recurrent synaptic connections.},
  language = {en},
  number = {7443},
  journal = {Nature},
  author = {Ko, Ho and Cossell, Lee and Baragli, Chiara and Antolik, Jan and Clopath, Claudia and Hofer, Sonja B. and {Mrsic-Flogel}, Thomas D.},
  month = apr,
  year = {2013},
  keywords = {Cellular neuroscience,Striate cortex,Synaptic development},
  pages = {96-100},
  file = {articles/Ko2013.pdf}
}

@book{Chaovalitwongse2010,
  address = {New York},
  edition = {2010 edition},
  title = {Computational {{Neuroscience}}},
  isbn = {978-0-387-88629-9},
  abstract = {This volume includes contributions from diverse disciplines including electrical engineering, biomedical engineering, industrial engineering, and medicine, bridging a vital gap between the mathematical sciences and neuroscience research. Covering a wide range of research topics, this volume demonstrates how various methods from data mining, signal processing, optimization and cutting-edge medical techniques can be used to tackle the most challenging problems in~modern neuroscience.},
  language = {English},
  publisher = {{Springer}},
  editor = {Chaovalitwongse, Wanpracha and Pardalos, Panos M. and Xanthopoulos, Petros},
  month = may,
  year = {2010},
  file = {books/Chaovalitwongse2010_Computational-Neuroscience.pdf}
}

@book{Peters1984,
  address = {New York},
  title = {Cerebral Cortex},
  isbn = {0-306-41544-5 978-0-306-41544-9 0-306-43635-3 978-0-306-43635-2 0-306-45727-X 978-0-306-45727-2 0-306-44605-7 978-0-306-44605-4 0-306-45530-7 978-0-306-45530-8},
  language = {English},
  publisher = {{Plenum Press}},
  author = {Peters, Alan and Jones, Edward G},
  year = {1984}
}

@article{Mainen1995,
  title = {Reliability of Spike Timing in Neocortical Neurons},
  volume = {268},
  issn = {0036-8075},
  abstract = {It is not known whether the variability of neural activity in the cerebral cortex carries information or reflects noisy underlying mechanisms. In an examination of the reliability of spike generation using recordings from neurons in rat neocortical slices, the precision of spike timing was found to depend on stimulus transients. Constant stimuli led to imprecise spike trains, whereas stimuli with fluctuations resembling synaptic activity produced spike trains with timing reproducible to less than 1 millisecond. These data suggest a low intrinsic noise level in spike generation, which could allow cortical neurons to accurately transform synaptic input into spike sequences, supporting a possible role for spike timing in the processing of cortical information by the neocortex.},
  language = {eng},
  number = {5216},
  journal = {Science (New York, N.Y.)},
  author = {Mainen, Z. F. and Sejnowski, T. J.},
  month = jun,
  year = {1995},
  keywords = {Animals,Electric Stimulation,Evoked Potentials,In Vitro Techniques,Neurons,Occipital Lobe,Rats,Rats; Sprague-Dawley,Synaptic Transmission},
  pages = {1503-1506},
  file = {articles/Mainen1995.pdf},
  pmid = {7770778}
}

@book{Eilenberg1952,
  title = {Foundations of Algebraic Topology},
  language = {en},
  publisher = {{Princeton University Press}},
  author = {Eilenberg, Samuel and Steenrod, Norman Earl},
  year = {1952},
  keywords = {Mathematics / Topology},
  file = {books/Eilenberg1952_Foundations-of-algebraic-topology.pdf}
}

@article{Ganguly2013,
  title = {Activity-{{Dependent Neural Plasticity}} from {{Bench}} to {{Bedside}}},
  volume = {80},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2013.10.028},
  abstract = {Much progress has been made in understanding how behavioral experience and neural activity can modify the structure and function of neural circuits during development and in the adult brain. Studies of physiological and molecular mechanisms underlying activity-dependent plasticity in animal models have suggested potential therapeutic approaches for a wide range of brain disorders in humans. Physiological and electrical stimulations as well as plasticity-modifying molecular agents may facilitate functional recovery by selectively enhancing existing neural circuits or promoting the formation of new functional circuits. Here, we review the advances in basic studies of neural plasticity mechanisms in developing and adult nervous systems and current clinical treatments that harness neural plasticity, and we offer perspectives on future development of plasticity-based therapy.},
  language = {English},
  number = {3},
  journal = {Neuron},
  author = {Ganguly, Karunesh and Poo, Mu-ming},
  month = oct,
  year = {2013},
  pages = {729-741},
  file = {articles/Ganguly2013.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/J637Q98M/S0896-6273(13)00932-X.html},
  pmid = {24183023}
}

@article{Shepherd2005,
  title = {Geometric and Functional Organization of Cortical Circuits},
  volume = {8},
  copyright = {\textcopyright{} 2005 Nature Publishing Group},
  issn = {1097-6256},
  doi = {10.1038/nn1447},
  abstract = {Can neuronal morphology predict functional synaptic circuits? In the rat barrel cortex, 'barrels' and 'septa' delineate an orderly matrix of cortical columns. Using quantitative laser scanning photostimulation we measured the strength of excitatory projections from layer 4 (L4) and L5A to L2/3 pyramidal cells in barrel- and septum-related columns. From morphological reconstructions of excitatory neurons we computed the geometric circuit predicted by axodendritic overlap. Within most individual projections, functional inputs were predicted by geometry and a single scale factor, the synaptic strength per potential synapse. This factor, however, varied between projections and, in one case, even within a projection, up to 20-fold. Relationships between geometric overlap and synaptic strength thus depend on the laminar and columnar locations of both the pre- and postsynaptic neurons, even for neurons of the same type. A large plasticity potential appears to be incorporated into these circuits, allowing for functional 'tuning' with fixed axonal and dendritic arbor geometry.},
  language = {en},
  number = {6},
  journal = {Nature Neuroscience},
  author = {Shepherd, Gordon M. G. and Stepanyants, Armen and Bureau, Ingrid and Chklovskii, Dmitri and Svoboda, Karel},
  month = jun,
  year = {2005},
  pages = {782-790},
  file = {articles/Shepherd2005.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/TDH5BJKH/nn1447.html}
}

@article{Dong2007,
  title = {Random Graph Theory Based Connectivity Analysis in Wireless Sensor Networks with {{Rayleigh}} Fading Channels},
  doi = {10.1109/APCC.2007.4433515},
  abstract = {Connectivity is an essential merit of wireless sensor networks. There has been great interest in exploring the minimum density of sensor nodes that is needed to achieve a connected wireless network. This becomes difficult when uncertain features increase, such as Rayleigh fading channels. In this paper, we describe a range-dependent model for sensor networks by using random graph theory, and study the connectivity problem with this model. We calculate the probability of an arbitrary node being isolated, and thus obtain the probability of the whole network being connected. By giving the required minimum density, our work can guide in designing of the wireless sensor networks with fading channels. Moreover, the numerical results shows that the fading effect would degrade the connectivity of the wireless sensor networks.},
  journal = {Asia-Pacific Conference on Communications, 2007. APCC 2007},
  author = {Dong, Jingbo and Chen, Qing and Niu, Zhisheng},
  year = {2007},
  keywords = {Degradation,Fading,Lattices,Mobile communication,Monitoring,Probability,Rayleigh channels,Rayleigh fading channels,Sensor phenomena and characterization,Shadow mapping,arbitrary node probability,graph theory,random graph theory,range-dependent model,sensor nodes,wireless sensor networks},
  pages = {123-126},
  file = {articles/Dong2007.pdf}
}

@article{Muller2015,
  title = {Python in {{Neuroscience}}},
  volume = {9},
  issn = {1662-5196},
  doi = {10.3389/fninf.2015.00011},
  journal = {Frontiers in Neuroinformatics},
  author = {Muller, Eilif and Bednar, James A and Diesmann, Markus and Gewaltig, Marc-Oliver and Hines, Michael and Davison, Andrew P},
  year = {2015},
  file = {articles/Muller2015.EPUB;articles/Muller2015.PDF}
}

@unpublished{Pfaffelhuber2014,
  title = {Stochastische {{Prozesse}}},
  author = {Pfaffelhuber, Peter},
  year = {2014},
  file = {manuscripts/Pfaffelhuber2014_Stochastische-Prozesse.pdf}
}

@unpublished{Hoffmann2009c,
  title = {Funktionentheorie},
  author = {Hoffmann, Felix},
  year = {2009},
  file = {manuscripts/Hoffmann2009_Funktionentheorie.pdf}
}

@article{Pallotto2015,
  title = {Extracellular Space Preservation Aids the Connectomic Analysis of Neural Circuits},
  copyright = {\textcopyright{}  . This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the Creative Commons CC0 public domain dedication.},
  issn = {2050-084X},
  doi = {10.7554/eLife.08206},
  abstract = {Dense connectomic mapping of neuronal circuits is limited by the time and effort required to analyze 3D electron microscopy (EM) datasets. Algorithms designed to automate image segmentation suffer from substantial error rates and require significant manual error correction. Any improvement in segmentation error rates would therefore directly reduce the time required to analyze 3D EM data. We explored preserving extracellular space (ECS) during chemical tissue fixation to improve the ability to segment neurites and to identify synaptic contacts. ECS preserved tissue is easier to segment using machine learning algorithms, leading to significantly reduced error rates. In addition, we observed that electrical synapses are readily identified in ECS preserved tissue. Finally, we determined that antibodies penetrate deep into ECS preserved tissue with only minimal permeabilization, thereby enabling correlated light microscopy (LM) and EM studies. We conclude that preservation of ECS benefits multiple aspects of the connectomic analysis of neural circuits.To Top
Dense connectomic mapping of neuronal circuits is limited by the time and effort required to analyze 3D electron microscopy (EM) datasets. Algorithms designed to automate image segmentation suffer from substantial error rates and require significant manual error correction. Any improvement in segmentation error rates would therefore directly reduce the time required to analyze 3D EM data. We explored preserving extracellular space (ECS) during chemical tissue fixation to improve the ability to segment neurites and to identify synaptic contacts. ECS preserved tissue is easier to segment using machine learning algorithms, leading to significantly reduced error rates. In addition, we observed that electrical synapses are readily identified in ECS preserved tissue. Finally, we determined that antibodies penetrate deep into ECS preserved tissue with only minimal permeabilization, thereby enabling correlated light microscopy (LM) and EM studies. We conclude that preservation of ECS benefits multiple aspects of the connectomic analysis of neural circuits.},
  language = {en},
  journal = {eLife},
  author = {Pallotto, Marta and Watkins, Paul V. and Fubara, Boma and Singer, Joshua H. and Briggman, Kevin L.},
  month = dec,
  year = {2015},
  pages = {e08206},
  file = {articles/Pallotto2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/DB6X59MS/eLife.html}
}

@article{Fox2005,
  title = {A {{Comparison}} of {{Experience}}-{{Dependent Plasticity}} in the {{Visual}} and {{Somatosensory Systems}}},
  volume = {48},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2005.10.013},
  abstract = {Summary
In the visual and somatosensory systems, maturation of neuronal circuits continues for days to weeks after sensory stimulation occurs. Deprivation of sensory input at various stages of development can induce physiological, and often structural, changes that modify the circuitry of these sensory systems. Recent studies also reveal a surprising degree of plasticity in the mature visual and somatosensory pathways. Here, we compare and contrast the effects of sensory experience on the connectivity and function of these pathways and discuss what is known to date concerning the structural, physiological, and molecular mechanisms underlying their plasticity.},
  number = {3},
  journal = {Neuron},
  author = {Fox, Kevin and Wong, Rachel O. L.},
  month = nov,
  year = {2005},
  pages = {465-477},
  file = {articles/Fox2005.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/69R9XRVF/S0896627305008779.html}
}

@article{Hiratani2016,
  title = {Detailed Dendritic Excitatory/Inhibitory Balance through Heterosynaptic Spike-Timing-Dependent Plasticity},
  copyright = {\textcopyright{} 2016, Published by Cold Spring Harbor Laboratory Press. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  doi = {10.1101/056093},
  abstract = {Balance between excitatory and inhibitory inputs is a key feature of cortical dynamics. Such balance is arguably preserved in dendritic branches, yet its underlying mechanism and functional roles are still unknown. Here, by considering computational models of heterosynaptic spike-timing-dependent plasticity (STDP), we show that the detailed excitatory/inhibitory balance on dendritic branch is robustly achieved through heterosynaptic interaction between excitatory and inhibitory synapses. The acquired dendritic balance enables neuron to perform change detection, due to functional specialization at each branch. Furthermore, heterosynaptic STDP explains how maturation of inhibitory neurons modulates selectivity of excitatory neurons in critical period plasticity of binocular matching. Our results propose heterosynaptic STDP as a critical factor in synaptic organization and resultant dendritic computation.},
  language = {en},
  journal = {bioRxiv},
  author = {Hiratani, Naoki and Fukai, Tomoki},
  month = may,
  year = {2016},
  pages = {056093},
  file = {articles/Hiratani2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/4UZ3ARVE/056093.html}
}

@book{Raschka2015,
  address = {Birmingham},
  series = {Community experience distilled},
  title = {Python Machine Learning},
  isbn = {978-1-78355-513-0},
  shorttitle = {Python Machine Learning},
  language = {eng},
  publisher = {{Packt Publishing}},
  author = {Raschka, Sebastian},
  year = {2015},
  keywords = {Maschinelles Lernen,Python},
  file = {books/Raschka2015_Python-machine-learning.pdf},
  note = {OCLC: 927915246}
}

@article{Fauth2015a,
  title = {The {{Formation}} of {{Multi}}-Synaptic {{Connections}} by the {{Interaction}} of {{Synaptic}} and {{Structural Plasticity}} and {{Their Functional Consequences}}},
  volume = {11},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004031},
  abstract = {Author Summary   The connectivity between neurons is modified by different mechanisms. On a time scale of minutes to hours one finds synaptic plasticity, whereas mechanisms for structural changes at axons or dendrites may take days. One main factor determining structural changes is the weight of a connection, which, in turn, is adapted by synaptic plasticity. Both mechanisms, synaptic and structural plasticity, are influenced and determined by the activity pattern in the network. Hence, it is important to understand how activity and the different plasticity mechanisms influence each other. Especially how activity influences rewiring in adult networks is still an open question.   We present a model, which captures these complex interactions by abstracting structural plasticity with weight-dependent probabilities. This allows for calculating the distribution of the number of synapses between two neurons analytically. We report that biologically realistic connection patterns for different cortical layers generically arise with synaptic plasticity rules in which the synaptic weights grow with postsynaptic activity. The connectivity patterns also lead to different activity levels resembling those found in the different cortical layers. Interestingly such a system exhibits a hysteresis by which connections remain stable longer than expected, which may add to the stability of information storage in the network.},
  number = {1},
  journal = {PLOS Comput Biol},
  author = {Fauth, Michael and W\"org\"otter, Florentin and Tetzlaff, Christian},
  month = jan,
  year = {2015},
  keywords = {Dynamical systems,Neuronal dendrites,Neuronal plasticity,Neurons,Probability distribution,Synapses,neural networks,synaptic plasticity},
  pages = {e1004031},
  file = {articles/Fauth2015.pdf}
}

@article{Gilson2009c,
  title = {Emergence of Network Structure Due to Spike-Timing-Dependent Plasticity in Recurrent Neuronal Networks. {{I}}. {{Input}} Selectivity\textendash{}Strengthening Correlated Input Pathways},
  volume = {101},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-009-0319-4},
  abstract = {Spike-timing-dependent plasticity (STDP) determines the evolution of the synaptic weights according to their pre- and post-synaptic activity, which in turn changes the neuronal activity. In this paper, we extend previous studies of input selectivity induced by (STDP) for single neurons to the biologically interesting case of a neuronal network with fixed recurrent connections and plastic connections from external pools of input neurons. We use a theoretical framework based on the Poisson neuron model to analytically describe the network dynamics (firing rates and spike-time correlations) and thus the evolution of the synaptic weights. This framework incorporates the time course of the post-synaptic potentials and synaptic delays. Our analysis focuses on the asymptotic states of a network stimulated by two homogeneous pools of ``steady'' inputs, namely Poisson spike trains which have fixed firing rates and spike-time correlations. The (STDP) model extends rate-based learning in that it can implement, at the same time, both a stabilization of the individual neuron firing rates and a slower weight specialization depending on the input spike-time correlations. When one input pathway has stronger within-pool correlations, the resulting synaptic dynamics induced by (STDP) are shown to be similar to those arising in the case of a purely feed-forward network: the weights from the more correlated inputs are potentiated at the expense of the remaining input connections.},
  language = {en},
  number = {2},
  journal = {Biological Cybernetics},
  author = {Gilson, Matthieu and Burkitt, Anthony N. and Grayden, David B. and Thomas, Doreen A. and van Hemmen, J. Leo},
  month = jun,
  year = {2009},
  keywords = {Bioinformatics,Complexity,Computer Appl. in Life Sciences,Neurobiology,Neurosciences,Recurrent neuronal network,STDP,Spike-time correlation,learning},
  pages = {81-102},
  file = {articles/Gilson2009.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/IKQAMS8M/10.html}
}

@article{Vlachos2012,
  title = {Repetitive {{Magnetic Stimulation Induces Functional}} and {{Structural Plasticity}} of {{Excitatory Postsynapses}} in {{Mouse Organotypic Hippocampal Slice Cultures}}},
  volume = {32},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0409-12.2012},
  language = {en},
  number = {48},
  journal = {Journal of Neuroscience},
  author = {Vlachos, A. and {Muller-Dahlhaus}, F. and Rosskopp, J. and Lenz, M. and Ziemann, U. and Deller, T.},
  month = nov,
  year = {2012},
  pages = {17514-17523},
  file = {articles/Vlachos2012.pdf}
}

@article{Nestler2010,
  title = {Animal Models of Neuropsychiatric Disorders},
  volume = {13},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.2647},
  number = {10},
  journal = {Nature Neuroscience},
  author = {Nestler, Eric J and Hyman, Steven E},
  month = oct,
  year = {2010},
  pages = {1161-1169},
  file = {articles/Nestler2010.pdf}
}

@article{Ocker2015,
  title = {Self-{{Organization}} of {{Microcircuits}} in {{Networks}} of {{Spiking Neurons}} with {{Plastic Synapses}}},
  volume = {11},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004458},
  abstract = {Author Summary   The connectivity of mammalian brains exhibits structure at a wide variety of spatial scales, from the broad (which brain areas connect to which) to the extremely fine (where synapses form on the morphology of individual neurons). Recent experimental work in the neocortex has highlighted structure at the level of microcircuits: different patterns of connectivity between small groups of neurons are either more or less abundant than would be expected by chance. A central question in systems neuroscience is how this structure emerges. Attempts to answer this question are confounded by the mutual interaction of network structure and spiking activity. Synaptic connections influence spiking statistics, while individual synapses are highly plastic and become stronger or weaker depending on the activity of the pre- and postsynaptic neurons. We present a self-consistent theory for how activity-dependent synaptic plasticity leads to the emergence of neuronal microcircuits. We use this theory to show how the form of the plasticity rule can govern the promotion or suppression of different connectivity patterns. Our work provides a foundation for understanding how cortical circuits, and not just individual synapses, are malleable in response to inputs both external and internal to a network.},
  number = {8},
  journal = {PLOS Comput Biol},
  author = {Ocker, Gabriel Koch and {Litwin-Kumar}, Ashok and Doiron, Brent},
  month = aug,
  year = {2015},
  keywords = {Action potentials,Covariance,Network motifs,Neuronal plasticity,Neurons,Synapses,neural networks,synaptic plasticity},
  pages = {e1004458},
  file = {articles/Ocker2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/PQTSXKI5/article.html}
}

@article{Fino2011a,
  title = {Dense {{Inhibitory Connectivity}} in {{Neocortex}}},
  volume = {69},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2011.02.025},
  abstract = {Summary
The connectivity diagram of neocortical circuits is still unknown, and there are conflicting data as to whether cortical neurons are wired specifically or not. To investigate the basic structure of cortical microcircuits, we use a two-photon photostimulation technique that enables the systematic mapping of synaptic connections with single-cell resolution. We map the inhibitory connectivity between upper layers somatostatin-positive GABAergic interneurons and pyramidal cells in mouse frontal cortex. Most, and sometimes all, inhibitory neurons are locally connected to every sampled pyramidal cell. This dense inhibitory connectivity is found at both young and mature developmental ages. Inhibitory innervation of neighboring pyramidal cells is similar, regardless of whether they are connected among themselves or not. We conclude that local inhibitory connectivity is promiscuous, does not form subnetworks, and can approach the theoretical limit of a completely connected synaptic matrix.},
  number = {6},
  journal = {Neuron},
  author = {Fino, Elodie and Yuste, Rafael},
  month = mar,
  year = {2011},
  pages = {1188-1203},
  file = {articles/Fino2011.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/4EXKPEAU/S0896627311001231.html}
}

@article{Goedeke2008,
  title = {The Mechanism of Synchronization in Feed-Forward Neuronal Networks},
  volume = {10},
  issn = {1367-2630},
  doi = {10.1088/1367-2630/10/1/015007},
  abstract = {Synchronization in feed-forward subnetworks of the brain has been proposed to explain the precisely timed spike patterns observed in experiments. While the attractor dynamics of these networks is now well understood, the underlying single neuron mechanisms remain unexplained. Previous attempts have captured the effects of the highly fluctuating membrane potential by relating spike intensity f ( U ) to the instantaneous voltage U generated by the input. This article shows that f is high during the rise and low during the decay of U ( t ), demonstrating that the \#\#IMG\#\# [http://ej.iop.org/images/1367-2630/10/1/015007/nj256198ieqn1.gif] $\backslash$dotU -dependence of f , not refractoriness, is essential for synchronization. Moreover, the bifurcation scenario is quantitatively described by a simple \#\#IMG\#\# [http://ej.iop.org/images/1367-2630/10/1/015007/nj256198ieqn2.gif] f(U,$\backslash$dotU) relationship. These findings suggest \#\#IMG\#\# [http://ej.iop.org/images/1367-2630/10/1/015007/nj256198ieqn3.gif] f(U,$\backslash$dotU) as the relevant model class for the investigation of neural synchronization phenomena in a noisy environment.},
  language = {en},
  number = {1},
  journal = {New Journal of Physics},
  author = {Goedeke, S. and Diesmann, M.},
  year = {2008},
  pages = {015007},
  file = {articles/Goedeke2008.pdf}
}

@book{Snustad2012,
  address = {Hoboken, NJ},
  edition = {6th ed},
  title = {Principles of Genetics},
  isbn = {978-0-470-90359-9},
  lccn = {QH430 .S68 2012},
  publisher = {{Wiley}},
  author = {Snustad, D. Peter and Simmons, Michael J.},
  year = {2012},
  keywords = {Genetics},
  file = {books/Snustad2012_Principles-of-genetics.pdf}
}

@article{Izhikevich2006,
  title = {Polychronization: {{Computation}} with {{Spikes}}},
  volume = {18},
  issn = {0899-7667, 1530-888X},
  shorttitle = {Polychronization},
  doi = {10.1162/089976606775093882},
  language = {en},
  number = {2},
  journal = {Neural Computation},
  author = {Izhikevich, Eugene M.},
  month = feb,
  year = {2006},
  pages = {245-282},
  file = {articles/Izhikevich2006.pdf}
}

@book{Plato2010,
  address = {Wiesbaden},
  edition = {4. aktualisierte Aufl},
  series = {Numerische Mathematik},
  title = {{Numerische Mathematik kompakt: Grundlagenwissen f\"ur Studium und Praxis ; [mit Online-Service]}},
  isbn = {978-3-8348-1018-2},
  shorttitle = {{Numerische Mathematik kompakt}},
  language = {ger},
  publisher = {{Vieweg + Teubner}},
  author = {Plato, Robert},
  year = {2010},
  keywords = {Lehrbuch,Numerical analysis,Numerische Mathematik},
  file = {articles/Book/Plato20102.pdf;books/Plato2010_Numerische-Mathematik-kompakt-Grundlagenwissen-für-Studium-und-Praxis-\;-[mit-Online-Service].pdf}
}

@article{Stevenson2008,
  title = {Inferring Functional Connections between Neurons},
  volume = {18},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2008.11.005},
  abstract = {A central question in neuroscience is how interactions between neurons give rise to behavior. In many electrophysiological experiments, the activity of a set of neurons is recorded while sensory stimuli or movement tasks are varied. Tools that aim to reveal underlying interactions between neurons from such data can be extremely useful. Traditionally, neuroscientists have studied these interactions using purely descriptive statistics (cross-correlograms or joint peri-stimulus time histograms). However, the interpretation of such data is often difficult, particularly as the number of recorded neurons grows. Recent research suggests that model-based, maximum likelihood methods can improve these analyses. In addition to estimating neural interactions, application of these techniques has improved decoding of external variables, created novel interpretations of existing electrophysiological data, and may provide new insight into how the brain represents information.},
  number = {6},
  journal = {Current opinion in neurobiology},
  author = {Stevenson, Ian H. and Rebesco, James M. and Miller, Lee E. and K\"ording, Konrad P.},
  month = dec,
  year = {2008},
  pages = {582-588},
  file = {articles/Stevenson2008.pdf},
  pmid = {19081241},
  pmcid = {PMC2706692}
}

@article{Boettiger2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1410.0846},
  title = {An Introduction to {{Docker}} for Reproducible Research, with Examples from the {{R}} Environment},
  volume = {49},
  issn = {01635980},
  doi = {10.1145/2723872.2723882},
  abstract = {As computational work becomes more and more integral to many aspects of scientific research, computational reproducibility has become an issue of increasing importance to computer systems researchers and domain scientists alike. Though computational reproducibility seems more straight forward than replicating physical experiments, the complex and rapidly changing nature of computer environments makes being able to reproduce and extend such work a serious challenge. In this paper, I explore common reasons that code developed for one research project cannot be successfully executed or extended by subsequent researchers. I review current approaches to these issues, including virtual machines and workflow systems, and their limitations. I then examine how the popular emerging technology Docker combines several areas from systems research - such as operating system virtualization, cross-platform portability, modular re-usable elements, versioning, and a ‘DevOps' philosophy, to address these challenges. I illustrate this with several examples of Docker use with a focus on the R statistical environment.},
  number = {1},
  journal = {ACM SIGOPS Operating Systems Review},
  author = {Boettiger, Carl},
  month = jan,
  year = {2015},
  keywords = {Computer Science - Software Engineering},
  pages = {71-79},
  file = {articles/Boettiger2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/9EXWD4S3/1410.html}
}

@techreport{zotero-317,
  title = {{{TikZ}} - {{Minimal Introduction}}},
  file = {manuals/latex/tikz_-_minimal_introduction.pdf},
  note = {manuals/latex}
}

@article{Brunel1999,
  title = {Fast {{Global Oscillations}} in {{Networks}} of {{Integrate}}-and-{{Fire Neurons}} with {{Low Firing Rates}}},
  volume = {11},
  issn = {0899-7667},
  doi = {10.1162/089976699300016179},
  abstract = {We study analytically the dynamics of a network of sparsely connected inhibitory integrate-and-fire neurons in a regime where individual neurons emit spikes irregularly and at a low rate. In the limit when the number of neurons N $\rightarrow$ $\infty$, the network exhibits a sharp transition between a stationary and an oscillatory global activity regime where neurons are weakly synchronized. The activity becomes oscillatory when the inhibitory feedback is strong enough. The period of the global oscillation is found to be mainly controlled by synaptic times but depends also on the characteristics of the external input. In large but finite networks, the analysis shows that global oscillations of finite coherence time generically exist both above and below the critical inhibition threshold. Their characteristics are determined as functions of systems parameters in these two different regimes. The results are found to be in good agreement with numerical simulations.},
  number = {7},
  journal = {Neural Computation},
  author = {Brunel, N. and Hakim, V.},
  month = oct,
  year = {1999},
  pages = {1621-1671},
  file = {articles/Brunel1999.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/D954Z3ZV/login.html}
}

@book{Jaynes2003,
  address = {Cambridge, UK ; New York, NY},
  title = {Probability {{Theory}}: {{The Logic}} of {{Science}}},
  isbn = {978-0-521-59271-0},
  shorttitle = {Probability {{Theory}}},
  abstract = {Going beyond the conventional mathematics of probability theory, this study views the subject in a wider context. It discusses new results, along with applications of probability theory to a variety of problems. The book contains many exercises and is suitable for use as a textbook on graduate-level courses involving data analysis. Aimed at readers already familiar with applied mathematics at an advanced undergraduate level or higher, it is of interest to scientists concerned with inference from incomplete information.},
  language = {English},
  publisher = {{Cambridge University Press}},
  author = {Jaynes, E. T.},
  editor = {Bretthorst, G. Larry},
  month = jun,
  year = {2003},
  file = {books/Jaynes2003_Probability-Theory-The-Logic-of-Science.pdf}
}

@article{Duarte2014,
  title = {Dynamic Stability of Sequential Stimulus Representations in Adapting Neuronal Networks},
  volume = {8},
  issn = {1662-5188},
  doi = {10.3389/fncom.2014.00124},
  journal = {Frontiers in Computational Neuroscience},
  author = {Duarte, Renato C. F. and Morrison, Abigail},
  month = oct,
  year = {2014},
  file = {articles/Duarte2014.pdf}
}

@book{Werner2011,
  address = {Berlin},
  edition = {7., korrigierte und erw. Aufl},
  series = {Springer-Lehrbuch},
  title = {{Funktionalanalysis}},
  isbn = {978-3-642-21016-7 978-3-642-21017-4},
  language = {ger},
  publisher = {{Springer}},
  author = {Werner, Dirk},
  year = {2011},
  keywords = {Funktionalanalysis,Global analysis,Lehrbuch,Mathematics},
  file = {books/Werner2011_Funktionalanalysis.pdf}
}

@article{Shannon1948,
  title = {A Mathematical Theory of Communication},
  volume = {27},
  issn = {0005-8580},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
  number = {3},
  journal = {Bell System Technical Journal, The},
  author = {Shannon, C.E.},
  month = jul,
  year = {1948},
  pages = {379-423},
  file = {articles/Shannon1948.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/T34Q7RJT/articleDetails.html}
}

@article{Krone1986,
  title = {Spatiotemporal {{Receptive Fields}}: {{A Dynamical Model Derived}} from {{Cortical Architectonics}}},
  volume = {226},
  issn = {0080-4649},
  shorttitle = {Spatiotemporal {{Receptive Fields}}},
  doi = {10.1098/rspb.1986.0002},
  abstract = {We assume that the mammalian neocortex is built up out of some six layers which differ in their morphology and their external connections. Intrinsic connectivity is largely excitatory, leading to a considerable amount of positive feedback. The majority of cortical neurons can be divided into two main classes: the pyramidal cells, which are said to be excitatory, and local cells (most notably the non-spiny stellate cells), which are said to be inhibitory. The form of the dendritic and axonal arborizations of both groups is discussed in detail. This results in a simplified model of the cortex as a stack of six layers with mutual connections determined by the principles of fibre anatomy. This stack can be treated as a multi-input-multi-output system by means of the linear systems theory of homogeneous layers. The detailed equations for the simulation are derived in the Appendix. The results of the simulations show that the temporal and spatial behaviour of an excitation distribution cannot be treated separately. Further, they indicate specific processing in the different layers and some independence from details of wiring. Finally, the simulation results are applied to the theory of visual receptive fields. This yields some insight into the mechanisms possibly underlying hypercomplexity, putative nonlinearities, lateral inhibition, oscillating cell responses, and velocity-dependent tuning curves.},
  language = {en},
  number = {1245},
  journal = {Proceedings of the Royal Society of London B: Biological Sciences},
  author = {Krone, G. and Mallot, H. and Palm, G. and Schuz, A.},
  month = jan,
  year = {1986},
  pages = {421-444},
  file = {articles/Krone1986.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/SZWFJZB6/421.html},
  pmid = {2869496}
}

@techreport{zotero-324,
  title = {{{TikZ}}-{{Graph}} ({{French}})},
  file = {manuals/latex/tikz-graph_(french).pdf},
  note = {manuals/latex}
}

@techreport{zotero-325,
  title = {Auctex},
  file = {manuals/emacs/auctex.pdf},
  note = {manuals/emacs}
}

@article{Ozeki2009,
  title = {Inhibitory {{Stabilization}} of the {{Cortical Network Underlies Visual Surround Suppression}}},
  volume = {62},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.03.028},
  language = {en},
  number = {4},
  journal = {Neuron},
  author = {Ozeki, Hirofumi and Finn, Ian M. and Schaffer, Evan S. and Miller, Kenneth D. and Ferster, David},
  month = may,
  year = {2009},
  pages = {578-592},
  file = {articles/Ozeki2009.pdf}
}

@article{Ostojic2014,
  title = {Two Types of Asynchronous Activity in Networks of Excitatory and Inhibitory Spiking Neurons},
  volume = {17},
  copyright = {\textcopyright{} 2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  doi = {10.1038/nn.3658},
  abstract = {Asynchronous activity in balanced networks of excitatory and inhibitory neurons is believed to constitute the primary medium for the propagation and transformation of information in the neocortex. Here we show that an unstructured, sparsely connected network of model spiking neurons can display two fundamentally different types of asynchronous activity that imply vastly different computational properties. For weak synaptic couplings, the network at rest is in the well-studied asynchronous state, in which individual neurons fire irregularly at constant rates. In this state, an external input leads to a highly redundant response of different neurons that favors information transmission but hinders more complex computations. For strong couplings, we find that the network at rest displays rich internal dynamics, in which the firing rates of individual neurons fluctuate strongly in time and across neurons. In this regime, the internal dynamics interact with incoming stimuli to provide a substrate for complex information processing and learning.},
  language = {en},
  number = {4},
  journal = {Nature Neuroscience},
  author = {Ostojic, Srdjan},
  month = apr,
  year = {2014},
  keywords = {Network models},
  pages = {594-600},
  file = {articles/Ostojic2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/6P6TSIAX/nn.3658.html}
}

@article{Gulledge2012,
  title = {Electrical {{Advantages}} of {{Dendritic Spines}}},
  volume = {7},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0036007},
  language = {en},
  number = {4},
  journal = {PLoS ONE},
  author = {Gulledge, Allan T. and Carnevale, Nicholas T. and Stuart, Greg J.},
  editor = {Mansvelder, Huibert D.},
  month = apr,
  year = {2012},
  pages = {e36007},
  file = {articles/Gulledge2012.pdf}
}

@book{Braitenberg1998,
  address = {Berlin; New York},
  edition = {2nd edition},
  title = {Cortex: Statistics and Geometry of Neuronal Connectivity},
  isbn = {3-540-63816-4 978-3-540-63816-2},
  shorttitle = {Cortex},
  language = {English},
  publisher = {{Springer}},
  author = {Braitenberg, Valentino and Sch\"uz, A and Braitenberg, Valentino},
  year = {1998},
  file = {books/Braitenberg1998_Cortex-statistics-and-geometry-of-neuronal-connectivity.pdf}
}

@article{Kleberg2014,
  title = {Excitatory and Inhibitory {{STDP}} Jointly Tune Feedforward Neural Circuits to Selectively Propagate Correlated Spiking Activity},
  volume = {8},
  doi = {10.3389/fncom.2014.00053},
  abstract = {Spike-timing-dependent plasticity (STDP) has been well established between excitatory neurons and several computational functions have been proposed in various neural systems. Despite some recent efforts, however, there is a significant lack of functional understanding of inhibitory STDP (iSTDP) and its interplay with excitatory STDP (eSTDP). Here, we demonstrate by analytical and numerical methods that iSTDP contributes crucially to the balance of excitatory and inhibitory weights for the selection of a specific signaling pathway among other pathways in a feedforward circuit. This pathway selection is based on the high sensitivity of STDP to correlations in spike times, which complements a recent proposal for the role of iSTDP in firing-rate based selection. Our model predicts that asymmetric anti-Hebbian iSTDP exceeds asymmetric Hebbian iSTDP for supporting pathway-specific balance, which we show is useful for propagating transient neuronal responses. Furthermore, we demonstrate how STDPs at excitatory\textendash{}excitatory, excitatory\textendash{}inhibitory, and inhibitory\textendash{}excitatory synapses cooperate to improve the pathway selection. We propose that iSTDP is crucial for shaping the network structure that achieves efficient processing of synchronous spikes.},
  journal = {Frontiers in Computational Neuroscience},
  author = {Kleberg, Florence I. and Fukai, Tomoki and Gilson, Matthieu},
  year = {2014},
  keywords = {Correlation,STDP,disynaptic,excitation–inhibition balance,inhibition,plasticity,spike-timing},
  pages = {53},
  file = {articles/Kleberg2014.pdf}
}

@article{Lenz2015,
  title = {Repetitive Magnetic Stimulation Induces Plasticity of Excitatory Postsynapses on Proximal Dendrites of Cultured Mouse {{CA1}} Pyramidal Neurons},
  volume = {220},
  issn = {1863-2653, 1863-2661},
  doi = {10.1007/s00429-014-0859-9},
  language = {en},
  number = {6},
  journal = {Brain Structure and Function},
  author = {Lenz, Maximilian and Platschek, Steffen and Priesemann, Viola and Becker, Denise and Willems, Laurent M. and Ziemann, Ulf and Deller, Thomas and {M\"uller-Dahlhaus}, Florian and Jedlicka, Peter and Vlachos, Andreas},
  month = nov,
  year = {2015},
  pages = {3323-3337},
  file = {articles/Lenz2015.pdf}
}

@book{Hofstadter1999,
  address = {New York},
  edition = {20 Anv edition},
  title = {G\"odel, {{Escher}}, {{Bach}}: {{An Eternal Golden Braid}}},
  isbn = {978-0-465-02656-2},
  shorttitle = {G\"odel, {{Escher}}, {{Bach}}},
  abstract = {Douglas Hofstadter's book is concerned directly with the nature of ``maps'' or links between formal systems. However, according to Hofstadter, the formal system that underlies all mental activity transcends the system that supports it. If life can grow out of the formal chemical substrate of the cell, if consciousness can emerge out of a formal system of firing neurons, then so too will computers attain human intelligence. G\"odel Escher and Bach is a wonderful exploration of fascinating ideas at the heart of cognitive science: meaning, reduction, recursion, and much more.},
  language = {English},
  publisher = {{Basic Books}},
  author = {Hofstadter, Douglas R.},
  month = feb,
  year = {1999},
  file = {books/Hofstadter1999_Gödel,-Escher,-Bach-An-Eternal-Golden-Braid.epub}
}

@article{Hopfield1982,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities.},
  volume = {79},
  issn = {0027-8424},
  abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
  number = {8},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  author = {Hopfield, J J},
  month = apr,
  year = {1982},
  pages = {2554-2558},
  file = {articles/Hopfield1982.pdf},
  pmid = {6953413},
  pmcid = {PMC346238}
}

@article{Hayashi-Takagi2015,
  title = {Labelling and Optical Erasure of Synaptic Memory Traces in the Motor Cortex},
  volume = {525},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature15257},
  number = {7569},
  journal = {Nature},
  author = {{Hayashi-Takagi}, Akiko and Yagishita, Sho and Nakamura, Mayumi and Shirai, Fukutoshi and Wu, Yi I. and Loshbaugh, Amanda L. and Kuhlman, Brian and Hahn, Klaus M. and Kasai, Haruo},
  month = sep,
  year = {2015},
  pages = {333-338},
  file = {articles/Hayashi-Takagi2015.pdf}
}

@article{Hines2009,
  title = {{{NEURON}} and {{Python}}},
  volume = {3},
  issn = {1662-5196},
  doi = {10.3389/neuro.11.001.2009},
  abstract = {The NEURON simulation program now allows Python to be used, alone or in combination with NEURON's traditional Hoc interpreter. Adding Python to NEURON has the immediate benefit of making available a very extensive suite of analysis tools written for engineering and science. It also catalyzes NEURON software development by offering users a modern programming tool that is recognized for its flexibility and power to create and maintain complex programs. At the same time, nothing is lost because all existing models written in Hoc, including graphical user interface tools, continue to work without change and are also available within the Python context. An example of the benefits of Python availability is the use of the xml module in implementing NEURON's Import3D and CellBuild tools to read MorphML and NeuroML model specifications.},
  journal = {Frontiers in Neuroinformatics},
  author = {Hines, Michael L. and Davison, Andrew P. and Muller, Eilif},
  month = jan,
  year = {2009},
  file = {articles/Hines2009.pdf},
  pmid = {19198661},
  pmcid = {PMC2636686}
}

@article{Diesmann1999,
  title = {Stable Propagation of Synchronous Spiking in Cortical Neural Networks},
  volume = {402},
  copyright = {\textcopyright{} 1999 Nature Publishing Group},
  issn = {0028-0836},
  doi = {10.1038/990101},
  abstract = {The classical view of neural coding has emphasized the importance of information carried by the rate at which neurons discharge action potentials. More recent proposals that information may be carried by precise spike timing have been challenged by the assumption that these neurons operate in a noisy fashion\textemdash{}presumably reflecting fluctuations in synaptic input\textemdash{}and, thus, incapable of transmitting signals with millisecond fidelity. Here we show that precisely synchronized action potentials can propagate within a model of cortical network activity that recapitulates many of the features of biological systems. An attractor, yielding a stable spiking precision in the (sub)millisecond range, governs the dynamics of synchronization. Our results indicate that a combinatorial neural code, based on rapid associations of groups of neurons co-ordinating their activity at the single spike level, is possible within a cortical-like network.},
  language = {en},
  number = {6761},
  journal = {Nature},
  author = {Diesmann, Markus and Gewaltig, Marc-Oliver and Aertsen, Ad},
  month = dec,
  year = {1999},
  pages = {529-533},
  file = {articles/Diesmann1999.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/CEHEA4KE/402529a0.html}
}

@article{Uhlenbeck1930,
  title = {On the {{Theory}} of the {{Brownian Motion}}},
  volume = {36},
  doi = {10.1103/PhysRev.36.823},
  abstract = {With a method first indicated by Ornstein the mean values of all the powers of the velocity u and the displacement s of a free particle in Brownian motion are calculated. It is shown that u-u0exp(-$\beta$t) and s-u0$\beta$[1-exp(-$\beta$t)] where u0 is the initial velocity and $\beta$ the friction coefficient divided by the mass of the particle, follow the normal Gaussian distribution law. For s this gives the exact frequency distribution corresponding to the exact formula for s2 of Ornstein and F\"urth. Discussion is given of the connection with the Fokker-Planck partial differential equation. By the same method exact expressions are obtained for the square of the deviation of a harmonically bound particle in Brownian motion as a function of the time and the initial deviation. Here the periodic, aperiodic and overdamped cases have to be treated separately. In the last case, when $\beta$ is much larger than the frequency and for values of t$\gg\beta$-1, the formula takes the form of that previously given by Smoluchowski.},
  number = {5},
  journal = {Physical Review},
  author = {Uhlenbeck, G. E. and Ornstein, L. S.},
  month = sep,
  year = {1930},
  pages = {823-841},
  file = {articles/Uhlenbeck1930.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/JTG2ZQDB/PhysRev.36.html}
}

@book{Izhikevich2010,
  address = {Cambridge, Mass.; London},
  title = {Dynamical {{Systems}} in {{Neuroscience}}: {{The Geometry}} of {{Excitability}} and {{Bursting}}},
  isbn = {978-0-262-51420-0},
  shorttitle = {Dynamical {{Systems}} in {{Neuroscience}}},
  abstract = {In order to model neuronal behavior or to interpret the results of                 modeling studies, neuroscientists must call upon methods of nonlinear dynamics. This                 book offers an introduction to nonlinear dynamical systems theory for researchers                 and graduate students in neuroscience. It also provides an overview of neuroscience                 for mathematicians who want to learn the basic facts of                 electrophysiology. Dynamical Systems in Neuroscience                 presents a systematic study of the relationship of electrophysiology,                 nonlinear dynamics, and computational properties of neurons. It emphasizes that                 information processing in the brain depends not only on the electrophysiological                 properties of neurons but also on their dynamical properties. The book introduces                 dynamical systems, starting with one- and two-dimensional Hodgkin-Huxley-type models                 and continuing to a description of bursting systems. Each chapter proceeds from the                 simple to the complex, and provides sample problems at the end. The book explains                 all necessary mathematical concepts using geometrical intuition; it includes many                 figures and few equations, making it especially suitable for non-mathematicians.                 Each concept is presented in terms of both neuroscience and mathematics, providing a                 link between the two disciplines.Nonlinear dynamical systems                 theory is at the core of computational neuroscience research, but it is not a                 standard part of the graduate neuroscience curriculum -- or taught by math or                 physics department in a way that is suitable for students of biology. This book                 offers neuroscience students and researchers a comprehensive account of concepts and                 methods increasingly used in computational neuroscience.An                 additional chapter on synchronization, with more advanced material, can be found at                 the author's website, www.izhikevich.com.},
  language = {English},
  publisher = {{The MIT Press}},
  author = {Izhikevich, Eugene M.},
  month = jan,
  year = {2010},
  file = {books/Izhikevich2010_Dynamical-Systems-in-Neuroscience-The-Geometry-of-Excitability-and-Bursting.pdf}
}

@book{Bower2013,
  address = {New York},
  edition = {2013 edition},
  title = {20 {{Years}} of {{Computational Neuroscience}}},
  isbn = {978-1-4614-1423-0},
  abstract = {When funding agencies and policy organizations consider the role of modeling and simulation in modern biology, the question is often posed, what has been accomplished ? This book will be organized around a symposium on the 20 year history of the CNS meetings, to be held as part of CNS 2010 in San Antonio Texas in July 2010.  The book, like the symposium is intended to summarize progress made in Computational Neuroscience over the last 20 years while also considering current challenges in the field.  As described in the table of contents, the chapter's authors have been selected to provide wide coverage of the applications of computational techniques to a broad range of questions and model systems in neuroscience.    The proposed book will include several features that establish the history of the field.   For each article, its author will select an article originally appearing in a CNS conference proceedings from 15 \textendash{} 20 years ago.  These short (less than 6 page) articles will provide illustrations of the state of the field 20 years ago.  The new articles will describe what has been learned about the subject in the following 20 years, and pose specific challenges for the next 20 years.   The second historical mechanism will be the reproduction of the first 12 years of posters from the CNS meeting.  These posters in and of themselves have become famous in the field (they hang in the halls of the NIH in Bethesda Maryland) and were constructed as allegories  for the state and development of computational neuroscience.  The posters were designed by the book's editor, who will, for the first time, provide a written description of each poster.},
  language = {English},
  publisher = {{Springer}},
  editor = {Bower, James M.},
  month = jul,
  year = {2013},
  file = {books/Bower2013_20-Years-of-Computational-Neuroscience.pdf}
}

@article{Steinberg2013,
  title = {A Causal Link between Prediction Errors, Dopamine Neurons and Learning},
  volume = {16},
  copyright = {\textcopyright{} 2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  doi = {10.1038/nn.3413},
  abstract = {Situations in which rewards are unexpectedly obtained or withheld represent opportunities for new learning. Often, this learning includes identifying cues that predict reward availability. Unexpected rewards strongly activate midbrain dopamine neurons. This phasic signal is proposed to support learning about antecedent cues by signaling discrepancies between actual and expected outcomes, termed a reward prediction error. However, it is unknown whether dopamine neuron prediction error signaling and cue-reward learning are causally linked. To test this hypothesis, we manipulated dopamine neuron activity in rats in two behavioral procedures, associative blocking and extinction, that illustrate the essential function of prediction errors in learning. We observed that optogenetic activation of dopamine neurons concurrent with reward delivery, mimicking a prediction error, was sufficient to cause long-lasting increases in cue-elicited reward-seeking behavior. Our findings establish a causal role for temporally precise dopamine neuron signaling in cue-reward learning, bridging a critical gap between experimental evidence and influential theoretical frameworks.},
  language = {en},
  number = {7},
  journal = {Nature Neuroscience},
  author = {Steinberg, Elizabeth E. and Keiflin, Ronald and Boivin, Josiah R. and Witten, Ilana B. and Deisseroth, Karl and Janak, Patricia H.},
  month = jul,
  year = {2013},
  keywords = {Classical conditioning},
  pages = {966-973},
  file = {articles/Steinberg2013.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/JTTIPGEF/nn.3413.html}
}

@article{Beggs2012,
  title = {Being {{Critical}} of {{Criticality}} in the {{Brain}}},
  volume = {3},
  issn = {1664-042X},
  doi = {10.3389/fphys.2012.00163},
  journal = {Frontiers in Physiology},
  author = {Beggs, John M. and Timme, Nicholas},
  year = {2012},
  file = {articles/Beggs2012.pdf}
}

@article{Renart2010,
  title = {The {{Asynchronous State}} in {{Cortical Circuits}}},
  volume = {327},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1179850},
  language = {en},
  number = {5965},
  journal = {Science},
  author = {Renart, A. and {de la Rocha}, J. and Bartho, P. and Hollender, L. and Parga, N. and Reyes, A. and Harris, K. D.},
  month = jan,
  year = {2010},
  keywords = {_tablet},
  pages = {587-590},
  file = {articles/Renart2010.pdf}
}

@article{vanElburg2010,
  title = {Impact of {{Dendritic Size}} and {{Dendritic Topology}} on {{Burst Firing}} in {{Pyramidal Cells}}},
  volume = {6},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000781},
  language = {en},
  number = {5},
  journal = {PLoS Computational Biology},
  author = {{van Elburg}, Ronald A. J. and {van Ooyen}, Arjen},
  editor = {Graham, Lyle J.},
  month = may,
  year = {2010},
  pages = {e1000781},
  file = {articles/van Elburg2010.pdf}
}

@book{Proakis1996,
  address = {Upper Saddle River, N.J},
  edition = {3rd ed},
  title = {Digital Signal Processing: Principles, Algorithms, and Applications},
  isbn = {978-0-13-373762-2},
  lccn = {TK5102.9 .P757 1996},
  shorttitle = {Digital Signal Processing},
  publisher = {{Prentice Hall}},
  author = {Proakis, John G. and Manolakis, Dimitris G.},
  year = {1996},
  keywords = {Digital techniques,Signal processing},
  file = {books/Proakis1996_Digital-signal-processing-principles,-algorithms,-and-applications.pdf}
}

@article{Gjorgjieva2011a,
  title = {A Triplet Spike-Timing-Dependent Plasticity Model Generalizes the {{Bienenstock}}-{{Cooper}}-{{Munro}} Rule to Higher-Order Spatiotemporal Correlations},
  volume = {108},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1105933108},
  language = {en},
  number = {48},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Gjorgjieva, J. and Clopath, C. and Audet, J. and Pfister, J.-P.},
  month = nov,
  year = {2011},
  pages = {19383-19388},
  file = {articles/Gjorgjieva2011.pdf}
}

@book{Jianfeng2007,
  edition = {1 edition},
  title = {Computational {{Neuroscience}}: {{A Comprehensive Approach}}},
  shorttitle = {Computational {{Neuroscience}}},
  abstract = {No description available},
  language = {English},
  publisher = {{CRC Press}},
  author = {Jianfeng, Feng},
  editor = {Feng, Jianfeng},
  month = apr,
  year = {2007},
  file = {books/Jianfeng2007_Computational-Neuroscience-A-Comprehensive-Approach.pdf}
}

@article{Izhikevich2008,
  title = {Large-Scale Model of Mammalian Thalamocortical Systems},
  volume = {105},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0712231105},
  abstract = {The understanding of the structural and dynamic complexity of mammalian brains is greatly facilitated by computer simulations. We present here a detailed large-scale thalamocortical model based on experimental measures in several mammalian species. The model spans three anatomical scales. (i) It is based on global (white-matter) thalamocortical anatomy obtained by means of diffusion tensor imaging (DTI) of a human brain. (ii) It includes multiple thalamic nuclei and six-layered cortical microcircuitry based on in vitro labeling and three-dimensional reconstruction of single neurons of cat visual cortex. (iii) It has 22 basic types of neurons with appropriate laminar distribution of their branching dendritic trees. The model simulates one million multicompartmental spiking neurons calibrated to reproduce known types of responses recorded in vitro in rats. It has almost half a billion synapses with appropriate receptor kinetics, short-term plasticity, and long-term dendritic spike-timing-dependent synaptic plasticity (dendritic STDP). The model exhibits behavioral regimes of normal brain activity that were not explicitly built-in but emerged spontaneously as the result of interactions among anatomical and dynamic processes. We describe spontaneous activity, sensitivity to changes in individual neurons, emergence of waves and rhythms, and functional connectivity on different scales.},
  language = {en},
  number = {9},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Izhikevich, Eugene M. and Edelman, Gerald M.},
  month = apr,
  year = {2008},
  keywords = {Cerebral Cortex,brain models,diffusion tensor imaging,oscillations,spike-timing-dependent synaptic plasticity},
  pages = {3593-3598},
  file = {articles/Izhikevich2008_2.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/N5NXEPP3/3593.html},
  pmid = {18292226}
}

@article{Markram2008,
  title = {Fixing the Location and Dimensions of Functional Neocortical Columns},
  volume = {2},
  issn = {1955-2068},
  doi = {10.2976/1.2919545},
  abstract = {The quest to understand the way in which neurons interconnect to form circuits that function as a unit began when Ramon y Cajal concluded that axo-dendritic apposition were too conspicuous to be incidental and proposed that two neurons must be communicating through these points of contact (see Shepherd and Erulkar, 1997, Trends Neurosci., 20, 385\textendash{}392). Lorente de N\'o was probably the first to predict that a defined group of vertically displaced neurons in the neocortex could form functional units (Lorente de N\'o, 1938, Physiology of the Nervous System, 20, OUP: 291\textendash{}330) for which Mountcastle found experimental evidence (see Mountcastle, 1997, Brain, 120, 701\textendash{}722) and which was ultimately demonstrated by Hubel and Wiesel in their elegant discovery of the orientation selective columns (Hubel and Wiesel, 1959, J. Physiol., 148, 574\textendash{}591). Until today, however, it is still not clear what shapes functional columns. Anatomical units, as in the barrel cortex, would make it easier to explain, but the neocortex is largely a continuous slab of closely packed neurons from which multiple modules emerge that can overlap partially or even completely on the same anatomical space. Are the columns in fixed anatomical locations or are they dynamically assigned and what anatomical and physiological properties are operating to shape their dimensions? A recent study explores how the geometry of single neurons places structural constraints on the dimensions of columns in the visual cortex (Stepanyants et al., 2008, Cereb Cortex, 18, 13\textendash{}24).},
  number = {3},
  journal = {HFSP Journal},
  author = {Markram, Henry},
  month = jun,
  year = {2008},
  pages = {132-135},
  file = {articles/Markram2008.pdf},
  pmid = {19404466},
  pmcid = {PMC2645561}
}

@article{Fares2009,
  title = {Cooperative Synapse Formation in the Neocortex},
  volume = {106},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0813265106},
  abstract = {Neuron morphology plays an important role in defining synaptic connectivity. Clearly, only pairs of neurons with closely positioned axonal and dendritic branches can be synaptically coupled. For excitatory neurons in the cerebral cortex, such axo-dendritic oppositions, termed potential synapses, must be bridged by dendritic spines to form synaptic connections. To explore the rules by which synaptic connections are formed within the constraints imposed by neuron morphology, we compared the distributions of the numbers of actual and potential synapses between pre- and postsynaptic neurons forming different laminar projections in rat barrel cortex. Quantitative comparison explicitly ruled out the hypothesis that individual synapses between neurons are formed independently of each other. Instead, the data are consistent with a cooperative scheme of synapse formation where multiple-synaptic connections between neurons are stabilized while neurons that do not establish a critical number of synapses are not likely to remain synaptically coupled.},
  language = {en},
  number = {38},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Fares, Tarec and Stepanyants, Armen},
  month = sep,
  year = {2009},
  keywords = {barrel cortex,connectivity,morphology,potential synapse,pyramidal cell},
  pages = {16463-16468},
  file = {articles/Fares2009.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/XUKKCNPW/16463.html},
  pmid = {19805321}
}

@article{Bush2010,
  title = {Reconciling the {{STDP}} and {{BCM Models}} of {{Synaptic Plasticity}} in a {{Spiking Recurrent Neural Network}}},
  volume = {22},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00003-Bush},
  abstract = {Rate-coded Hebbian learning, as characterized by the BCM formulation, is an established computational model of synaptic plasticity. Recently it has been demonstrated that changes in the strength of synapses in vivo can also depend explicitly on the relative timing of pre- and postsynaptic firing. Computational modeling of this spike-timing-dependent plasticity (STDP) has demonstrated that it can provide inherent stability or competition based on local synaptic variables. However, it has also been demonstrated that these properties rely on synaptic weights being either depressed or unchanged by an increase in mean stochastic firing rates, which directly contradicts empirical data. Several analytical studies have addressed this apparent dichotomy and identified conditions under which distinct and disparate STDP rules can be reconciled with rate-coded Hebbian learning. The aim of this research is to verify, unify, and expand on these previous findings by manipulating each element of a standard computational STDP model in turn. This allows us to identify the conditions under which this plasticity rule can replicate experimental data obtained using both rate and temporal stimulation protocols in a spiking recurrent neural network. Our results describe how the relative scale of mean synaptic weights and their dependence on stochastic pre- or postsynaptic firing rates can be manipulated by adjusting the exact profile of the asymmetric learning window and temporal restrictions on spike pair interactions respectively. These findings imply that previously disparate models of rate-coded autoassociative learning and temporally coded heteroassociative learning, mediated by symmetric and asymmetric connections respectively, can be implemented in a single network using a single plasticity rule. However, we also demonstrate that forms of STDP that can be reconciled with rate-coded Hebbian learning do not generate inherent synaptic competition, and thus some additional mechanism is required to guarantee long-term input-output selectivity.},
  number = {8},
  journal = {Neural Computation},
  author = {Bush, Daniel and Philippides, Andrew and Husbands, Phil and O'Shea, Michael},
  month = may,
  year = {2010},
  pages = {2059-2085},
  file = {articles/Bush2010.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/QX568NHZ/NECO_a_00003-Bush.html}
}

@book{Brette2012,
  address = {Cambridge},
  title = {Handbook of Neural Activity Measurement},
  isbn = {978-1-139-54906-6 1-139-54906-5 1-139-55156-6 978-1-139-55156-4 978-0-511-97995-8 0-511-97995-9 1-139-55402-6 978-1-139-55402-2},
  abstract = {"Neuroscientists employ many different techniques to observe the activity of the brain, from single-channel recording to functional imaging (fMRI). Many practical books explain how to use these techniques, but in order to extract meaningful information from the results it is necessary to understand the physical and mathematical principles underlying each measurement. This book covers an exhaustive range of techniques, with each chapter focusing on one in particular. Each author, a leading expert, explains exactly which quantity is being measured, the underlying principles at work, and most importantly the precise relationship between the signals measured and neural activity. The book is an important reference for neuroscientists who use these techniques in their own experimental protocols and need to interpret their results precisely; for computational neuroscientists who use such experimental results in their models; and for scientists who want to develop new measurement techniques or enhance existing ones"--Provided by publisher.},
  language = {English},
  publisher = {{Cambridge University Press}},
  author = {Brette, Romain and Destexhe, Alain},
  year = {2012},
  file = {books/Brette2012_Handbook-of-neural-activity-measurement.pdf}
}

@book{Edelman1987,
  address = {New York},
  edition = {New edition edition},
  title = {Neural {{Darwinism}}: {{The Theory Of Neuronal Group Selection}}},
  isbn = {978-0-465-04934-9},
  shorttitle = {Neural {{Darwinism}}},
  abstract = {Already the subject of considerable pre-publication discussion, this magisterial work by one of the nation's leading neuroscientists presents a radically new view of the function of the brain and nervous system. Its central idea is that the nervous system in each individual operates as a selective system resembling natural selection in evolution, but operating by different mechanisms. By providing a fundamental neural basis for categorization of the things of this world it unifies perception, action, and learning. The theory also completely revises our view of memory, which it considers to be a dynamic process of recategorization rather than a replicative store of attributes. This has deep implications for the interpretation of various psychological states from attention to dreaming.Neural Darwinism ranges over many disciplines, focusing on key problems in developmental and evolutionary biology, anatomy, physiology, ethology, and psychology. This book should therefore prove indispensable to advanced undergraduate and graduate students in these fields, to students of medicine, and to those in the social sciences concerned with the relation of behavior to biology. Beyond that, this far-ranging theory of brain function is bound to stimulate renewed discussions of such philosophical issues as the mind-body problem, the origins of knowledge, and the perceptual basis of language.},
  language = {English},
  publisher = {{Basic Books}},
  author = {Edelman, Gerald},
  month = dec,
  year = {1987}
}

@article{Snider2010,
  title = {A {{Universal Property}} of {{Axonal}} and {{Dendritic Arbors}}},
  volume = {66},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2010.02.013},
  abstract = {Axonal and dendritic arbors can be characterized statistically by their spatial density function, a function that specifies the probability of finding a branch of a particular arbor at each point in a neural circuit. Based on an analysis of over a thousand arbors from many neuron types in various species, we have discovered an unexpected simplicity in arbor structure: all of the arbors we have examined, both axonal and dendritic, can be described by a Gaussian density function truncated at about two standard deviations. Because all arbors are characterized by density functions with this single functional form, only four parameters are required to specify an arbor's size and shape: the total length of its branches and the standard deviations of the Gaussian in three orthogonal directions. This simplicity in arbor structure can have implications for the developmental wiring of neural circuits.},
  language = {English},
  number = {1},
  journal = {Neuron},
  author = {Snider, Joseph and Pillai, Andrea and Stevens, Charles F.},
  month = apr,
  year = {2010},
  keywords = {DEVBIO,SYSNEURO},
  pages = {45-56},
  file = {articles/Snider2010.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/RZFMAAC9/S0896-6273(10)00104-2.html},
  pmid = {20399728}
}

@book{Burgi2008,
  title = {Structure {{Correlation}}},
  isbn = {978-3-527-61608-4},
  abstract = {This book leaves the conventional view of chemical structures far behind: it demonstrates how a wealth of valuable, but hitherto unused information can be extracted from available structural data. For example, a single structure determination does not reveal much about a reaction pathway, but a sufficiently large number of comparable structures does. Finding the 'right' question is as important as is the intelligent use of crystallographic databases.Contributions by F.H. Allen, T.L. Blundell, I.D. Brown, H.B. B\"urgi, J.D. Dunitz, L. Leiserowitz and others, authoritatively discuss the structure correlation method as well as illustrative results in detail, covering such apparently unrelated subjects as * Bond strength relations in soldis* Crystal structure prediction* Reaction pathways of organic molecules* Ligand/receptor interactions and enzyme mechanismsThis book will be useful to the academic and industrial reader alike. It offers both fundamental aspects and diverse applications of what will surely become a powerful branch of structural chemistry.},
  language = {en},
  publisher = {{John Wiley \& Sons}},
  author = {B\"urgi, Hans-Beat and Dunitz, Jack D.},
  month = jul,
  year = {2008},
  keywords = {Science / Chemistry / Physical \& Theoretical}
}

@article{Klaus2011a,
  title = {Statistical {{Analyses Support Power Law Distributions Found}} in {{Neuronal Avalanches}}},
  volume = {6},
  doi = {10.1371/journal.pone.0019779},
  abstract = {The size distribution of neuronal avalanches in cortical networks has been reported to follow a power law distribution with exponent close to -1.5, which is a reflection of long-range spatial correlations in spontaneous neuronal activity. However, identifying power law scaling in empirical data can be difficult and sometimes controversial. In the present study, we tested the power law hypothesis for neuronal avalanches by using more stringent statistical analyses. In particular, we performed the following steps: (i) analysis of finite-size scaling to identify scale-free dynamics in neuronal avalanches, (ii) model parameter estimation to determine the specific exponent of the power law, and (iii) comparison of the power law to alternative model distributions. Consistent with critical state dynamics, avalanche size distributions exhibited robust scaling behavior in which the maximum avalanche size was limited only by the spatial extent of sampling (``finite size'' effect). This scale-free dynamics suggests the power law as a model for the distribution of avalanche sizes. Using both the Kolmogorov-Smirnov statistic and a maximum likelihood approach, we found the slope to be close to -1.5, which is in line with previous reports. Finally, the power law model for neuronal avalanches was compared to the exponential and to various heavy-tail distributions based on the Kolmogorov-Smirnov distance and by using a log-likelihood ratio test. Both the power law distribution without and with exponential cut-off provided significantly better fits to the cluster size distributions in neuronal avalanches than the exponential, the lognormal and the gamma distribution. In summary, our findings strongly support the power law scaling in neuronal avalanches, providing further evidence for critical state dynamics in superficial layers of cortex.},
  number = {5},
  journal = {PLoS ONE},
  author = {Klaus, Andreas and Yu, Shan and Plenz, Dietmar},
  month = may,
  year = {2011},
  pages = {e19779},
  file = {articles/Klaus2011.pdf}
}

@article{Shadlen1998,
  title = {The {{Variable Discharge}} of {{Cortical Neurons}}: {{Implications}} for {{Connectivity}}, {{Computation}}, and {{Information Coding}}},
  volume = {18},
  issn = {0270-6474, 1529-2401},
  shorttitle = {The {{Variable Discharge}} of {{Cortical Neurons}}},
  abstract = {Cortical neurons exhibit tremendous variability in the number and temporal distribution of spikes in their discharge patterns. Furthermore, this variability appears to be conserved over large regions of the cerebral cortex, suggesting that it is neither reduced nor expanded from stage to stage within a processing pathway. To investigate the principles underlying such statistical homogeneity, we have analyzed a model of synaptic integration incorporating a highly simplified integrate and fire mechanism with decay. We analyzed a ``high-input regime'' in which neurons receive hundreds of excitatory synaptic inputs during each interspike interval. To produce a graded response in this regime, the neuron must balance excitation with inhibition. We find that a simple integrate and fire mechanism with balanced excitation and inhibition produces a highly variable interspike interval, consistent with experimental data. Detailed information about the temporal pattern of synaptic inputs cannot be recovered from the pattern of output spikes, and we infer that cortical neurons are unlikely to transmit information in the temporal pattern of spike discharge. Rather, we suggest that quantities are represented as rate codes in ensembles of 50\textendash{}100 neurons. These column-like ensembles tolerate large fractions of common synaptic input and yet covary only weakly in their spike discharge. We find that an ensemble of 100 neurons provides a reliable estimate of rate in just one interspike interval (10\textendash{}50 msec). Finally, we derived an expression for the variance of the neural spike count that leads to a stable propagation of signal and noise in networks of neurons\textemdash{}that is, conditions that do not impose an accumulation or diminution of noise. The solution implies that single neurons perform simple algebra resembling averaging, and that more sophisticated computations arise by virtue of the anatomical convergence of novel combinations of inputs to the cortical column from external sources.},
  language = {en},
  number = {10},
  journal = {The Journal of Neuroscience},
  author = {Shadlen, Michael N. and Newsome, William T.},
  month = may,
  year = {1998},
  keywords = {Correlation,interspike interval,neural model,noise,rate code,response variability,spike count variance,synaptic integration,temporal coding,visual cortex},
  pages = {3870-3896},
  file = {articles/Shadlen1998.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/WXQ8K2P7/3870.html},
  pmid = {9570816}
}

@book{Laures2009,
  address = {Heidelberg},
  title = {{Grundkurs Topologie}},
  isbn = {978-3-8274-2040-4},
  abstract = {Die Topologie besch\"aftigt sich mit den qualitativen Eigenschaften geometrischer Objekte. Ihr Begriffsapparat ist so m\"achtig, dass kaum eine mathematische Struktur nicht mit Gewinn topologisiert wurde. Dieses Buch versteht sich als Br\"ucke von den einf\"uhrenden Vorlesungen der Analysis und Linearen Algebra zu den fortgeschrittenen Vorlesungen der Algebraischen und Geometrischen Topologie. Es eignet sich besonders f\"ur Studierende in einem Bachelor- oder Masterstudiengang der Mathematik, kann aber auch zum Selbststudium f\"ur mathematisch interessierte Naturwissenschaftler dienen. ... Definitionen werden stets mit vielen Beispielen unterlegt und neue Konzepte werden mit zahlreichen Bildern illustriert. \"Uber 170 \"Ubungsaufgaben (mit L\"osungen zu ausgew\"ahlten Aufgaben auf der Website zum Buch) helfen, die vermittelten Inhalte einzu\"uben und zu vertiefen. (3)},
  language = {ger},
  publisher = {{Spektrum, Akad. Verl}},
  author = {Laures, Gerd and Szymik, Markus},
  year = {2009},
  keywords = {Homotopie,Kompaktheit,Lehrbuch,Topologie,Transformationsgruppe},
  file = {books/Laures2009_Grundkurs-Topologie.pdf;books/Laures2009_Grundkurs-Topologie2.pdf}
}

@article{Gilson2010,
  title = {Emergence of Network Structure Due to Spike-Timing-Dependent Plasticity in Recurrent Neuronal Networks {{V}}: Self-Organization Schemes and Weight Dependence},
  volume = {103},
  issn = {0340-1200, 1432-0770},
  shorttitle = {Emergence of Network Structure Due to Spike-Timing-Dependent Plasticity in Recurrent Neuronal Networks {{V}}},
  doi = {10.1007/s00422-010-0405-7},
  abstract = {Spike-timing-dependent plasticity (STDP) determines the evolution of the synaptic weights according to their pre- and post-synaptic activity, which in turn changes the neuronal activity on a (much) slower time scale. This paper examines the effect of STDP in a recurrently connected network stimulated by external pools of input spike trains, where both input and recurrent synapses are plastic. Our previously developed theoretical framework is extended to incorporate weight-dependent STDP and dendritic delays. The weight dynamics is determined by an interplay between the neuronal activation mechanisms, the input spike-time correlations, and the learning parameters. For the case of two external input pools, the resulting learning scheme can exhibit a symmetry breaking of the input connections such that two neuronal groups emerge, each specialized to one input pool only. In addition, we show how the recurrent connections within each neuronal group can be strengthened by STDP at the expense of those between the two groups. This neuronal self-organization can be seen as a basic dynamical ingredient for the emergence of neuronal maps induced by activity-dependent plasticity.},
  language = {en},
  number = {5},
  journal = {Biological Cybernetics},
  author = {Gilson, Matthieu and Burkitt, Anthony N. and Grayden, David B. and Thomas, Doreen A. and van Hemmen, J. Leo},
  month = sep,
  year = {2010},
  keywords = {Bioinformatics,Computer Appl. in Life Sciences,Neurobiology,Neurosciences,Recurrent neuronal network,Self-organization,Spike-time correlation,Statistical Physics; Dynamical Systems and Complexity,Weight-dependent STDP,learning},
  pages = {365-386},
  file = {articles/Gilson2010.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/G5H2D8IF/s00422-010-0405-7.html}
}

@article{Kim2007,
  title = {Ubiquitous {{Plasticity}} and {{Memory Storage}}},
  volume = {56},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2007.10.030},
  abstract = {To date, most hypotheses of memory storage in the mammalian brain have focused upon long-term synaptic potentiation and depression (LTP and LTD) of fast glutamatergic excitatory postsynaptic currents (EPSCs). In recent years, it has become clear that many additional electrophysiological components of neurons, from electrical synapses to glutamate transporters to voltage-sensitive ion channels, can also undergo use-dependent long-term plasticity. Models of memory storage that incorporate this full range of demonstrated electrophysiological plasticity are better able to account for both the storage of memory in neuronal networks and the complexities of memory storage, indexing, and recall as measured behaviorally.},
  language = {English},
  number = {4},
  journal = {Neuron},
  author = {Kim, Sang Jeong and Linden, David J.},
  month = nov,
  year = {2007},
  pages = {582-592},
  file = {articles/Kim2007.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/27N4IGEE/S0896-6273(07)00826-4.html},
  pmid = {18031678}
}

@article{London2010,
  title = {Sensitivity to Perturbations in Vivo Implies High Noise and Suggests Rate Coding in Cortex},
  volume = {466},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature09086},
  number = {7302},
  journal = {Nature},
  author = {London, Michael and Roth, Arnd and Beeren, Lisa and H\"ausser, Michael and Latham, Peter E.},
  month = jul,
  year = {2010},
  pages = {123-127},
  file = {articles/London2010.pdf}
}

@article{Branco2011,
  title = {Synaptic {{Integration Gradients}} in {{Single Cortical Pyramidal Cell Dendrites}}},
  volume = {69},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2011.02.006},
  abstract = {Summary
Cortical pyramidal neurons receive thousands of synaptic inputs arriving at different dendritic locations with varying degrees of temporal synchrony. It is not known if different locations along single cortical dendrites integrate excitatory inputs in different ways. Here we have used two-photon glutamate uncaging and compartmental modeling to reveal a gradient of nonlinear synaptic integration in basal and apical oblique dendrites of cortical pyramidal neurons. Excitatory inputs to the proximal dendrite sum linearly and require precise temporal coincidence for effective summation, whereas distal inputs are amplified with high gain and integrated over broader time windows. This allows distal inputs to overcome their electrotonic disadvantage, and become surprisingly more effective than proximal inputs at influencing action potential output. Thus, single dendritic branches can already exhibit nonuniform synaptic integration, with the computational strategy shifting from temporal coding to rate coding along the dendrite.},
  number = {5},
  journal = {Neuron},
  author = {Branco, Tiago and H\"ausser, Michael},
  month = mar,
  year = {2011},
  pages = {885-892},
  file = {articles/Branco2011.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/6EC8Z2GX/S0896627311001036.html}
}

@unpublished{Kuwert2014,
  title = {Funktionalanalysis 2014},
  author = {Kuwert, Ernst},
  year = {2014},
  file = {manuscripts/Kuwert2014_Funktionalanalysis-2014.pdf}
}

@article{Rickert2009,
  title = {Dynamic {{Encoding}} of {{Movement Direction}} in {{Motor Cortical Neurons}}},
  volume = {29},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5441-08.2009},
  abstract = {When we perform a skilled movement such as reaching for an object, we can make use of prior information, for example about the location of the object in space. This helps us to prepare the movement, and we gain improved accuracy and speed during movement execution. Here, we investigate how prior information affects the motor cortical representation of movements during preparation and execution. We trained two monkeys in a delayed reaching task and provided a varying degree of prior information about the final target location. We decoded movement direction from multiple single-unit activity recorded from M1 (primary motor cortex) in one monkey and from PMd (dorsal premotor cortex) in a second monkey. Our results demonstrate that motor cortical cells in both areas exhibit individual encoding characteristics that change dynamically in time and dependent on prior information. On the population level, the information about movement direction is at any point in time accurately represented in a neuronal ensemble of time-varying composition. We conclude that movement representation in the motor cortex is not a static one, but one in which neurons dynamically allocate their computational resources to meet the demands defined by the movement task and the context of the movement. Consequently, we find that the decoding accuracy decreases if the precise task time, or the previous information that was available to the monkey, were disregarded in the decoding process. An optimal strategy for the readout of movement parameters from motor cortex should therefore take into account time and contextual parameters.},
  language = {en},
  number = {44},
  journal = {The Journal of Neuroscience},
  author = {Rickert, J\"orn and Riehle, Alexa and Aertsen, Ad and Rotter, Stefan and Nawrot, Martin P.},
  month = apr,
  year = {2009},
  pages = {13870-13882},
  file = {articles/Rickert2009.pdf},
  pmid = {19889998}
}

@article{Deger2012,
  title = {Spike-{{Timing Dependence}} of {{Structural Plasticity Explains Cooperative Synapse Formation}} in the {{Neocortex}}},
  volume = {8},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002689},
  abstract = {Author Summary  Structural plasticity has been observed even in the adult mammalian neocortex \textendash{} in seemingly static neuronal circuits structural remodeling is continuously at work. Still, it has been shown that the connection patterns between pairs of neurons are not random. In contrast, there is evidence that the synaptic contacts between a pair of neurons cooperate: several experimental studies report either zero or about 3\textendash{}6 synapses between neuron pairs. The mechanism by which the synapses cooperate, however, has not yet been identified. Here we propose a model for structural plasticity that relies on local processes at the dendritic spine. We combine and extend the previous models and determine the equilibrium probability distribution of synaptic contact numbers of the model. By optimizing the parameters numerically for each of three reference datasets, we obtain equilibrium contact number distributions that fit the references very well. We conclude that the local dendritic mechanisms that we assume suffice to explain the cooperative synapse formation in the neocortex.},
  number = {9},
  journal = {PLOS Comput Biol},
  author = {Deger, Moritz and Helias, Moritz and Rotter, Stefan and Diesmann, Markus},
  month = sep,
  year = {2012},
  keywords = {Action potentials,Neuronal dendrites,Neuronal plasticity,Neurons,Probability distribution,Synapses,neocortex,synaptic plasticity},
  pages = {e1002689},
  file = {articles/Deger2012_2.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/48TNV728/article.html}
}

@article{Burkitt2007,
  title = {Spike-Timing-Dependent Plasticity for Neurons with Recurrent Connections},
  volume = {96},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-007-0148-2},
  abstract = {The dynamics of the learning equation, which describes the evolution of the synaptic weights, is derived in the situation where the network contains recurrent connections. The derivation is carried out for the Poisson neuron model. The spiking-rates of the recurrently connected neurons and their cross-correlations are determined self- consistently as a function of the external synaptic inputs. The solution of the learning equation is illustrated by the analysis of the particular case in which there is no external synaptic input. The general learning equation and the fixed-point structure of its solutions is discussed.},
  language = {en},
  number = {5},
  journal = {Biological Cybernetics},
  author = {Burkitt, A. N. and Gilson, M. and van Hemmen, J. L.},
  month = apr,
  year = {2007},
  keywords = {Bioinformatics,Computer Appl. in Life Sciences,Neurobiology,Neurosciences},
  pages = {533-546},
  file = {articles/Burkitt2007.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/36XKZT22/10.html}
}

@article{Duijnhouwer2001,
  series = {Computational Neuroscience: Trends in Research 2001},
  title = {Influence of Dendritic Topology on Firing Patterns in Model Neurons},
  volume = {38\textendash{}40},
  issn = {0925-2312},
  doi = {10.1016/S0925-2312(01)00502-1},
  abstract = {Neuronal electrophysiology is influenced by both channel distribution and morphology. Distinguishing two sources of morphological variability\textemdash{}metrics and topology\textemdash{}we show that model neurons sharing the same channel densities and anatomical size can derive functional differentiation from their dendritic topology. Firing frequencies in these metrically reduced neurons show a strong correlation with both mean path length and total electrotonic transformed size of the dendritic tree. This dependency of spiking behaviour is robust to different modes of stimulation, different tapering powers, and the absence of active dendritic channels.},
  journal = {Neurocomputing},
  author = {Duijnhouwer, Jacob and Remme, Michiel W. H and {van Ooyen}, Arjen and {van Pelt}, Jaap},
  month = jun,
  year = {2001},
  keywords = {Firing patterns,Morpho-electrotonic transform,Path length,Topology,morphology},
  pages = {183-189},
  file = {articles/Duijnhouwer2001.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/R93ASR8H/S0925231201005021.html}
}

@book{Dayan2001,
  address = {Cambridge, Mass},
  edition = {1st edition},
  title = {Theoretical {{Neuroscience}}: {{Computational}} and {{Mathematical Modeling}} of {{Neural Systems}}},
  isbn = {978-0-262-04199-7},
  shorttitle = {Theoretical {{Neuroscience}}},
  abstract = {Theoretical neuroscience provides a quantitative basis for describing what nervous systems do, determining how they function, and uncovering the general principles by which they operate. This text introduces the basic mathematical and computational methods of theoretical neuroscience and presents applications in a variety of areas including vision, sensory-motor integration, development, learning, and memory.  The book is divided into three parts. Part I discusses the relationship between sensory stimuli and neural responses, focusing on the representation of information by the spiking activity of neurons. Part II discusses the modeling of neurons and neural circuits on the basis of cellular and synaptic biophysics. Part III analyzes the role of plasticity in development and learning. An appendix covers the mathematical methods used, and exercises are available on the book's Web site.},
  language = {English},
  publisher = {{The MIT Press}},
  author = {Dayan, Peter and Abbott, L. F.},
  month = dec,
  year = {2001},
  file = {books/Dayan2001_Theoretical-Neuroscience-Computational-and-Mathematical-Modeling-of-Neural-Systems.pdf}
}

@article{Hu2013a,
  title = {Motif Statistics and Spike Correlations in Neuronal Networks},
  volume = {2013},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/2013/03/P03012},
  number = {03},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  author = {Hu, Yu and Trousdale, James and Josi\'c, Kre{\v s}imir and {Shea-Brown}, Eric},
  month = mar,
  year = {2013},
  pages = {P03012},
  file = {articles/Hu2013.pdf}
}

@book{Fisher1995,
  address = {Cambridge [u.a.},
  title = {Statistical Analysis of Circular Data},
  isbn = {0-521-35018-2 978-0-521-35018-1 0-521-56890-0 978-0-521-56890-6},
  language = {English},
  publisher = {{Univ. Press}},
  author = {Fisher, Nicholas I},
  year = {1995},
  file = {books/Fisher1995_Statistical-analysis-of-circular-data.pdf}
}

@article{Schubert2001,
  title = {Layer-{{Specific Intracolumnar}} and {{Transcolumnar Functional Connectivity}} of {{Layer V Pyramidal Cells}} in {{Rat Barrel Cortex}}},
  volume = {21},
  issn = {0270-6474, 1529-2401},
  abstract = {Layer V pyramidal cells in rat barrel cortex are considered to play an important role in intracolumnar and transcolumnar signal processing. However, the precise circuitry mediating this processing is still incompletely understood. Here we obtained detailed maps of excitatory and inhibitory synaptic inputs onto the two major layer V pyramidal cell subtypes, intrinsically burst spiking (IB) and regular spiking (RS) cells, using a combination of caged glutamate photolysis, whole-cell patch-clamp recording, and three-dimensional reconstruction of biocytin-labeled cells. To excite presynaptic neurons with laminar specificity, the release of caged glutamate was calibrated and restricted to small areas of 50 \texttimes{} 50 $\mu$m in all cortical layers and in at least two neighboring barrel-related columns. IB cells received intracolumnar excitatory input from all layers, with the largest EPSP amplitudes originating from neurons in layers IV and VI. Prominent transcolumnar excitatory inputs were provided by presynaptic neurons also located in layers IV, V, and VI of neighboring columns. Inhibitory inputs were rare. In contrast, RS cells received distinct intracolumnar inhibitory inputs, especially from layers II/III and V. Intracolumnar excitatory inputs to RS cells were prominent from layers II\textendash{}V, but relatively weak from layer VI. Conspicuous transcolumnar excitatory inputs could be evoked solely in layers IV and V. Our results show that layer V pyramidal cells are synaptically driven by presynaptic neurons located in every layer of the barrel cortex. RS cells seem to be preferentially involved in intracolumnar signal processing, whereas IB cells effectively integrate excitatory inputs across several columns.},
  language = {en},
  number = {10},
  journal = {The Journal of Neuroscience},
  author = {Schubert, Dirk and Staiger, Jochen F. and Cho, Nichole and K\"otter, Rolf and Zilles, Karl and Luhmann, Heiko J.},
  month = may,
  year = {2001},
  keywords = {_tablet,barrel cortex,biocytin,burst spiking,caged glutamate,electrophysiology,excitatory inputs,functional connectivity,inhibitory inputs,layer V,morphology,pyramidal cell,regular spiking,slices,somatosensory},
  pages = {3580-3592},
  file = {articles/Schubert2001.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/XHDH5W3F/3580.html},
  pmid = {11331387}
}

@article{Kumar2010,
  title = {Spiking Activity Propagation in Neuronal Networks: Reconciling Different Perspectives on Neural Coding},
  volume = {11},
  issn = {1471-003X, 1471-0048},
  shorttitle = {Spiking Activity Propagation in Neuronal Networks},
  doi = {10.1038/nrn2886},
  number = {9},
  journal = {Nature Reviews Neuroscience},
  author = {Kumar, Arvind and Rotter, Stefan and Aertsen, Ad},
  month = sep,
  year = {2010},
  pages = {615-627},
  file = {articles/Kumar2010.pdf}
}

@article{Lubke2003,
  title = {Morphometric {{Analysis}} of the {{Columnar Innervation Domain}} of {{Neurons Connecting Layer}} 4 and {{Layer}} 2/3 of {{Juvenile Rat Barrel Cortex}}},
  volume = {13},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/13.10.1051},
  abstract = {We have investigated the dendritic and axonal morphology of connected pairs of L4 spiny neurons and L2/3 pyramidal cells in rat barrel cortex. The `projection' field of the axons of L4 spiny neurons in layers 2/3, 4 and 5 has a width of 400\textendash{}500 $\mu$m thereby defining an anatomical barrel-column. In layer 2/3, the averaged axonal `projection' field of L4 spiny neurons together with the dendritic `receptive' field of the connected L2/3 pyramidal cells form a mostly column-restricted anatomical L4-to-L2/3 `innervation domain' that extends 300\textendash{}400 $\mu$m and includes mostly basal dendrites. In the L4-to-L2/3 innervation domain a single L4 spiny neuron contacts \textasciitilde{}300\textendash{}400 pyramidal cells while in the L4-to-L4 innervation domain it contacts \textasciitilde{}200 other L4 spiny neurons. Similarly \textasciitilde{}300\textendash{}400 L4 spiny neurons converge onto a single pyramidal cell and \textasciitilde{}200 L4 spiny neurons innervate another L4 spiny neuron. The L2/3 pyramidal cell axon has a vertical projection field spanning all cortical layers, and a long-range horizontal field in layers 2/3 (width 1100\textendash{}1200 $\mu$m) and 5 (700\textendash{}800 $\mu$m) projecting across column borders. The results suggest that the flow of excitation within a barrel-column is determined by the largely columnar confinement of the L4-to-L4 and L4-to-L2/3 innervation domains. A whisker deflection activates \textasciitilde{}140 L4 spiny neurons that will generate EPSPs in most barrel-related L2/3 pyramidal cells of a principal whisker column. The translaminar synaptic transmission to layer 2/3 and the axonal projection fields of L2/3 pyramidal cells are the major determinants of the dynamic, multi-columnar map in which a single whisker deflection is represented in the cortex.},
  language = {en},
  number = {10},
  journal = {Cerebral Cortex},
  author = {L\"ubke, Joachim and Roth, Arnd and Feldmeyer, Dirk and Sakmann, Bert},
  month = jan,
  year = {2003},
  keywords = {_tablet},
  pages = {1051-1063},
  file = {articles/Lübke2003.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/EU8E4VUJ/1051.html},
  pmid = {12967922}
}

@article{Cooper1979,
  title = {A Theory for the Acquisition and Loss of Neuron Specificity in Visual Cortex},
  volume = {33},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00337414},
  abstract = {We assume that between lateral geniculate and visual cortical cells there exist labile synapses that modify themselves in a new fashion called threshold passive modification and in addition, non-labile synapses that contain permanent information. In the theory which results there is an increase in the specificity of response of a cortical cell when it is exposed to stimuli due to normal patterned visual experience. Non-patterned input, such as might be expected when an animal is dark-reared or raised with eyelids sutured, results in a loss of specificity, with details depending on whether noise to labile and non-labile junctions is correlated. Specificity can sometimes be regained, however, with a return of input due to patterned vision. We propose that this provides a possible explanation of experimental results obtained by Imbert and Buisseret (1975); Blakemore and Van Sluyters (1975); Buisseret and Imbert (1976); and Fr\'egnac and Imbert (1977, 1978).},
  language = {en},
  number = {1},
  journal = {Biological Cybernetics},
  author = {Cooper, Leon N. and Liberman, Fishel and Oja, Erkki},
  month = mar,
  year = {1979},
  keywords = {Neurosciences,Zoology},
  pages = {9-28},
  file = {articles/Cooper1979.pdf;articles/Cooper19792.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/W78EN59B/BF00337414.html}
}

@article{Edelman1993,
  title = {Neural {{Darwinism}}: {{Selection}} and Reentrant Signaling in Higher Brain Function},
  volume = {10},
  issn = {0896-6273},
  shorttitle = {Neural {{Darwinism}}},
  doi = {10.1016/0896-6273(93)90304-A},
  abstract = {Variation and selection within neural populations play key roles in the development and function of the brain. In this article, I review a population theory of the nervous system aimed at understanding the significance of these processes. Since its original formulation in 1978, considerable evidence has accumulated to support this theory of neuronal group selection. Extensive neural modeling based on the theory has provided useful insights into several outstanding neurobiological problems including those concerned with integration of cortical function, sensorimotor control, and perceptually based behavior.},
  number = {2},
  journal = {Neuron},
  author = {Edelman, Gerald M.},
  month = feb,
  year = {1993},
  pages = {115-125},
  file = {articles/Edelman1993.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/2D94R7FF/089662739390304A.html}
}

@article{Egger2014,
  title = {Generation of Dense Statistical Connectomes from Sparse Morphological Data},
  volume = {8},
  issn = {1662-5129},
  doi = {10.3389/fnana.2014.00129},
  abstract = {Sensory-evoked signal flow, at cellular and network levels, is primarily determined by the synaptic wiring of the underlying neuronal circuitry. Measurements of synaptic innervation, connection probabilities and subcellular organization of synaptic inputs are thus among the most active fields of research in contemporary neuroscience. Methods to measure these quantities range from electrophysiological recordings over reconstructions of dendrite-axon overlap at light-microscopic levels to dense circuit reconstructions of small volumes at electron-microscopic resolution. However, quantitative and complete measurements at subcellular resolution and mesoscopic scales to obtain all local and long-range synaptic in/outputs for any neuron within an entire brain region are beyond present methodological limits. Here, we present a novel concept, implemented within an interactive software environment called NeuroNet, which allows (i) integration of sparsely sampled (sub)cellular morphological data into an accurate anatomical reference frame of the brain region(s) of interest, (ii) up-scaling to generate an average dense model of the neuronal circuitry within the respective brain region(s) and (iii) statistical measurements of synaptic innervation between all neurons within the model. We illustrate our approach by generating a dense average model of the entire rat vibrissal cortex, providing the required anatomical data, and illustrate how to measure synaptic innervation statistically. Comparing our results with data from paired recordings in vitro and in vivo, as well as with reconstructions of synaptic contact sites at light- and electron-microscopic levels, we find that our in silico measurements are in line with previous results.},
  journal = {Frontiers in Neuroanatomy},
  author = {Egger, Robert and Dercksen, Vincent J. and Udvary, Daniel and Hege, Hans-Christian and Oberlaender, Marcel},
  month = nov,
  year = {2014},
  file = {articles/Egger2014.pdf},
  pmid = {25426033},
  pmcid = {PMC4226167}
}

@unpublished{Hoffmann2015g,
  title = {Topologie},
  author = {Hoffmann},
  year = {2015},
  file = {manuscripts/Hoffmann2015_Topologie.pdf}
}

@article{Deisseroth2013,
  title = {Engineering {{Approaches}} to {{Illuminating Brain Structure}} and {{Dynamics}}},
  volume = {80},
  issn = {08966273},
  doi = {10.1016/j.neuron.2013.10.032},
  language = {en},
  number = {3},
  journal = {Neuron},
  author = {Deisseroth, Karl and Schnitzer, Mark J.},
  month = oct,
  year = {2013},
  pages = {568-577},
  file = {articles/Deisseroth2013.pdf}
}

@article{Tkacik2014,
  title = {Searching for {{Collective Behavior}} in a {{Large Network}} of {{Sensory Neurons}}},
  volume = {10},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003408},
  language = {en},
  number = {1},
  journal = {PLoS Computational Biology},
  author = {Tka{\v c}ik, Ga{\v s}per and Marre, Olivier and Amodei, Dario and Schneidman, Elad and Bialek, William and Berry, Michael J.},
  editor = {Sporns, Olaf},
  month = jan,
  year = {2014},
  pages = {e1003408},
  file = {articles/Tkačik2014.pdf}
}

@article{Hartmann2015,
  title = {Where's the {{Noise}}? {{Key Features}} of {{Spontaneous Activity}} and {{Neural Variability Arise}} through {{Learning}} in a {{Deterministic Network}}},
  volume = {11},
  shorttitle = {Where's the {{Noise}}?},
  doi = {10.1371/journal.pcbi.1004640},
  abstract = {Author Summary Neural recordings seem very noisy. If the exact same stimulus is shown to an animal multiple times, the neural response will vary substantially. In fact, the activity of a single neuron shows many features of a random process. Furthermore, the spontaneous activity occurring in the absence of any sensory stimulus, which is usually considered a kind of background noise, often has a magnitude comparable to the activity evoked by stimulus presentation and interacts with sensory inputs in interesting ways. Here we show that the key features of neural variability and spontaneous activity can all be accounted for by a simple and completely deterministic neural network learning a predictive model of its sensory inputs. The network's deterministic dynamics give rise to structured but variable responses matching key experimental findings obtained in different mammalian species with different recording techniques. Our results suggest that the notorious variability of neural recordings and the complex features of spontaneous brain activity could reflect the dynamics of a largely deterministic but highly adaptive network learning a predictive model of its sensory environment.},
  number = {12},
  journal = {PLoS Comput Biol},
  author = {Hartmann, Christoph and Lazar, Andreea and Nessler, Bernhard and Triesch, Jochen},
  month = dec,
  year = {2015},
  pages = {e1004640},
  file = {articles/Hartmann2015.pdf}
}

@article{Gilbert1959,
  title = {Random {{Graphs}}},
  volume = {30},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177706098},
  abstract = {Project Euclid - mathematics and statistics online},
  language = {EN},
  number = {4},
  journal = {The Annals of Mathematical Statistics},
  author = {Gilbert, E. N.},
  month = dec,
  year = {1959},
  pages = {1141-1144},
  file = {articles/Gilbert1959.pdf},
  note = {Mathematical Reviews number (MathSciNet) MR108839, Zentralblatt MATH identifier0168.40801}
}

@article{Thomson2002,
  title = {Synaptic {{Connections}} and {{Small Circuits Involving Excitatory}} and {{Inhibitory Neurons}} in {{Layers}} 2\textendash{}5 of {{Adult Rat}} and {{Cat Neocortex}}: {{Triple Intracellular Recordings}} and {{Biocytin Labelling In Vitro}}},
  volume = {12},
  issn = {1047-3211, 1460-2199},
  shorttitle = {Synaptic {{Connections}} and {{Small Circuits Involving Excitatory}} and {{Inhibitory Neurons}} in {{Layers}} 2\textendash{}5 of {{Adult Rat}} and {{Cat Neocortex}}},
  doi = {10.1093/cercor/12.9.936},
  abstract = {Dual and triple intracellular recordings with biocytin labelling in slices of adult neocortex explored small circuits of synaptically connected neurons. 679 paired recordings in rat and 319 in cat yielded 135 and 42 excitatory postsynaptic potentials (EPSPs) and 37 and 26 inhibitory postsynaptic potentials (IPSPs), respectively. Patterns of connectivity and synaptic properties were similar in the two species, although differences of scale and in the range of morphologies were observed. Excitatory `forward' projections from layer 4 to 3, like those from layer 3 to 5, targeted pyramidal cells and a small proportion of interneurons, while excitatory `back' projections from layer 3 to 4 selected interneurons, including parvalbumin immuno-positive basket cells. Layer 4 interneurons that inhibited layer 3 pyramidal cells included both basket cells and dendrite-targeting cells. Large interneurons, resembling cells previously described as large basket cells, in layers 4 and 3 (cat), with long myelinated horizontal axon collaterals received frequent excitatory inputs from both layers. A very high rate of connectivity was observed between pairs of interneurons, often with quite different morphologies, and the resultant IPSPs, like the EPSPs recorded in interneurons, were brief compared with those recorded in pyramidal and spiny stellate cells.},
  language = {en},
  number = {9},
  journal = {Cerebral Cortex},
  author = {Thomson, Alex M. and West, David C. and Wang, Yun and Bannister, A. Peter},
  month = jan,
  year = {2002},
  pages = {936-953},
  file = {articles/Thomson2002.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/K7C4JVMR/936.html},
  pmid = {12183393}
}

@article{Feldmeyer2002,
  title = {Synaptic Connections between Layer 4 Spiny Neurone- Layer 2/3 Pyramidal Cell Pairs in Juvenile Rat Barrel Cortex: Physiology and Anatomy of Interlaminar Signalling within a Cortical Column},
  volume = {538},
  issn = {1469-7793},
  shorttitle = {Synaptic Connections between Layer 4 Spiny Neurone- Layer 2/3 Pyramidal Cell Pairs in Juvenile Rat Barrel Cortex},
  doi = {10.1113/jphysiol.2001.012959},
  abstract = {Whole-cell voltage recordings were obtained from 64 synaptically coupled excitatory layer 4 (L4) spiny neurones and L2/3 pyramidal cells in acute slices of the somatosensory cortex (`barrel' cortex) of 17- to 23-days-old rats. Single action potentials (APs) in the L4 spiny neurone evoked single unitary EPSPs in the L2/3 pyramidal cell with a peak amplitude of 0.7 $\pm$ 0.6 mV. The average latency was 2.1 $\pm$ 0.6 ms, the rise time was 0.8 $\pm$ 0.3 ms and the decay time constant was 12.7 $\pm$ 3.5 ms. The percentage of failures of an AP in a L4 spiny neurone to evoke a unitary EPSP in the L2/3 pyramidal cell was 4.9 $\pm$ 8.8 \% and the coefficient of variation (c.v.) of the unitary EPSP amplitude was 0.27 $\pm$ 0.13. Both c.v. and percentage of failures decreased with increased average EPSP amplitude. Postsynaptic glutamate receptors (GluRs) in L2/3 pyramidal cells were of the N-methyl-d-aspartate (NMDA) receptor (NMDAR) and the non-NMDAR type. At -60 mV in the presence of extracellular Mg2+ (1 mm), 29 $\pm$ 15 \% of the EPSP voltage-time integral was blocked by NMDAR antagonists. In 0 Mg2+, the NMDAR/AMPAR ratio of the EPSC was 0.50 $\pm$ 0.29, about half the value obtained for L4 spiny neurone connections. Burst stimulation of L4 spiny neurones showed that EPSPs in L2/3 pyramidal cells depressed over a wide range of frequencies (1\textendash{}100 s-1). However, at higher frequencies (30 s-1) EPSP summation overcame synaptic depression so that the summed EPSP was larger than the first EPSP amplitude in the train. The number of putative synaptic contacts established by the axonal collaterals of the L4 projection neurone with the target neurone in layer 2/3 varied between 4 and 5, with an average of 4.5 $\pm$ 0.5 (n= 13 pairs). Synapses were established on basal dendrites of the pyramidal cell. Their mean geometric distance from the pyramidal cell soma was 67 $\pm$ 34 $\mu$m (range, 16\textendash{}196 $\mu$m). The results suggest that each connected L4 spiny neurone produces a weak but reliable EPSP in the pyramidal cell. Therefore transmission of signals to layer 2/3 is likely to have a high threshold requiring simultaneous activation of many L4 neurons, implying that L4 spiny neurone to L2/3 pyramidal cell synapses act as a gate for the lateral spread of excitation in layer 2/3.},
  language = {en},
  number = {3},
  journal = {The Journal of Physiology},
  author = {Feldmeyer, Dirk and L\"ubke, Joachim and Silver, R. Angus and Sakmann, Bert},
  year = {2002},
  pages = {803-822},
  file = {articles/Feldmeyer2002.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/GMJHB7GE/abstract.html}
}

@book{Alt2012,
  address = {Berlin},
  edition = {6., \"uberarb. Aufl},
  title = {{Lineare Funktionalanalysis: eine anwendungsorientierte Einf\"uhrung}},
  isbn = {978-3-642-22260-3 978-3-642-22261-0},
  shorttitle = {{Lineare Funktionalanalysis}},
  language = {ger},
  publisher = {{Springer}},
  author = {Alt, Hans Wilhelm},
  year = {2012},
  keywords = {Dualraum,Functional analysis,Funktionenraum,Kompakter Operator,Lebesgue-Integral,Lehrbuch,Lineare Funktionalanalysis,Lineares Funktional,Spektraltheorie},
  file = {articles/Book/Alt20122.pdf;books/Alt2012_Lineare-Funktionalanalysis-eine-anwendungsorientierte-Einführung.pdf}
}

@article{Okabe2014,
  title = {Neurons {{Limit Angiogenesis}} by {{Titrating VEGF}} in {{Retina}}},
  volume = {159},
  issn = {00928674},
  doi = {10.1016/j.cell.2014.09.025},
  language = {en},
  number = {3},
  journal = {Cell},
  author = {Okabe, Keisuke and Kobayashi, Sakiko and Yamada, Toru and Kurihara, Toshihide and {Tai-Nagara}, Ikue and Miyamoto, Takeshi and Mukouyama, Yoh-suke and Sato, Thomas N. and Suda, Toshio and Ema, Masatsugu and Kubota, Yoshiaki},
  month = oct,
  year = {2014},
  pages = {584-596},
  file = {articles/Okabe2014.pdf}
}

@article{Borst2010,
  title = {Fly {{Motion Vision}}},
  volume = {33},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev-neuro-060909-153155},
  language = {en},
  number = {1},
  journal = {Annual Review of Neuroscience},
  author = {Borst, Alexander and Haag, Juergen and Reiff, Dierk F.},
  month = jun,
  year = {2010},
  pages = {49-70},
  file = {articles/Borst2010.pdf}
}

@article{Pouget2013,
  title = {Probabilistic Brains: Knowns and Unknowns},
  volume = {16},
  copyright = {\textcopyright{} 2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  shorttitle = {Probabilistic Brains},
  doi = {10.1038/nn.3495},
  abstract = {There is strong behavioral and physiological evidence that the brain both represents probability distributions and performs probabilistic inference. Computational neuroscientists have started to shed light on how these probabilistic representations and computations might be implemented in neural circuits. One particularly appealing aspect of these theories is their generality: they can be used to model a wide range of tasks, from sensory processing to high-level cognition. To date, however, these theories have only been applied to very simple tasks. Here we discuss the challenges that will emerge as researchers start focusing their efforts on real-life computations, with a focus on probabilistic learning, structural learning and approximate inference.},
  language = {en},
  number = {9},
  journal = {Nature Neuroscience},
  author = {Pouget, Alexandre and Beck, Jeffrey M. and Ma, Wei Ji and Latham, Peter E.},
  month = sep,
  year = {2013},
  pages = {1170-1178},
  file = {articles/Pouget2013.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/ZSZCM73F/nn.3495.html}
}

@book{Feller1968,
  address = {New York},
  edition = {3rd edition},
  title = {An {{Introduction}} to {{Probability Theory}} and {{Its Applications}}, {{Vol}}. 1, 3rd {{Edition}}},
  isbn = {978-0-471-25708-0},
  abstract = {Major changes in this edition include the substitution of probabilistic arguments for combinatorial artifices, and the addition of new sections on branching processes, Markov chains, and the De Moivre-Laplace theorem.},
  language = {English},
  publisher = {{Wiley}},
  author = {Feller, William},
  year = {1968},
  file = {books/Feller1968_An-Introduction-to-Probability-Theory-and-Its-Applications,-Vol.-1,-3rd-Edition.pdf}
}

@unpublished{Hoffmann2010a,
  title = {Kommutative {{Algebra}} - {{\"Ubungen}}},
  author = {Hoffmann, Felix},
  year = {2010},
  file = {manuscripts/Hoffmann2010_Kommutative-Algebra---Übungen.pdf}
}

@article{deSousa2015,
  title = {Dendritic Morphology Predicts Pattern Recognition Performance in Multi-Compartmental Model Neurons with and without Active Conductances},
  volume = {38},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-014-0537-1},
  language = {en},
  number = {2},
  journal = {Journal of Computational Neuroscience},
  author = {{de Sousa}, Giseli and Maex, Reinoud and Adams, Rod and Davey, Neil and Steuber, Volker},
  month = apr,
  year = {2015},
  pages = {221-234},
  file = {articles/de Sousa2015.pdf}
}

@article{Grabska-Barwinska2014,
  title = {How Well Do Mean Field Theories of Spiking Quadratic-Integrate-and-Fire Networks Work in Realistic Parameter Regimes?},
  volume = {36},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-013-0481-5},
  abstract = {We use mean field techniques to compute the distribution of excitatory and inhibitory firing rates in large networks of randomly connected spiking quadratic integrate and fire neurons. These techniques are based on the assumption that activity is asynchronous and Poisson. For most parameter settings these assumptions are strongly violated; nevertheless, so long as the networks are not too synchronous, we find good agreement between mean field prediction and network simulations. Thus, much of the intuition developed for randomly connected networks in the asynchronous regime applies to mildly synchronous networks.},
  language = {en},
  number = {3},
  journal = {Journal of Computational Neuroscience},
  author = {{Grabska-Barwi\'nska}, Agnieszka and Latham, Peter E.},
  month = jun,
  year = {2014},
  keywords = {Human Genetics,Mean field theory,Neurology,Neurosciences,Quadratic integrate and fire neuron,Random networks,Recurrent network,Synchronization,Theory of Computation,Theta neuron},
  pages = {469-481},
  file = {articles/Grabska-Barwińska2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/EQ8GSMBH/10.html}
}

@article{Maisak2013,
  title = {A Directional Tuning Map of {{Drosophila}} Elementary Motion Detectors},
  volume = {500},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature12320},
  number = {7461},
  journal = {Nature},
  author = {Maisak, Matthew S. and Haag, Juergen and Ammer, Georg and Serbe, Etienne and Meier, Matthias and Leonhardt, Aljoscha and Schilling, Tabea and Bahl, Armin and Rubin, Gerald M. and Nern, Aljoscha and Dickson, Barry J. and Reiff, Dierk F. and Hopp, Elisabeth and Borst, Alexander},
  month = aug,
  year = {2013},
  pages = {212-216},
  file = {articles/Maisak2013.pdf}
}

@article{Gjorgjieva2014,
  title = {Intrinsic {{Neuronal Properties Switch}} the {{Mode}} of {{Information Transmission}} in {{Networks}}},
  volume = {10},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003962},
  abstract = {Author Summary   Differences in ion channel composition endow different neuronal types with distinct computational properties. Understanding how these biophysical differences affect network-level computation is an important frontier. We focus on a set of biophysical properties, experimentally observed in developing cortical neurons, that allow these neurons to efficiently encode their inputs despite time-varying changes in the statistical context. Large-scale propagating waves are autonomously generated by the developing brain even before the onset of sensory experience. Using multi-layered feedforward networks, we examine how changes in intrinsic properties can lead to changes in the network's ability to represent and transmit information on multiple timescales. We demonstrate that measured changes in the computational properties of immature single neurons enable the propagation of slow-varying wave-like inputs. In contrast, neurons with more mature properties are more sensitive to fast fluctuations, which modulate the slow-varying information. While slow events are transmitted with high fidelity in initial network layers, noise degrades transmission in downstream network layers. Our results show how short-term adaptation and modulation of the neurons' input-output firing curves by background synaptic noise determine the ability of neural networks to transmit information on multiple timescales.},
  number = {12},
  journal = {PLOS Comput Biol},
  author = {Gjorgjieva, Julijana and Mease, Rebecca A. and Moody, William J. and Fairhall, Adrienne L.},
  month = dec,
  year = {2014},
  keywords = {Action potentials,Biophysics,Membrane potential,Neurons,Signaling networks,Single neuron function,Wave propagation,neural networks},
  pages = {e1003962},
  file = {articles/Gjorgjieva2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/P4TMUFZM/article.html}
}

@article{Mazzoni2007,
  title = {On the {{Dynamics}} of the {{Spontaneous Activity}} in {{Neuronal Networks}}},
  volume = {2},
  doi = {10.1371/journal.pone.0000439},
  abstract = {Most neuronal networks, even in the absence of external stimuli, produce spontaneous bursts of spikes separated by periods of reduced activity. The origin and functional role of these neuronal events are still unclear. The present work shows that the spontaneous activity of two very different networks, intact leech ganglia and dissociated cultures of rat hippocampal neurons, share several features. Indeed, in both networks: i) the inter-spike intervals distribution of the spontaneous firing of single neurons is either regular or periodic or bursting, with the fraction of bursting neurons depending on the network activity; ii) bursts of spontaneous spikes have the same broad distributions of size and duration; iii) the degree of correlated activity increases with the bin width, and the power spectrum of the network firing rate has a 1/f behavior at low frequencies, indicating the existence of long-range temporal correlations; iv) the activity of excitatory synaptic pathways mediated by NMDA receptors is necessary for the onset of the long-range correlations and for the presence of large bursts; v) blockage of inhibitory synaptic pathways mediated by GABAA receptors causes instead an increase in the correlation among neurons and leads to a burst distribution composed only of very small and very large bursts. These results suggest that the spontaneous electrical activity in neuronal networks with different architectures and functions can have very similar properties and common dynamics.},
  number = {5},
  journal = {PLoS ONE},
  author = {Mazzoni, Alberto and Broccard, Fr\'ed\'eric D. and {Garcia-Perez}, Elizabeth and Bonifazi, Paolo and Ruaro, Maria Elisabetta and Torre, Vincent},
  month = may,
  year = {2007},
  pages = {e439},
  file = {articles/Mazzoni2007.pdf}
}

@article{Markram2015,
  title = {Reconstruction and {{Simulation}} of {{Neocortical Microcircuitry}}},
  volume = {163},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2015.09.029},
  abstract = {Summary
We present a first-draft digital reconstruction of the microcircuitry of somatosensory cortex of juvenile rat. The reconstruction uses cellular and synaptic organizing principles to algorithmically reconstruct detailed anatomy and physiology from sparse experimental data. An objective anatomical method defines a neocortical volume of 0.29 $\pm$ 0.01~mm3 containing $\sim$31,000 neurons, and patch-clamp studies identify 55 layer-specific morphological and 207 morpho-electrical neuron subtypes. When digitally reconstructed neurons are positioned in the volume and synapse formation is restricted to biological bouton densities and numbers of synapses per connection, their overlapping arbors form $\sim$8 million connections with $\sim$37 million synapses. Simulations reproduce an~array of in~vitro and in~vivo experiments without parameter tuning. Additionally, we find a spectrum of network states with a sharp transition from synchronous to asynchronous activity, modulated by physiological mechanisms. The spectrum of network states, dynamically reconfigured around this transition, supports diverse information processing strategies.
PaperClip

Video Abstract},
  number = {2},
  journal = {Cell},
  author = {Markram, Henry and Muller, Eilif and Ramaswamy, Srikanth and Reimann, Michael W. and Abdellah, Marwan and Sanchez, Carlos Aguado and Ailamaki, Anastasia and {Alonso-Nanclares}, Lidia and Antille, Nicolas and Arsever, Selim and Kahou, Guy Antoine Atenekeng and Berger, Thomas K. and Bilgili, Ahmet and Buncic, Nenad and Chalimourda, Athanassia and Chindemi, Giuseppe and Courcol, Jean-Denis and Delalondre, Fabien and Delattre, Vincent and Druckmann, Shaul and Dumusc, Raphael and Dynes, James and Eilemann, Stefan and Gal, Eyal and Gevaert, Michael Emiel and Ghobril, Jean-Pierre and Gidon, Albert and Graham, Joe W. and Gupta, Anirudh and Haenel, Valentin and Hay, Etay and Heinis, Thomas and Hernando, Juan B. and Hines, Michael and Kanari, Lida and Keller, Daniel and Kenyon, John and Khazen, Georges and Kim, Yihwa and King, James G. and Kisvarday, Zoltan and Kumbhar, Pramod and Lasserre, S\'ebastien and Le B\'e, Jean-Vincent and Magalh\~aes, Bruno R. C. and {Merch\'an-P\'erez}, Angel and Meystre, Julie and Morrice, Benjamin Roy and Muller, Jeffrey and {Mu\~noz-C\'espedes}, Alberto and Muralidhar, Shruti and Muthurasa, Keerthan and Nachbaur, Daniel and Newton, Taylor H. and Nolte, Max and Ovcharenko, Aleksandr and Palacios, Juan and Pastor, Luis and Perin, Rodrigo and Ranjan, Rajnish and Riachi, Imad and Rodr\'iguez, Jos\'e-Rodrigo and Riquelme, Juan Luis and R\"ossert, Christian and Sfyrakis, Konstantinos and Shi, Ying and Shillcock, Julian C. and Silberberg, Gilad and Silva, Ricardo and Tauheed, Farhan and Telefont, Martin and {Toledo-Rodriguez}, Maria and Tr\"ankler, Thomas and Van Geit, Werner and D\'iaz, Jafet Villafranca and Walker, Richard and Wang, Yun and Zaninetta, Stefano M. and DeFelipe, Javier and Hill, Sean L. and Segev, Idan and Sch\"urmann, Felix},
  month = oct,
  year = {2015},
  pages = {456-492},
  file = {articles/Markram2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/26VCGNB9/S0092867415011915.html}
}

@article{Katz2009,
  title = {Synapse {{Distribution Suggests}} a {{Two}}-{{Stage Model}} of {{Dendritic Integration}} in {{CA1 Pyramidal Neurons}}},
  volume = {63},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2009.06.023},
  abstract = {Summary
Competing models have been proposed to explain how neurons integrate the thousands of inputs distributed throughout their dendritic trees. In~a simple global integration model, inputs from all locations sum in the axon. In a two-stage integration model, inputs contribute directly to dendritic spikes, and outputs from multiple branches sum in the axon. These two models yield opposite predictions of how synapses at different dendritic locations should be scaled if they are to contribute equally to neuronal output. We used serial-section electron microscopy to reconstruct individual apical oblique dendritic branches of CA1 pyramidal neurons and observe a synapse distribution consistent with the two-stage integration model. Computational modeling suggests that the observed synapse distribution enhances the~contribution of each dendritic branch to neuronal output.},
  number = {2},
  journal = {Neuron},
  author = {Katz, Yael and Menon, Vilas and Nicholson, Daniel A. and Geinisman, Yuri and Kath, William L. and Spruston, Nelson},
  month = jul,
  year = {2009},
  keywords = {CELLBIO,EVO_ECOL,SIGNALING},
  pages = {171-177},
  file = {articles/Katz2009.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/K2CWJTWS/S0896627309005108.html}
}

@book{Elstrodt2009,
  address = {Berlin},
  edition = {6., korrigierte Aufl},
  series = {Springer-Lehrbuch},
  title = {{Ma\ss{}- und Integrationstheorie}},
  isbn = {978-3-540-89727-9 978-3-540-89728-6},
  language = {ger},
  publisher = {{Springer}},
  author = {Elstrodt, J\"urgen},
  year = {2009},
  keywords = {Borel-Maß,Borel-Menge,Cantor-Diskontinuum,Integrals; generalized,Integrationstheorie,Konvergenztheorie,Lebesgue-Maß,Lehrbuch,Maßtheorie,Measure theory,Sigma-Algebra},
  file = {books/Elstrodt2009_Maß--und-Integrationstheorie.pdf}
}

@article{Graupner2013,
  title = {Synaptic {{Input Correlations Leading}} to {{Membrane Potential Decorrelation}} of {{Spontaneous Activity}} in {{Cortex}}},
  volume = {33},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0347-13.2013},
  language = {en},
  number = {38},
  journal = {Journal of Neuroscience},
  author = {Graupner, M. and Reyes, A. D.},
  month = sep,
  year = {2013},
  pages = {15075-15085},
  file = {articles/Graupner2013.pdf}
}

@article{Eshel2015,
  title = {Arithmetic and Local Circuitry Underlying Dopamine Prediction Errors},
  volume = {525},
  copyright = {\textcopyright{} 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {0028-0836},
  doi = {10.1038/nature14855},
  abstract = {Dopamine neurons are thought to facilitate learning by comparing actual and expected reward. Despite two decades of investigation, little is known about how this comparison is made. To determine how dopamine neurons calculate prediction error, we combined optogenetic manipulations with extracellular recordings in the ventral tegmental area while mice engaged in classical conditioning. Here we demonstrate, by manipulating the temporal expectation of reward, that dopamine neurons perform subtraction, a computation that is ideal for reinforcement learning but rarely observed in the brain. Furthermore, selectively exciting and inhibiting neighbouring GABA ($\gamma$-aminobutyric acid) neurons in the ventral tegmental area reveals that these neurons are a source of subtraction: they inhibit dopamine neurons when reward is expected, causally contributing to prediction-error calculations. Finally, bilaterally stimulating ventral tegmental area GABA neurons dramatically reduces anticipatory licking to conditioned odours, consistent with an important role for these neurons in reinforcement learning. Together, our results uncover the arithmetic and local circuitry underlying dopamine prediction errors.},
  language = {en},
  number = {7568},
  journal = {Nature},
  author = {Eshel, Neir and Bukwich, Michael and Rao, Vinod and Hemmelder, Vivian and Tian, Ju and Uchida, Naoshige},
  month = sep,
  year = {2015},
  keywords = {Learning and memory,Motivation,Reward},
  pages = {243-246},
  file = {articles/Eshel2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/RT5HCSGP/nature14855.html}
}

@article{Destexhe2009,
  title = {Self-Sustained Asynchronous Irregular States and {{Up}}\textendash{{Down}} States in Thalamic, Cortical and Thalamocortical Networks of Nonlinear Integrate-and-Fire Neurons},
  volume = {27},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-009-0164-4},
  abstract = {Randomly-connected networks of integrate-and-fire (IF) neurons are known to display asynchronous irregular (AI) activity states, which resemble the discharge activity recorded in the cerebral cortex of awake animals. However, it is not clear whether such activity states are specific to simple IF models, or if they also exist in networks where neurons are endowed with complex intrinsic properties similar to electrophysiological measurements. Here, we investigate the occurrence of AI states in networks of nonlinear IF neurons, such as the adaptive exponential IF (Brette-Gerstner-Izhikevich) model. This model can display intrinsic properties such as low-threshold spike (LTS), regular spiking (RS) or fast-spiking (FS). We successively investigate the oscillatory and AI dynamics of thalamic, cortical and thalamocortical networks using such models. AI states can be found in each case, sometimes with surprisingly small network size of the order of a few tens of neurons. We show that the presence of LTS neurons in cortex or in thalamus, explains the robust emergence of AI states for relatively small network sizes. Finally, we investigate the role of spike-frequency adaptation (SFA). In cortical networks with strong SFA in RS cells, the AI state is transient, but when SFA is reduced, AI states can be self-sustained for long times. In thalamocortical networks, AI states are found when the cortex is itself in an AI state, but with strong SFA, the thalamocortical network displays Up and Down state transitions, similar to intracellular recordings during slow-wave sleep or anesthesia. Self-sustained Up and Down states could also be generated by two-layer cortical networks with LTS cells. These models suggest that intrinsic properties such as adaptation and low-threshold bursting activity are crucial for the genesis and control of AI states in thalamocortical networks.},
  language = {en},
  number = {3},
  journal = {Journal of Computational Neuroscience},
  author = {Destexhe, Alain},
  month = jun,
  year = {2009},
  pages = {493-506},
  file = {articles/Destexhe2009.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/5CQ2DV2Q/10.html}
}

@article{Mazzoni2015,
  title = {Computing the {{Local Field Potential}} ({{LFP}}) from {{Integrate}}-and-{{Fire Network Models}}},
  volume = {11},
  doi = {10.1371/journal.pcbi.1004584},
  abstract = {Author Summary Leaky integrate-and-fire (LIF) networks are often used to model neural network activity. The spike trains they produce, however, cannot be directly compared to the local field potentials (LFPs) that are measured by low-pass filtering the potential recorded from extracellular electrodes. This is because LFPs are generated by neurons with spatial extensions, while LIF networks typically consist of point neurons. In order to still be able to approximately predict LFPs from LIF network simulations, we here explore simple proxies for computing LFPs based on standard output from LIF network simulations. Predictions from the various LFP proxies were compared with ``ground-truth'' LFPs computed by means of well-established volume conduction theory where synaptic currents corresponding to the LIF network simulation were injected into populations of multi-compartmental neurons with realistic morphologies. We found that a simple weighted sum of the LIF synaptic currents with a single universally applicable set of weights excellently capture the time course of the LFP signal when the LFP predominantly is generated by a single population of pyramidal cells. Our study therefore provides a simple formula by which the LFP signal can be estimated directly from the LIF network activity, providing a missing quantitative link between simple neural models and LFP measures in vivo.},
  number = {12},
  journal = {PLoS Comput Biol},
  author = {Mazzoni, Alberto and Lind\'en, Henrik and Cuntz, Hermann and Lansner, Anders and Panzeri, Stefano and Einevoll, Gaute T.},
  month = dec,
  year = {2015},
  pages = {e1004584},
  file = {articles/Mazzoni2015.pdf}
}

@techreport{zotero-403,
  title = {Beamer {{Class}} 3.33},
  file = {manuals/latex/beamer_class_3.33.pdf},
  note = {manuals/latex}
}

@incollection{Clopath2015,
  title = {Long {{Term Plasticity}}, {{Biophysical Models}}},
  copyright = {\textcopyright{}2015 Springer Science+Business Media New York (outside the USA)},
  isbn = {978-1-4614-6674-1 978-1-4614-6675-8},
  language = {en},
  booktitle = {Encyclopedia of {{Computational Neuroscience}}},
  publisher = {{Springer New York}},
  author = {Clopath, Claudia},
  editor = {Jaeger, Dieter and Jung, Ranu},
  year = {2015},
  keywords = {Computation by Abstract Devices,Neurobiology,Neurosciences},
  pages = {1628-1640},
  file = {book_sections/Clopath2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/D7QFNP75/978-1-4614-6675-8_351.html},
  doi = {10.1007/978-1-4614-6675-8_351}
}

@article{Eyherabide2013,
  title = {When and {{Why Noise Correlations Are Important}} in {{Neural Decoding}}},
  volume = {33},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0357-13.2013},
  abstract = {Information may be encoded both in the individual activity of neurons and in the correlations between their activities. Understanding whether knowledge of noise correlations is required to decode all the encoded information is fundamental for constructing computational models, brain\textendash{}machine interfaces, and neuroprosthetics. If correlations can be ignored with tolerable losses of information, the readout of neural signals is simplified dramatically. To that end, previous studies have constructed decoders assuming that neurons fire independently and then derived bounds for the information that is lost. However, here we show that previous bounds were not tight and overestimated the importance of noise correlations. In this study, we quantify the exact loss of information induced by ignoring noise correlations and show why previous estimations were not tight. Further, by studying the elementary parts of the decoding process, we determine when and why information is lost on a single-response basis. We introduce the minimum decoding error to assess the distinctive role of noise correlations under natural conditions. We conclude that all of the encoded information can be decoded without knowledge of noise correlations in many more situations than previously thought.},
  language = {en},
  number = {45},
  journal = {The Journal of Neuroscience},
  author = {Eyherabide, Hugo Gabriel and Samengo, In\'es},
  month = jun,
  year = {2013},
  pages = {17921-17936},
  file = {articles/Eyherabide2013.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/78SWGSDP/17921.html},
  pmid = {24198380}
}

@article{Muller-Dahlhaus2013,
  title = {Unraveling the Cellular and Molecular Mechanisms of Repetitive Magnetic Stimulation},
  volume = {6},
  issn = {1662-5099},
  doi = {10.3389/fnmol.2013.00050},
  journal = {Frontiers in Molecular Neuroscience},
  author = {{M\"uller-Dahlhaus}, Florian and Vlachos, Andreas},
  year = {2013},
  file = {articles/Müller-Dahlhaus2013.pdf}
}

@article{McCulloch1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  volume = {5},
  issn = {0007-4985, 1522-9602},
  doi = {10.1007/BF02478259},
  abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  language = {en},
  number = {4},
  journal = {The bulletin of mathematical biophysics},
  author = {McCulloch, Warren S. and Pitts, Walter},
  year = {1943},
  pages = {115-133},
  file = {articles/McCulloch1943.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/EANPFSUI/bf02478259.html}
}

@article{Ko2014,
  title = {Functional Organization of Synaptic Connections in the Neocortex},
  volume = {346},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1260780},
  abstract = {David Marr and Tomaso Poggio proposed that in order to figure out information processing in the brain, we must understand its operation at the computational, algorithmic, and implementational levels (1). Computational tasks of the visual system, for example, include extracting properties of the external world, such as recognizing objects, and estimating their locations and movements. Algorithmically, the visual system adopts a hierarchical organization, whereby visual features of increasing complexity are represented and integrated at successive stages of processing. Some retinal ganglion cells respond best to small, round, visual stimuli of high contrast. This information is relayed by the lateral geniculate nucleus of the thalamus to the primary visual cortex (V1), where neurons become sensitive to the orientation and motion direction of visual features (2). Further up the visual processing hierarchy, neuronal representations become increasingly more complex, as neurons become responsive to contours and objects often invariant of their precise location in visual space. What remains unknown is how, at the implementational level, these computations at different stages of the visual system are carried out by the neuronal networks. Similar to many proteins whose structures determined by crystallography provide mechanistic insights into their functions, knowledge of the connectivity-function relationship of neuronal networks may provide a mechanistic understanding of how the brain generates the representation of increasing levels of abstraction. In view of this, my work with Thomas Mrsic-Flogel and Sonja Hofer at University College London has helped to develop an experimental approach that allowed us to relate the connectivity between cortical neurons to their visual response properties (3).},
  language = {en},
  number = {6209},
  journal = {Science},
  author = {Ko, Ho},
  month = oct,
  year = {2014},
  pages = {555-555},
  file = {articles/Ko2014.pdf},
  pmid = {25359956}
}

@book{Feldmeyer2010,
  address = {New York ; London},
  edition = {2010 edition},
  title = {New {{Aspects}} of {{Axonal Structure}} and {{Function}}},
  isbn = {978-1-4419-1675-4},
  abstract = {Axons are neuronal output elements and are responsible for the transfer and processing of signals from one neuron to another, even over very large distances. For a given neuronal cell type, axons are unique and display very heterogeneous patterns with respect to shape, length and target structure. Axons are the usually long process of a nerve fiber that generally conducts impulses away from the body of the nerve cell. This book is intended to summarize recent findings covering morphological, physiological, developmental, computational and pathophysiological aspects of axons. It attempts to cover new findings concerning axonal structure and functions together with their implications for signal transduction, processes implicated in the formation of axonal arbors and the transport of subcellular elements to their targets, and finally how a dysfunction in one or several of these steps could lead to axonal degeneration and ultimately to neurodegenerative diseases.},
  language = {English},
  publisher = {{Springer}},
  editor = {Feldmeyer, Dirk and L\"ubke, Joachim},
  month = aug,
  year = {2010},
  file = {books/Feldmeyer2010_New-Aspects-of-Axonal-Structure-and-Function.pdf}
}

@unpublished{Hoffmann2012,
  title = {Appendix {{A}} - {{Topologie}}},
  author = {Hoffmann, Felix},
  year = {2012},
  file = {manuscripts/Hoffmann2012_Appendix-A---Topologie.pdf}
}

@article{Ferrante2013,
  title = {Functional {{Impact}} of {{Dendritic Branch}}-{{Point Morphology}}},
  volume = {33},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3495-12.2013},
  language = {en},
  number = {5},
  journal = {Journal of Neuroscience},
  author = {Ferrante, M. and Migliore, M. and Ascoli, G. A.},
  month = jan,
  year = {2013},
  pages = {2156-2165},
  file = {articles/Ferrante2013.pdf}
}

@article{Zaltieri2015,
  title = {-Synuclein and Synapsin {{III}} Cooperatively Regulate Synaptic Function in Dopamine Neurons},
  volume = {128},
  issn = {0021-9533, 1477-9137},
  doi = {10.1242/jcs.157867},
  language = {en},
  number = {13},
  journal = {Journal of Cell Science},
  author = {Zaltieri, M. and Grigoletto, J. and Longhena, F. and Navarria, L. and Favero, G. and Castrezzati, S. and Colivicchi, M. A. and Della Corte, L. and Rezzani, R. and Pizzi, M. and Benfenati, F. and Spillantini, M. G. and Missale, C. and Spano, P. and Bellucci, A.},
  month = jul,
  year = {2015},
  pages = {2231-2243},
  file = {articles/Zaltieri2015.pdf;articles/Zaltieri20152.pdf}
}

@article{Navlakha2015,
  title = {Decreasing-{{Rate Pruning Optimizes}} the {{Construction}} of {{Efficient}} and {{Robust Distributed Networks}}},
  volume = {11},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004347},
  abstract = {Author Summary   During development of neural circuits in the brain, synapses are massively over-produced and then pruned-back over time. This is a fundamental process that occurs in many brain regions and organisms, yet, despite decades of study of this process, the rate of synapse elimination, and how such rates affect the function and structure of networks, has not been studied. We performed large-scale brain imaging experiments to quantify synapse elimination rates in the developing mouse cortex and found that the rate is decreasing over time (i.e. aggressive elimination occurs early, followed by a longer phase of slow elimination). We show that such rates optimize the efficiency and robustness of distributed routing networks under several models. We also present an application of this strategy to improve the design of airline networks.},
  number = {7},
  journal = {PLOS Comput Biol},
  author = {Navlakha, Saket and Barth, Alison L. and {Bar-Joseph}, Ziv},
  month = jul,
  year = {2015},
  keywords = {Algorithms,Graphs,Imaging techniques,Network analysis,Signaling networks,Synapses,electron microscopy,neural networks},
  pages = {e1004347},
  file = {articles/Navlakha2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/WMNBMRUV/article.html}
}

@article{Waters2006,
  title = {Background {{Synaptic Activity Is Sparse}} in {{Neocortex}}},
  volume = {26},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2152-06.2006},
  language = {en},
  number = {32},
  journal = {Journal of Neuroscience},
  author = {Waters, J. and Helmchen, F.},
  month = aug,
  year = {2006},
  pages = {8267-8277},
  file = {articles/Waters2006.pdf}
}

@article{Granot-Atedgi2013,
  title = {Stimulus-Dependent {{Maximum Entropy Models}} of {{Neural Population Codes}}},
  volume = {9},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002922},
  language = {en},
  number = {3},
  journal = {PLoS Computational Biology},
  author = {{Granot-Atedgi}, Einat and Tka{\v c}ik, Ga{\v s}per and Segev, Ronen and Schneidman, Elad},
  editor = {Sporns, Olaf},
  month = mar,
  year = {2013},
  pages = {e1002922},
  file = {articles/Granot-Atedgi2013.pdf}
}

@article{Tan2013,
  title = {A Spontaneous State of Weakly Correlated Synaptic Excitation and Inhibition in Visual Cortex},
  volume = {247},
  issn = {03064522},
  doi = {10.1016/j.neuroscience.2013.05.037},
  language = {en},
  journal = {Neuroscience},
  author = {Tan, A.Y.Y. and Andoni, S. and Priebe, N.J.},
  month = sep,
  year = {2013},
  pages = {364-375},
  file = {articles/Tan2013.pdf}
}

@article{Sadeh2015,
  title = {Emergence of {{Functional Specificity}} in {{Balanced Networks}} with {{Synaptic Plasticity}}},
  volume = {11},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004307},
  abstract = {Author Summary   In primary visual cortex of mammals, neurons are selective to the orientation of contrast edges. In some species, as cats and monkeys, neurons preferring similar orientations are adjacent on the cortical surface, leading to smooth orientation maps. In rodents, in contrast, such spatial orientation maps do not exist, and neurons of different specificities are mixed in a salt-and-pepper fashion. During development, however, a ``functional'' map of orientation selectivity emerges, where connections between neurons of similar preferred orientations are selectively enhanced. Here we show how such feature-specific connectivity can arise in realistic neocortical networks of excitatory and inhibitory neurons. Our results demonstrate how recurrent dynamics can work in cooperation with synaptic plasticity to form networks where neurons preferring similar stimulus features connect more strongly together. Such networks, in turn, are known to enhance the specificity of neuronal responses to a stimulus. Our study thus reveals how self-organizing connectivity in neuronal networks enable them to achieve new or enhanced functions, and it underlines the essential role of recurrent inhibition and plasticity in this process.},
  number = {6},
  journal = {PLOS Comput Biol},
  author = {Sadeh, Sadra and Clopath, Claudia and Rotter, Stefan},
  month = jun,
  year = {2015},
  keywords = {Learning curves,Membrane potential,Neuronal plasticity,Neuronal tuning,Neurons,Synapses,neural networks,synaptic plasticity},
  pages = {e1004307},
  file = {articles/Sadeh2015_2.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/ESB8499U/article.html}
}

@article{Hu2013,
  title = {Motif Statistics and Spike Correlations in Neuronal Networks},
  volume = {2013},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/2013/03/P03012},
  number = {03},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  author = {Hu, Yu and Trousdale, James and Josi\'c, Kre{\v s}imir and {Shea-Brown}, Eric},
  month = mar,
  year = {2013},
  pages = {P03012},
  file = {articles/Hu2013_2.pdf}
}

@article{Lee2016a,
  title = {Anatomy and Function of an Excitatory Network in the Visual Cortex},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature17192},
  journal = {Nature},
  author = {Lee, Wei-Chung Allen and Bonin, Vincent and Reed, Michael and Graham, Brett J. and Hood, Greg and Glattfelder, Katie and Reid, R. Clay},
  month = mar,
  year = {2016},
  file = {articles/Lee2016.pdf}
}

@article{Hellwig2000,
  title = {A Quantitative Analysis of the Local Connectivity between Pyramidal Neurons in Layers 2/3 of the Rat Visual Cortex},
  volume = {82},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/PL00007964},
  abstract = {This study provides a detailed quantitative estimate for local synaptic connectivity between neocortical pyramidal neurons. A new way of obtaining such an estimate is presented. In acute slices of the rat visual cortex, four layer 2 and four layer 3 pyramidal neurons were intracellularly injected with biocytin. Axonal and dendritic arborizations were three-dimensionally reconstructed with the aid of a computer-based camera lucida system. In a computer experiment, pairs of pre- and postsynaptic neurons were formed and potential synaptic contacts were calculated. For each pair, the calculations were carried out for a whole range of distances (0 to 500 $\mu$m) between the presynaptic and the postsynaptic neuron, in order to estimate cortical connectivity as a function of the spatial separation of neurons. It was also differentiated whether neurons were situated in the same or in different cortical layers. The data thus obtained was used to compute connection probabilities, the average number of contacts between neurons, the frequency of specific numbers of contacts and the total number of contacts a dendritic tree receives from the surrounding cortical volume. Connection probabilities ranged from 50\% to 80\% for directly adjacent neurons and from 0\% to 15\% for neurons 500 $\mu$m apart. In many cases, connections were mediated by one contact only. However, close neighbors made on average up to 3 contacts with each other. The question as to whether the method employed in this study yields a realistic estimate of synaptic connectivity is discussed. It is argued that the results can be used as a detailed blueprint for building artificial neural networks with a cortex-like architecture.},
  language = {en},
  number = {2},
  journal = {Biological Cybernetics},
  author = {Hellwig, Bernhard},
  month = jan,
  year = {2000},
  pages = {111-121},
  file = {articles/Hellwig2000.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/6JMIN8KP/PL00007964.html}
}

@article{Plaza2014,
  title = {Toward Large-Scale Connectome Reconstructions},
  volume = {25},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.01.019},
  language = {en},
  journal = {Current Opinion in Neurobiology},
  author = {Plaza, Stephen M and Scheffer, Louis K and Chklovskii, Dmitri B},
  month = apr,
  year = {2014},
  keywords = {_tablet},
  pages = {201-210},
  file = {articles/Plaza2014.pdf}
}

@article{Egger2015,
  title = {Robustness of Sensory-Evoked Excitation Is Increased by Inhibitory Inputs to Distal Apical Tuft Dendrites},
  volume = {112},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1518773112},
  language = {en},
  number = {45},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Egger, Robert and Schmitt, Arno C. and Wallace, Damian J. and Sakmann, Bert and Oberlaender, Marcel and Kerr, Jason N. D.},
  month = nov,
  year = {2015},
  keywords = {_tablet},
  pages = {14072-14077},
  file = {articles/Egger2015.pdf}
}

@article{Clopath2010,
  title = {Connectivity Reflects Coding: A Model of Voltage-Based {{STDP}} with Homeostasis},
  volume = {13},
  copyright = {\textcopyright{} 2010 Nature Publishing Group},
  issn = {1097-6256},
  shorttitle = {Connectivity Reflects Coding},
  doi = {10.1038/nn.2479},
  abstract = {Electrophysiological connectivity patterns in cortex often have a few strong connections, which are sometimes bidirectional, among a lot of weak connections. To explain these connectivity patterns, we created a model of spike timing\textendash{}dependent plasticity (STDP) in which synaptic changes depend on presynaptic spike arrival and the postsynaptic membrane potential, filtered with two different time constants. Our model describes several nonlinear effects that are observed in STDP experiments, as well as the voltage dependence of plasticity. We found that, in a simulated recurrent network of spiking neurons, our plasticity rule led not only to development of localized receptive fields but also to connectivity patterns that reflect the neural code. For temporal coding procedures with spatio-temporal input correlations, strong connections were predominantly unidirectional, whereas they were bidirectional under rate-coded input with spatial correlations only. Thus, variable connectivity patterns in the brain could reflect different coding principles across brain areas; moreover, our simulations suggested that plasticity is fast.},
  language = {en},
  number = {3},
  journal = {Nature Neuroscience},
  author = {Clopath, Claudia and B\"using, Lars and Vasilaki, Eleni and Gerstner, Wulfram},
  month = mar,
  year = {2010},
  keywords = {STDP},
  pages = {344-352},
  file = {articles/Clopath2010.pdf}
}

@article{Rall1973,
  title = {Branch {{Input Resistance}} and {{Steady Attenuation}} for {{Input}} to {{One Branch}} of a {{Dendritic Neuron Model}}},
  volume = {13},
  issn = {0006-3495},
  doi = {10.1016/S0006-3495(73)86014-X},
  abstract = {Mathematical solutions and numerical illustrations are presented for the steady-state distribution of membrane potential in an extensively branched neuron model, when steady electric current is injected into only one dendritic branch. Explicit expressions are obtained for input resistance at the branch input site and for voltage attenuation from the input site to the soma; expressions for AC steady-state input impedance and attenuation are also presented. The theoretical model assumes passive membrane properties and the equivalent cylinder constraint on branch diameters. Numerical examples illustrate how branch input resistance and steady attenuation depend upon the following: the number of dendritic trees, the orders of dendritic branching, the electrotonic length of the dendritic trees, the location of the dendritic input site, and the input resistance at the soma. The application to cat spinal motoneurons, and to other neuron types, is discussed. The effect of a large dendritic input resistance upon the amount of local membrane depolarization at the synaptic site, and upon the amount of depolarization reaching the soma, is illustrated and discussed; simple proportionality with input resistance does not hold, in general. Also, branch input resistance is shown to exceed the input resistance at the soma by an amount that is always less than the sum of core resistances along the path from the input site to the soma.},
  number = {7},
  journal = {Biophysical Journal},
  author = {Rall, Wilfrid and Rinzel, John},
  month = jul,
  year = {1973},
  pages = {648-688},
  file = {articles/Rall1973.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/NDHZCP2S/S000634957386014X.html}
}

@article{Turrigiano1998,
  title = {Activity-Dependent Scaling of Quantal Amplitude in Neocortical Neurons},
  volume = {391},
  copyright = {\textcopyright{} 1998 Nature Publishing Group},
  issn = {0028-0836},
  doi = {10.1038/36103},
  abstract = {Information is stored in neural circuits through long-lasting changes in synaptic strengths,. Most studies of information storage have focused on mechanisms such as long-term potentiation and depression (LTP and LTD), in which synaptic strengths change in a synapse-specific manner,. In contrast, little attention has been paid to mechanisms that regulate the total synaptic strength of a neuron. Here we describe a new form of synaptic plasticity that increases or decreases the strength of all of a neuron's synaptic inputs as a function of activity. Chronic blockade of cortical culture activity increased the amplitude of miniature excitatory postsynaptic currents (mEPSCs) without changing their kinetics. Conversely, blocking GABA (-aminutyric acid)-mediated inhibition initially raised firing rates, but over a 48-hour period mESPC amplitudes decreased and firing rates returned to close to control values. These changes were at least partly due to postsynaptic alterations in the response to glutamate, and apparently affected each synapse in proportion to its initial strength. Such 'synaptic scaling' may help to ensure that firing rates do not become saturated during developmental changes in the number and strength of synaptic inputs, as well as stabilizing synaptic strengths during Hebbian modification, and facilitating competition between synapses.},
  language = {en},
  number = {6670},
  journal = {Nature},
  author = {Turrigiano, Gina G. and Leslie, Kenneth R. and Desai, Niraj S. and Rutherford, Lana C. and Nelson, Sacha B.},
  month = feb,
  year = {1998},
  pages = {892-896},
  file = {articles/Turrigiano1998.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/R2J24MZS/391892a0.html},
  note = {synaptic scaling}
}

@article{Sun2016,
  title = {Thalamus Provides Layer 4 of Primary Visual Cortex with Orientation- and Direction-Tuned Inputs},
  volume = {19},
  copyright = {\textcopyright{} 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  doi = {10.1038/nn.4196},
  abstract = {Understanding the functions of a brain region requires knowing the neural representations of its myriad inputs, local neurons and outputs. Primary visual cortex (V1) has long been thought to compute visual orientation from untuned thalamic inputs, but very few thalamic inputs have been measured in any mammal. We determined the response properties of \textasciitilde{}28,000 thalamic boutons and \textasciitilde{}4,000 cortical neurons in layers 1\textendash{}5 of awake mouse V1. Using adaptive optics that allows accurate measurement of bouton activity deep in cortex, we found that around half of the boutons in the main thalamorecipient L4 carried orientation-tuned information and that their orientation and direction biases were also dominant in the L4 neuron population, suggesting that these neurons may inherit their selectivity from tuned thalamic inputs. Cortical neurons in all layers exhibited sharper tuning than thalamic boutons and a greater diversity of preferred orientations. Our results provide data-rich constraints for refining mechanistic models of cortical computation.},
  language = {en},
  number = {2},
  journal = {Nature Neuroscience},
  author = {Sun, Wenzhi and Tan, Zhongchao and Mensh, Brett D. and Ji, Na},
  month = feb,
  year = {2016},
  keywords = {Motion detection,Neural circuits,Striate cortex,Thalamus},
  pages = {308-315},
  file = {articles/Sun2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/RMK3R2VF/nn.4196.html}
}

@article{Rumpel2016a,
  title = {The Dynamic Connectome: Keeping the Balance},
  author = {Rumpel, Simon and Kaschube, Matthias and Triesch, Jochen},
  year = {2016},
  file = {articles/Rumpel2016.pdf},
  note = {project\_proposal}
}

@book{Sornette2006,
  address = {Berlin ; New York},
  edition = {2nd ed},
  series = {Springer series in synergetics},
  title = {Critical Phenomena in Natural Sciences: Chaos, Fractals, Selforganization, and Disorder: Concepts and Tools},
  isbn = {978-3-540-30882-9},
  lccn = {QC173.4.C74 S67 2006},
  shorttitle = {Critical Phenomena in Natural Sciences},
  publisher = {{Springer}},
  author = {Sornette, Didier},
  year = {2006},
  keywords = {Critical phenomena (Physics)},
  file = {books/Sornette2006_Critical-phenomena-in-natural-sciences-chaos,-fractals,-selforganization,-and-disorder-concepts-and-tools.pdf},
  note = {OCLC: ocm67829893}
}

@article{Kesten1973,
  title = {Random Difference Equations and {{Renewal}} Theory for Products of Random Matrices},
  volume = {131},
  issn = {0001-5962, 1871-2509},
  doi = {10.1007/BF02392040},
  language = {en},
  number = {1},
  journal = {Acta Mathematica},
  author = {Kesten, Harry},
  year = {1973},
  pages = {207-248},
  file = {articles/Kesten1973.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/2Q2SX8TG/10.html}
}

@article{Goldie1991,
  title = {Implicit {{Renewal Theory}} and {{Tails}} of {{Solutions}} of {{Random Equations}}},
  volume = {1},
  issn = {1050-5164},
  abstract = {For the solutions of certain random equations, or equivalently the stationary solutions of certain random recurrences, the distribution tails are evaluated by renewal-theoretic methods. Six such equations, including one arising in queueing theory, are studied in detail. Implications in extreme-value theory are discussed by way of an illustration from economics.},
  number = {1},
  journal = {The Annals of Applied Probability},
  author = {Goldie, Charles M.},
  year = {1991},
  pages = {126-166},
  file = {articles/Goldie1991.pdf}
}

@article{Gillespie1996,
  title = {Exact Numerical Simulation of the {{Ornstein}}-{{Uhlenbeck}} Process and Its Integral},
  volume = {54},
  doi = {10.1103/PhysRevE.54.2084},
  abstract = {A numerical simulation algorithm that is exact for any time step $\Delta$t$>$0 is derived for the Ornstein-Uhlenbeck process X(t) and its time integral Y(t). The algorithm allows one to make efficient, unapproximated simulations of, for instance, the velocity and position components of a particle undergoing Brownian motion, and the electric current and transported charge in a simple R-L circuit, provided appropriate values are assigned to the Ornstein-Uhlenbeck relaxation time $\tau$ and diffusion constant c. A simple Taylor expansion in $\Delta$t of the exact simulation formulas shows how the first-order simulation formulas, which are implicit in the Langevin equation for X(t) and the defining equation for Y(t), are modified in second order. The exact simulation algorithm is used here to illustrate the zero-$\tau$ limit theorem. \textcopyright{} 1996 The American Physical Society.},
  number = {2},
  journal = {Physical Review E},
  author = {Gillespie, Daniel T.},
  month = aug,
  year = {1996},
  pages = {2084-2091},
  file = {articles/Gillespie1996.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/FBW3A83N/PhysRevE.54.html}
}

@article{Statman2014,
  title = {Synaptic {{Size Dynamics}} as an {{Effectively Stochastic Process}}},
  volume = {10},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003846},
  abstract = {Author Summary   Synapses are specialized sites of cell\textendash{}cell contact that serve to transmit signals between neurons and their targets, most commonly other neurons. It is widely believed that changes in synaptic properties, driven by prior activity or by other physiological signals, represent a major cellular mechanism by which neuronal networks are modified. Recent experiments show that in addition to directed changes, synaptic sizes also change spontaneously, with dynamics that seem to have strong stochastic components. In spite of these dynamics, however, population distributions of synaptic sizes are remarkably stable, and scale smoothly in response to various perturbations. In this study we show that fundamental aspects of synapse size dynamics are captured remarkably well by a simple statistical model known as the Kesten process: the random-like nature of synaptic size changes; the stability and shape of synaptic size distributions; their scaling following various perturbations; and the kinetics of new synapse formation. These findings indicate that the multiple microscopic processes involved in determining synaptic size combine in such a way that their collective behavior buffers many of the underlying details. The simplicity of the model and its robustness provide a new route for understanding the emergence of invariants at the level of the synaptic population.},
  number = {10},
  journal = {PLOS Comput Biol},
  author = {Statman, Adiel and Kaufman, Maya and Minerbi, Amir and Ziv, Noam E. and Brenner, Naama},
  month = oct,
  year = {2014},
  keywords = {Neurons,Stochastic processes,Synapses,neural networks,Population dynamics,Random variables,Statistical distributions,Statistical models},
  pages = {e1003846},
  file = {articles/Statman2014.pdf}
}

@article{Hartmann2016,
  title = {Precise {{Synaptic Efficacy Alignment Suggests Potentiation Dominated Learning}}},
  volume = {9},
  issn = {1662-5110},
  doi = {10.3389/fncir.2015.00090},
  abstract = {Recent evidence suggests that parallel synapses from the same axonal branch onto the same dendritic branch have almost identical strength. It has been proposed that this alignment is only possible through learning rules that integrate activity over long time spans. However, learning mechanisms such as spike-timing-dependent plasticity (STDP) are commonly assumed to be temporally local. Here, we propose that the combination of temporally local STDP and a multiplicative synaptic normalization mechanism is sufficient to explain the alignment of parallel synapses. To address this issue, we introduce three increasingly complex models: First, we model the idealized interaction of STDP and synaptic normalization in a single neuron as a simple stochastic process and derive analytically that the alignment effect can be described by a so-called Kesten process. From this we can derive that synaptic efficacy alignment requires potentiation-dominated learning regimes. We verify these conditions in a single-neuron model with independent spiking activities but more realistic synapses. As expected, we only observe synaptic efficacy alignment for long-term potentiation-biased STDP. Finally, we explore how well the findings transfer to recurrent neural networks where the learning mechanisms interact with the correlated activity of the network. We find that due to the self-reinforcing correlations in recurrent circuits under STDP, alignment occurs for both long-term potentiation- and depression-biased STDP, because the learning will be potentiation dominated in both cases due to the potentiating events induced by correlated activity. This is in line with recent results demonstrating a dominance of potentiation over depression during waking and normalization during sleep. This leads us to predict that individual spine pairs will be more similar after sleep compared to after sleep deprivation. In conclusion, we show that synaptic normalization in conjunction with coordinated potentiation\textemdash{}in this case, from STDP in the presence of correlated pre- and post-synaptic activity\textemdash{}naturally leads to an alignment of parallel synapses.},
  journal = {Frontiers in Neural Circuits},
  author = {Hartmann, Christoph and Miner, Daniel C. and Triesch, Jochen},
  month = jan,
  year = {2016},
  file = {articles/Hartmann2016.pdf}
}

@article{Lazar2009,
  title = {{{SORN}}: A Self-Organizing Recurrent Neural Network},
  volume = {3},
  shorttitle = {{{SORN}}},
  doi = {10.3389/neuro.10.023.2009},
  abstract = {Understanding the dynamics of recurrent neural networks is crucial for explaining how the brain processes information. In the neocortex, a range of different plasticity mechanisms are shaping recurrent networks into effective information processing circuits that learn appropriate representations for time-varying sensory stimuli. However, it has been difficult to mimic these abilities in artificial neural network models. Here we introduce SORN, a self-organizing recurrent network. It combines three distinct forms of local plasticity to learn spatio-temporal patterns in its input while maintaining its dynamics in a healthy regime suitable for learning. The SORN learns to encode information in the form of trajectories through its high-dimensional state space reminiscent of recent biological findings on cortical coding. All three forms of plasticity are shown to be essential for the network's success.},
  journal = {Frontiers in Computational Neuroscience},
  author = {Lazar, Andreea and Pipa, Gordon and Triesch, Jochen and Lazar, Andreea and Pipa, Gordon and Triesch, Jochen},
  year = {2009},
  keywords = {synaptic plasticity,intrinsic plasticity,recurrent neural networks,reservoir computing,time series prediction},
  pages = {23},
  file = {articles/Lazar2009.pdf}
}

@article{Desai1999,
  title = {Plasticity in the Intrinsic Excitability of Cortical Pyramidal Neurons},
  volume = {2},
  copyright = {\textcopyright{} 1999 Nature Publishing Group},
  issn = {1097-6256},
  doi = {10.1038/9165},
  abstract = {During learning and development, the level of synaptic input received by cortical neurons may change dramatically. Given a limited range of possible firing rates, how do neurons maintain responsiveness to both small and large synaptic inputs? We demonstrate that in response to changes in activity, cultured cortical pyramidal neurons regulate intrinsic excitability to promote stability in firing. Depriving pyramidal neurons of activity for two days increased sensitivity to current injection by selectively regulating voltage-dependent conductances. This suggests that one mechanism by which neurons maintain sensitivity to different levels of synaptic input is by altering the function relating current to firing rate.},
  language = {en},
  number = {6},
  journal = {Nature Neuroscience},
  author = {Desai, Niraj S. and Rutherford, Lana C. and Turrigiano, Gina G.},
  month = jun,
  year = {1999},
  pages = {515-520},
  file = {articles/Desai1999.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/W3TT2CIC/nn0699_515.html}
}

@article{Abbott2000,
  title = {Synaptic Plasticity: Taming the Beast},
  volume = {3},
  copyright = {\textcopyright{} 2000 Nature Publishing Group},
  shorttitle = {Synaptic Plasticity},
  doi = {10.1038/81453},
  abstract = {Synaptic plasticity provides the basis for most models of learning, memory and development in neural circuits. To generate realistic results, synapse-specific Hebbian forms of plasticity, such as long-term potentiation and depression, must be augmented by global processes that regulate overall levels of neuronal and network activity. Regulatory processes are often as important as the more intensively studied Hebbian processes in determining the consequences of synaptic plasticity for network function. Recent experimental results suggest several novel mechanisms for regulating levels of activity in conjunction with Hebbian synaptic modification. We review three of them\textemdash{}synaptic scaling, spike-timing dependent plasticity and synaptic redistribution\textemdash{}and discuss their functional implications.},
  language = {en},
  journal = {Nature Neuroscience},
  author = {Abbott, L. F. and Nelson, Sacha B.},
  month = nov,
  year = {2000},
  pages = {1178-1183},
  file = {articles/Abbott2000.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/XCP54DRA/nn1100_1178.html}
}

@article{Loewenstein2015,
  title = {Predicting the {{Dynamics}} of {{Network Connectivity}} in the {{Neocortex}}},
  volume = {35},
  copyright = {Copyright \textcopyright{} 2015 the authors 0270-6474/15/3512535-10\$15.00/0},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2917-14.2015},
  abstract = {Dynamic remodeling of connectivity is a fundamental feature of neocortical circuits. Unraveling the principles underlying these dynamics is essential for the understanding of how neuronal circuits give rise to computations. Moreover, as complete descriptions of the wiring diagram in cortical tissues are becoming available, deciphering the dynamic elements in these diagrams is crucial for relating them to cortical function. Here, we used chronic in vivo two-photon imaging to longitudinally follow a few thousand dendritic spines in the mouse auditory cortex to study the determinants of these spines' lifetimes. We applied nonlinear regression to quantify the independent contribution of spine age and several morphological parameters to the prediction of the future survival of a spine. We show that spine age, size, and geometry are parameters that can provide independent contributions to the prediction of the longevity of a synaptic connection. In addition, we use this framework to emulate a serial sectioning electron microscopy experiment and demonstrate how incorporation of morphological information of dendritic spines from a single time-point allows estimation of future connectivity states. The distinction between predictable and nonpredictable connectivity changes may be used in the future to identify the specific adaptations of neuronal circuits to environmental changes. The full dataset is publicly available for further analysis.
SIGNIFICANCE STATEMENT The neural architecture in the neocortex exhibits constant remodeling. The functional consequences of these modifications are poorly understood, in particular because the determinants of these changes are largely unknown. Here, we aimed to identify those modifications that are predictable from current network state. To that goal, we repeatedly imaged thousands of dendritic spines in the auditory cortex of mice to assess the morphology and lifetimes of synaptic connections. We developed models based on morphological features of dendritic spines that allow predicting future turnover of synaptic connections. The dynamic models presented in this paper provide a quantitative framework for adding putative temporal dynamics to the static description of a neuronal circuit from single time-point connectomics experiments.},
  language = {en},
  number = {36},
  journal = {Journal of Neuroscience},
  author = {Loewenstein, Yonatan and Yanover, Uri and Rumpel, Simon},
  month = sep,
  year = {2015},
  keywords = {Connectome,dendritic spines,synaptic dynamics},
  pages = {12535-12544},
  file = {articles/Loewenstein2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/C35U9SIA/12535.html},
  pmid = {26354919}
}

@article{Haas2006,
  title = {Spike-{{Timing}}-{{Dependent Plasticity}} of {{Inhibitory Synapses}} in the {{Entorhinal Cortex}}},
  volume = {96},
  copyright = {Copyright \textcopyright{} 2006 by the American Physiological Society},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00551.2006},
  abstract = {Actions of inhibitory interneurons organize and modulate many neuronal processes, yet the mechanisms and consequences of plasticity of inhibitory synapses remain poorly understood. We report on spike-timing-dependent plasticity of inhibitory synapses in the entorhinal cortex. After pairing presynaptic stimulations at time tpre with evoked postsynaptic spikes at time tpost under pharmacological blockade of excitation we found, via whole cell recordings, an asymmetrical timing rule for plasticity of the remaining inhibitory responses. Strength of response varied as a function of the time interval $\Delta$t = tpost - tpre: for $\Delta$t $>$ 0 inhibitory responses potentiated, peaking at a delay of 10 ms. For $\Delta$t $<$ 0, the synaptic coupling depressed, again with a maximal effect near 10 ms of delay. We also show that changes in synaptic strength depend on changes in intracellular calcium concentrations and demonstrate that the calcium enters the postsynaptic cell through voltage-gated channels. Using network models, we demonstrate how this novel form of plasticity can sculpt network behavior efficiently and with remarkable flexibility.},
  language = {en},
  number = {6},
  journal = {Journal of Neurophysiology},
  author = {Haas, Julie S. and Nowotny, Thomas and Abarbanel, H. D. I.},
  month = dec,
  year = {2006},
  pages = {3305-3313},
  file = {articles/Haas2006.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/77X7WVSU/3305.html},
  pmid = {16928795}
}

@article{Yasumatsu2008,
  title = {Principles of {{Long}}-{{Term Dynamics}} of {{Dendritic Spines}}},
  volume = {28},
  copyright = {Copyright \textcopyright{} 2008 Society for Neuroscience 0270-6474/08/2813592-17\$15.00/0},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0603-08.2008},
  abstract = {Long-term potentiation of synapse strength requires enlargement of dendritic spines on cerebral pyramidal neurons. Long-term depression is linked to spine shrinkage. Indeed, spines are dynamic structures: they form, change their shapes and volumes, or can disappear in the space of hours. Do all such changes result from synaptic activity, or do some changes result from intrinsic processes? How do enlargement and shrinkage of spines relate to elimination and generation of spines, and how do these processes contribute to the stationary distribution of spine volumes? To answer these questions, we recorded the volumes of many individual spines daily for several days using two-photon imaging of CA1 pyramidal neurons in cultured slices of rat hippocampus between postnatal days 17 and 23. With normal synaptic transmission, spines often changed volume or were created or eliminated, thereby showing activity-dependent plasticity. However, we found that spines changed volume even after we blocked synaptic activity, reflecting a native instability of these small structures over the long term. Such ``intrinsic fluctuations'' showed unique dependence on spine volume. A mathematical model constructed from these data and the theory of random fluctuations explains population behaviors of spines, such as rates of elimination and generation, stationary distribution of volumes, and the long-term persistence of large spines. Our study finds that generation and elimination of spines are more prevalent than previously believed, and spine volume shows significant correlation with its age and life expectancy. The population dynamics of spines also predict key psychological features of memory.},
  language = {en},
  number = {50},
  journal = {Journal of Neuroscience},
  author = {Yasumatsu, Nobuaki and Matsuzaki, Masanori and Miyazaki, Takashi and Noguchi, Jun and Kasai, Haruo},
  month = dec,
  year = {2008},
  keywords = {Hippocampus,Memory,synaptic plasticity,NMDA receptors,dendritic spine,slice culture},
  pages = {13592-13608},
  file = {articles/Yasumatsu2008.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/VSF3NUEB/13592.html},
  pmid = {19074033}
}

@article{Hoffmann2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1609.04245},
  primaryClass = {q-bio},
  title = {Non-Random Network Connectivity Comes in Pairs},
  abstract = {Overrepresentation of bidirectional connections in local cortical networks has been repeatedly reported and is in the focus of the ongoing discussion of non-random connectivity. Here we show in a brief mathematical analysis that in a network in which connection probabilities are symmetric in pairs, \$P\_\{ij\} = P\_\{ji\}\$, the occurrence of bidirectional connections and non-random structures are inherently linked; an overabundance of reciprocally connected pairs emerges necessarily when the network structure deviates from a random network in any form.},
  journal = {arXiv:1609.04245 [q-bio]},
  author = {Hoffmann, Felix Z. and Triesch, Jochen},
  month = sep,
  year = {2016},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {articles/Hoffmann2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/V5F7EF2K/1609.html}
}

@article{Kurth-Nelson2016,
  title = {Fast {{Sequences}} of {{Non}}-Spatial {{State Representations}} in {{Humans}}},
  volume = {91},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2016.05.028},
  abstract = {Summary
Fast internally generated sequences of neural representations are suggested to support learning and online planning. However, these sequences have only been studied in the context of spatial tasks and never in humans. Here, we recorded magnetoencephalography (MEG) while human subjects performed a novel non-spatial reasoning task. The task required selecting paths through a set of six visual objects. We trained pattern classifiers on the MEG activity elicited by direct presentation of the visual objects alone and tested these classifiers on activity recorded during periods when no object was presented. During these object-free periods, the brain spontaneously visited representations of approximately four objects in fast sequences lasting on the order of 120~ms. These sequences followed backward trajectories along the permissible paths in the task. Thus, spontaneous fast sequential representation of states can be measured non-invasively in humans, and these sequences may be a fundamental feature of neural computation across tasks.},
  number = {1},
  journal = {Neuron},
  author = {{Kurth-Nelson}, Zeb and Economides, Marcos and Dolan, Raymond J. and Dayan, Peter},
  month = jul,
  year = {2016},
  pages = {194-204},
  file = {articles/Kurth-Nelson2016.pdf}
}

@inproceedings{Ceolini2016,
  title = {Temporal Sequence Recognition in a Self-Organizing Recurrent Network},
  doi = {10.1109/EBCCSP.2016.7605258},
  abstract = {A big challenge of reservoir-based Recurrent Neural Networks (RNNs) is the optimization of the connection weights within the network so that the network performance is optimal for the intended task of temporal sequence recognition. One particular RNN called the Self-Organizing Recurrent Network (SORN) avoids the mathematical normalization required after each initialization. Instead, three types of cortical plasticity mechanisms optimize the weights within the network during the initial part of the training. The success of this unsupervised training method was demonstrated on temporal sequences that use input symbols with a binary encoding and that activate only one input pool in each time step. This work extends the analysis towards different types of symbol encoding ranging from encoding methods that activate multiple input pools and that use encoding levels that are not strictly binary but analog in nature. Preliminary results show that the SORN model is able to classify well temporal sequences with symbols using these encoding methods and the advantages of this network over a static network in a classification task is still retained.},
  booktitle = {2016 {{Second International Conference}} on {{Event}}-Based {{Control}}, {{Communication}}, and {{Signal Processing}} ({{EBCCSP}})},
  author = {Ceolini, E. and Neil, D. and Delbruck, T. and Liu, S. C.},
  month = jun,
  year = {2016},
  keywords = {Neurons,Statistics,Brain modeling,Encoding,Sociology,Sparse matrices,Training},
  pages = {1-4},
  file = {conferences/Ceolini2016_Temporal-sequence-recognition-in-a-self-organizing-recurrent-network.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/9MXR65T5/7605258.html}
}

@article{Fauth2015,
  title = {Formation and {{Maintenance}} of {{Robust Long}}-{{Term Information Storage}} in the {{Presence}} of {{Synaptic Turnover}}},
  volume = {11},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004684},
  language = {en},
  number = {12},
  journal = {PLOS Computational Biology},
  author = {Fauth, Michael and W\"org\"otter, Florentin and Tetzlaff, Christian},
  editor = {Beck, Jeff},
  month = dec,
  year = {2015},
  pages = {e1004684},
  file = {articles/Fauth2015_2.pdf}
}

@book{Nelsen2006,
  address = {New York},
  edition = {2nd ed},
  series = {Springer series in statistics},
  title = {An Introduction to Copulas},
  isbn = {978-0-387-28659-4},
  lccn = {QA273.6 .N45 2006},
  publisher = {{Springer}},
  author = {Nelsen, Roger B.},
  year = {2006},
  keywords = {Copulas (Mathematical statistics)},
  file = {books/Nelsen2006_An-introduction-to-copulas.pdf}
}

@article{Dvorkin2016,
  title = {Relative {{Contributions}} of {{Specific Activity Histories}} and {{Spontaneous Processes}} to {{Size Remodeling}} of {{Glutamatergic Synapses}}},
  volume = {14},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002572},
  language = {en},
  number = {10},
  journal = {PLOS Biology},
  author = {Dvorkin, Roman and Ziv, Noam E.},
  editor = {Schuman, Erin M},
  month = oct,
  year = {2016},
  pages = {e1002572},
  file = {articles/Dvorkin2016.PDF}
}

@article{Howe2016,
  title = {Rapid Signalling in Distinct Dopaminergic Axons during Locomotion and Reward},
  volume = {535},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature18942},
  number = {7613},
  journal = {Nature},
  author = {Howe, M. W. and Dombeck, D. A.},
  month = jul,
  year = {2016},
  pages = {505-510},
  file = {articles/Howe2016.pdf}
}

@article{Hofer2009,
  title = {Experience Leaves a Lasting Structural Trace in Cortical Circuits},
  volume = {457},
  copyright = {\textcopyright{} 2008 Nature Publishing Group},
  issn = {0028-0836},
  doi = {10.1038/nature07487},
  abstract = {Sensory experiences exert a powerful influence on the function and future performance of neuronal circuits in the mammalian neocortex. Restructuring of synaptic connections is believed to be one mechanism by which cortical circuits store information about the sensory world. Excitatory synaptic structures, such as dendritic spines, are dynamic entities that remain sensitive to alteration of sensory input throughout life. It remains unclear, however, whether structural changes at the level of dendritic spines can outlast the original experience and thereby provide a morphological basis for long-term information storage. Here we follow spine dynamics on apical dendrites of pyramidal neurons in functionally defined regions of adult mouse visual cortex during plasticity of eye-specific responses induced by repeated closure of one eye (monocular deprivation). The first monocular deprivation episode doubled the rate of spine formation, thereby increasing spine density. This effect was specific to layer-5 cells located in binocular cortex, where most neurons increase their responsiveness to the non-deprived eye. Restoring binocular vision returned spine dynamics to baseline levels, but absolute spine density remained elevated and many monocular deprivation-induced spines persisted during this period of functional recovery. However, spine addition did not increase again when the same eye was closed for a second time. This absence of structural plasticity stands out against the robust changes of eye-specific responses that occur even faster after repeated deprivation. Thus, spines added during the first monocular deprivation experience may provide a structural basis for subsequent functional shifts. These results provide a strong link between functional plasticity and specific synaptic rearrangements, revealing a mechanism of how prior experiences could be stored in cortical circuits.},
  language = {en},
  number = {7227},
  journal = {Nature},
  author = {Hofer, Sonja B. and {Mrsic-Flogel}, Thomas D. and Bonhoeffer, Tobias and H\"ubener, Mark},
  month = jan,
  year = {2009},
  pages = {313-317},
  file = {articles/Hofer2009.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/ASXF9CQ6/nature07487.html}
}

@unpublished{Hoffmann2009b,
  title = {Linear {{Algebra}} - {{Basis}} of a Vector Space},
  author = {Hoffmann, Felix},
  year = {2009},
  file = {manuscripts/Hoffmann2009_Linear-Algebra---Basis-of-a-vector-space.pdf}
}

@unpublished{Hoffmann2009a,
  title = {Analysis - {{Mastercards}}},
  author = {Hoffmann, Felix},
  year = {2009},
  file = {manuscripts/Hoffmann2009_Analysis---Mastercards.pdf}
}

@unpublished{Hoffmann2009,
  title = {Analysis - {{Zusammenfassung}}},
  author = {Hoffmann, Felix},
  year = {2009},
  file = {manuscripts/Hoffmann2009_Analysis---Zusammenfassung.pdf}
}

@unpublished{Hoffmann2011,
  title = {Linear {{Algebra}} - Script},
  author = {Hoffmann, Felix},
  year = {2011},
  file = {manuscripts/Hoffmann2011_Linear-Algebra---script.pdf}
}

@article{Mandali2016,
  title = {Electrode {{Position}} and {{Current Amplitude Modulate Impulsivity}} after {{Subthalamic Stimulation}} in {{Parkinsons Disease}}\textemdash{{A Computational Study}}},
  volume = {7},
  issn = {1664-042X},
  doi = {10.3389/fphys.2016.00585},
  abstract = {Background: Subthalamic Nucleus Deep Brain Stimulation (STN-DBS) is highly effective in alleviating motor symptoms of Parkinson's disease (PD) which are not optimally controlled by dopamine replacement therapy. Clinical studies and reports suggest that STN-DBS may result in increased impulsivity and de novo impulse control disorders (ICD)., Objective/Hypothesis: We aimed to compare performance on a decision making task, the Iowa Gambling Task (IGT), in healthy conditions (HC), untreated and medically-treated PD conditions with and without STN stimulation. We hypothesized that the position of electrode and stimulation current modulate impulsivity after STN-DBS., Methods: We built a computational spiking network model of basal ganglia (BG) and compared the model's STN output with STN activity in PD. Reinforcement learning methodology was applied to simulate IGT performance under various conditions of dopaminergic and STN stimulation where IGT total and bin scores were compared among various conditions., Results: The computational model reproduced neural activity observed in normal and PD conditions. Untreated and medically-treated PD conditions had lower total IGT scores (higher impulsivity) compared to HC (P $<$ 0.0001). The electrode position that happens to selectively stimulate the part of the STN corresponding to an advantageous panel on IGT resulted in de-selection of that panel and worsening of performance (P $<$ 0.0001). Supratherapeutic stimulation amplitudes also worsened IGT performance (P $<$ 0.001)., Conclusion(s): In our computational model, STN stimulation led to impulsive decision making in IGT in PD condition. Electrode position and stimulation current influenced impulsivity which may explain the variable effects of STN-DBS reported in patients.},
  journal = {Frontiers in Physiology},
  author = {Mandali, Alekhya and Chakravarthy, V. Srinivasa and Rajan, Roopa and Sarma, Sankara and Kishore, Asha},
  month = nov,
  year = {2016},
  file = {articles/Mandali2016.pdf}
}

@article{Tsuchiya2016,
  title = {Using Category Theory to Assess the Relationship between Consciousness and Integrated Information Theory},
  volume = {107},
  issn = {0168-0102},
  doi = {10.1016/j.neures.2015.12.007},
  abstract = {One of the most mysterious phenomena in science is the nature of conscious experience. Due to its subjective nature, a reductionist approach is having a hard time in addressing some fundamental questions about consciousness. These questions are squarely and quantitatively tackled by a recently developed theoretical framework, called integrated information theory (IIT) of consciousness. In particular, IIT proposes that a maximally irreducible conceptual structure (MICS) is identical to conscious experience. However, there has been no principled way to assess the claimed identity. Here, we propose to apply a mathematical formalism, category theory, to assess the proposed identity and suggest that it is important to consider if there exists a proper translation between the domain of conscious experience and that of the MICS. If such translation exists, we postulate that questions in one domain can be answered in the other domain; very difficult questions in the domain of consciousness can be resolved in the domain of mathematics. We claim that it is possible to empirically test if such a functor exists, by using a combination of neuroscientific and computational approaches. Our general, principled and empirical framework allows us to assess the relationship between the domain of consciousness and the domain of mathematical structures, including those suggested by IIT.},
  journal = {Neuroscience Research},
  author = {Tsuchiya, Naotsugu and Taguchi, Shigeru and Saigo, Hayato},
  month = jun,
  year = {2016},
  keywords = {Category theory,Consciousness,Equivalence,Integrated information theory,Phenomenology,Qualia},
  pages = {1-7},
  file = {articles/Tsuchiya2016.pdf}
}

@book{Murphy2012,
  address = {Cambridge, MA},
  series = {Adaptive computation and machine learning series},
  title = {Machine Learning: A Probabilistic Perspective},
  isbn = {978-0-262-01802-9},
  lccn = {Q325.5 .M87 2012},
  shorttitle = {Machine Learning},
  publisher = {{MIT Press}},
  author = {Murphy, Kevin P.},
  year = {2012},
  keywords = {Machine learning,Probabilities},
  file = {books/Murphy2012_Machine-learning-a-probabilistic-perspective.pdf}
}

@book{Emoto2016,
  address = {Tokyo},
  title = {Dendrites},
  isbn = {978-4-431-56048-7 978-4-431-56050-0},
  language = {en},
  publisher = {{Springer Japan}},
  editor = {Emoto, Kazuo and Wong, Rachel and Huang, Eric and Hoogenraad, Casper},
  year = {2016},
  file = {books/Emoto2016_Dendrites.pdf}
}

@book{Pearl2000,
  address = {Cambridge, U.K. ; New York},
  title = {Causality: Models, Reasoning, and Inference},
  isbn = {978-0-521-89560-6 978-0-521-77362-1},
  lccn = {BD541 .P43 2000},
  shorttitle = {Causality},
  publisher = {{Cambridge University Press}},
  author = {Pearl, Judea},
  year = {2000},
  keywords = {Probabilities,Causation},
  file = {books/Pearl2000_Causality-models,-reasoning,-and-inference.epub;books/Pearl2000_Causality-models,-reasoning,-and-inference.pdf}
}

@article{Rosenbaum2016,
  title = {The Spatial Structure of Correlated Neuronal Variability},
  volume = {20},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4433},
  number = {1},
  journal = {Nature Neuroscience},
  author = {Rosenbaum, Robert and Smith, Matthew A and Kohn, Adam and Rubin, Jonathan E and Doiron, Brent},
  month = oct,
  year = {2016},
  pages = {107-114},
  file = {articles/Rosenbaum2016_2.pdf;articles/Rosenbaum2016.pdf}
}

@article{Rolls2013,
  title = {The Mechanisms for Pattern Completion and Pattern Separation in the Hippocampus},
  volume = {7},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2013.00074},
  abstract = {The mechanisms for pattern completion and pattern separation are described in the context of a theory of hippocampal function in which the hippocampal CA3 system operates as a single attractor or autoassociation network to enable rapid, one-trial, associations between any spatial location (place in rodents, or spatial view in primates) and an object or reward, and to provide for completion of the whole memory during recall from any part. The factors important in the pattern completion in CA3 together with a large number of independent memories stored in CA3 include a sparse distributed representation which is enhanced by the graded firing rates of CA3 neurons, representations that are independent due to the randomizing effect of the mossy fibers, heterosynaptic long-term depression as well as long-term potentiation in the recurrent collateral synapses, and diluted connectivity to minimize the number of multiple synapses between any pair of CA3 neurons which otherwise distort the basins of attraction. Recall of information from CA3 is implemented by the entorhinal cortex perforant path synapses to CA3 cells, which in acting as a pattern associator allow some pattern generalization. Pattern separation is performed in the dentate granule cells using competitive learning to convert grid-like entorhinal cortex firing to place-like fields. Pattern separation in CA3, which is important for completion of any one of the stored patterns from a fragment, is provided for by the randomizing effect of the mossy fiber synapses to which neurogenesis may contribute, by the large number of dentate granule cells each with a sparse representation, and by the sparse independent representations in CA3. Recall to the neocortex is achieved by a reverse hierarchical series of pattern association networks implemented by the hippocampo-cortical backprojections, each one of which performs some pattern generalization, to retrieve a complete pattern of cortical firing in higher-order cortical areas.},
  journal = {Frontiers in Systems Neuroscience},
  author = {Rolls, Edmund T.},
  month = oct,
  year = {2013},
  file = {articles/Rolls2013.pdf}
}

@article{Zenke2015,
  title = {Diverse Synaptic Plasticity Mechanisms Orchestrated to Form and Retrieve Memories in Spiking Neural Networks},
  volume = {6},
  copyright = {\textcopyright{} 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  issn = {2041-1723},
  doi = {10.1038/ncomms7922},
  abstract = {The brain exhibits a diversity of plasticity mechanisms across different timecales that constitute the putative basis for learning and memory. Here, the authors demonstrate how these different plasticity mechanisms are orchestrated to support the formation of robust and stable neural cell assemblies.},
  language = {en},
  journal = {Nature Communications},
  author = {Zenke, Friedemann and Agnes, Everton J. and Gerstner, Wulfram},
  month = apr,
  year = {2015},
  pages = {6922},
  file = {articles/Zenke2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/H97DK2DW/ncomms7922.html}
}

@article{Abbott2016,
  title = {Building Functional Networks of Spiking Model Neurons},
  volume = {19},
  copyright = {\textcopyright{} 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  doi = {10.1038/nn.4241},
  abstract = {Most of the networks used by computer scientists and many of those studied by modelers in neuroscience represent unit activities as continuous variables. Neurons, however, communicate primarily through discontinuous spiking. We review methods for transferring our ability to construct interesting networks that perform relevant tasks from the artificial continuous domain to more realistic spiking network models. These methods raise a number of issues that warrant further theoretical and experimental study.
View full text},
  language = {en},
  number = {3},
  journal = {Nature Neuroscience},
  author = {Abbott, L. F. and DePasquale, Brian and Memmesheimer, Raoul-Martin},
  month = mar,
  year = {2016},
  keywords = {Network models,Neural encoding},
  pages = {350-355},
  file = {articles/Abbott2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/2VHXPHD8/nn.4241.html}
}

@article{Hindy2016,
  title = {Linking Pattern Completion in the Hippocampus to Predictive Coding in Visual Cortex},
  volume = {19},
  copyright = {\textcopyright{} 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  doi = {10.1038/nn.4284},
  abstract = {Models of predictive coding frame perception as a generative process in which expectations constrain sensory representations. These models account for expectations about how a stimulus will move or change from moment to moment, but do not address expectations about what other, distinct stimuli are likely to appear based on prior experience. We show that such memory-based expectations in human visual cortex are related to the hippocampal mechanism of pattern completion.},
  language = {en},
  number = {5},
  journal = {Nature Neuroscience},
  author = {Hindy, Nicholas C. and Ng, Felicia Y. and {Turk-Browne}, Nicholas B.},
  month = may,
  year = {2016},
  keywords = {Learning and memory,Visual system,Psychology},
  pages = {665-667},
  file = {articles/Hindy2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/MS58RMK8/nn.4284.html}
}

@article{Tetzlaff2015,
  title = {The {{Use}} of {{Hebbian Cell Assemblies}} for {{Nonlinear Computation}}},
  volume = {5},
  copyright = {\textcopyright{} 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  issn = {2045-2322},
  doi = {10.1038/srep12866},
  abstract = {When learning a complex task our nervous system self-organizes large groups of neurons into coherent dynamic activity patterns. During this, a network with multiple, simultaneously active, and computationally powerful cell assemblies is created.},
  language = {en},
  journal = {Scientific Reports},
  author = {Tetzlaff, Christian and Dasgupta, Sakyasingha and Kulvicius, Tomas and W\"org\"otter, Florentin},
  month = aug,
  year = {2015},
  pages = {12866},
  file = {articles/Tetzlaff2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/Z4EI6QIQ/srep12866.html}
}

@article{Litwin-Kumar2014,
  title = {Formation and Maintenance of Neuronal Assemblies through Synaptic Plasticity},
  volume = {5},
  copyright = {\textcopyright{} 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  issn = {2041-1723},
  doi = {10.1038/ncomms6319},
  abstract = {Connectivity patterns between neurons in the brain store recent sensory experiences, but how these patterns form is unclear. Here, the authors provide a model describing the process through which synaptic plasticity combined with homeostatic mechanisms allow stable neuronal assemblies to form.},
  language = {en},
  journal = {Nature Communications},
  author = {{Litwin-Kumar}, Ashok and Doiron, Brent},
  month = nov,
  year = {2014},
  pages = {5319},
  file = {articles/Litwin-Kumar2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/ZDEEGBCE/ncomms6319.html}
}

@article{Suvrathan2016,
  title = {Timing {{Rules}} for {{Synaptic Plasticity Matched}} to {{Behavioral Function}}},
  volume = {92},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.10.022},
  language = {en},
  number = {5},
  journal = {Neuron},
  author = {Suvrathan, Aparna and Payne, Hannah L. and Raymond, Jennifer L.},
  month = dec,
  year = {2016},
  pages = {959-967},
  file = {articles/Suvrathan2016.pdf}
}

@article{Landau2016,
  title = {The {{Impact}} of {{Structural Heterogeneity}} on {{Excitation}}-{{Inhibition Balance}} in {{Cortical Networks}}},
  volume = {92},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.10.027},
  language = {en},
  number = {5},
  journal = {Neuron},
  author = {Landau, Itamar D. and Egger, Robert and Dercksen, Vincent J. and Oberlaender, Marcel and Sompolinsky, Haim},
  month = dec,
  year = {2016},
  pages = {1106-1121},
  file = {articles/Landau2016.pdf}
}

@article{Rose2016,
  title = {Reactivation of Latent Working Memories with Transcranial Magnetic Stimulation},
  volume = {354},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aah7011},
  language = {en},
  number = {6316},
  journal = {Science},
  author = {Rose, N. S. and LaRocque, J. J. and Riggall, A. C. and Gosseries, O. and Starrett, M. J. and Meyering, E. E. and Postle, B. R.},
  month = dec,
  year = {2016},
  pages = {1136-1139},
  file = {articles/Rose2016.pdf}
}

@article{Vreeswijk1998,
  title = {Chaotic {{Balanced State}} in a {{Model}} of {{Cortical Circuits}}},
  volume = {10},
  issn = {0899-7667},
  doi = {10.1162/089976698300017214},
  abstract = {The nature and origin of the temporal irregularity in the electrical activity of cortical neurons in vivo are not well understood. We consider the hypothesis that this irregularity is due to a balance of excitatory and inhibitory currents into the cortical cells. We study a network model with excitatory and inhibitory populations of simple binary units. The internal feedback is mediated by relatively large synaptic strengths, so that the magnitude of the total excitatory and inhibitory feedback is much larger than the neuronal threshold. The connectivity is random and sparse. The mean number of connections per unit is large, though small compared to the total number of cells in the network. The network also receives a large, temporally regular input from external sources. We present an analytical solution of the mean-field theory of this model, which is exact in the limit of large network size. This theory reveals a new cooperative stationary state of large networks, which we term a balanced state. In this state, a balance between the excitatory and inhibitory inputs emerges dynamically for a wide range of parameters, resulting in a net input whose temporal fluctuations are of the same order as its mean. The internal synaptic inputs act as a strong negative feedback, which linearizes the population responses to the external drive despite the strong nonlinearity of the individual cells. This feedback also greatly stabilizes the system's state and enables it to track a time-dependent input on time scales much shorter than the time constant of a single cell. The spatiotemporal statistics of the balanced state are calculated. It is shown that the autocorrelations decay on a short time scale, yielding an approximate Poissonian temporal statistics. The activity levels of single cells are broadly distributed, and their distribution exhibits a skewed shape with a long power-law tail. The chaotic nature of the balanced state is revealed by showing that the evolution of the microscopic state of the network is extremely sensitive to small deviations in its initial conditions. The balanced state generated by the sparse, strong connections is an asynchronous chaotic state. It is accompanied by weak spatial cross-correlations, the strength of which vanishes in the limit of large network size. This is in contrast to the synchronized chaotic states exhibited by more conventional network models with high connectivity of weak synapses.},
  number = {6},
  journal = {Neural Computation},
  author = {van Vreeswijk, C. and Sompolinsky, H.},
  month = aug,
  year = {1998},
  pages = {1321-1371},
  file = {articles/Vreeswijk1998.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/VA2M5KKW/089976698300017214.html}
}

@article{Deneve2016,
  title = {Efficient Codes and Balanced Networks},
  volume = {19},
  copyright = {\textcopyright{} 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  doi = {10.1038/nn.4243},
  abstract = {Recent years have seen a growing interest in inhibitory interneurons and their circuits. A striking property of cortical inhibition is how tightly it balances excitation. Inhibitory currents not only match excitatory currents on average, but track them on a millisecond time scale, whether they are caused by external stimuli or spontaneous fluctuations. We review, together with experimental evidence, recent theoretical approaches that investigate the advantages of such tight balance for coding and computation. These studies suggest a possible revision of the dominant view that neurons represent information with firing rates corrupted by Poisson noise. Instead, tight excitatory/inhibitory balance may be a signature of a highly cooperative code, orders of magnitude more precise than a Poisson rate code. Moreover, tight balance may provide a template that allows cortical neurons to construct high-dimensional population codes and learn complex functions of their inputs.
View full text},
  language = {en},
  number = {3},
  journal = {Nature Neuroscience},
  author = {Den\`eve, Sophie and Machens, Christian K.},
  month = mar,
  year = {2016},
  keywords = {Network models,Neural circuits,Neural encoding},
  pages = {375-382},
  file = {articles/Denève2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/U4D63CNU/nn.4243.html}
}

@article{Rosenbaum2014,
  title = {Balanced {{Networks}} of {{Spiking Neurons}} with {{Spatially Dependent Recurrent Connections}}},
  volume = {4},
  doi = {10.1103/PhysRevX.4.021039},
  abstract = {Networks of model neurons with balanced recurrent excitation and inhibition capture the irregular and asynchronous spiking activity reported in cortex. While mean-field theories of spatially homogeneous balanced networks are well understood, a mean-field analysis of spatially heterogeneous balanced networks has not been fully developed. We extend the analysis of balanced networks to include a connection probability that depends on the spatial separation between neurons. In the continuum limit, we derive that stable, balanced firing rate solutions require that the spatial spread of external inputs be broader than that of recurrent excitation, which in turn must be broader than or equal to that of recurrent inhibition. Notably, this implies that network models with broad recurrent inhibition are inconsistent with the balanced state. For finite size networks, we investigate the pattern-forming dynamics arising when balanced conditions are not satisfied. Our study highlights the new challenges that balanced networks pose for the spatiotemporal dynamics of complex systems.},
  number = {2},
  journal = {Physical Review X},
  author = {Rosenbaum, Robert and Doiron, Brent},
  month = may,
  year = {2014},
  pages = {021039},
  file = {articles/Rosenbaum2014_2.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/SZKP5DBK/PhysRevX.4.html}
}

@article{Palatella2011,
  title = {Distribution of First-Return Times in Correlated Stationary Signals},
  volume = {83},
  doi = {10.1103/PhysRevE.83.041102},
  abstract = {We present an analytical expression for the first return time (FRT) probability density function of a stationary correlated signal. Precisely, we start by considering a stationary discrete-time Ornstein-Uhlenbeck (OU) process with exponential decaying correlation function. The first return time distribution for this process is derived by adopting a well-known formalism typically used in the study of the FRT statistics for nonstationary diffusive processes. Then, by a subordination approach, we treat the case of a stationary process with power-law tail correlation function and diverging correlation time. We numerically test our findings, obtaining in both cases a good agreement with the analytical results. We notice that neither in the standard OU nor in the subordinated case a simple form of waiting time statistics, like stretched-exponential or similar, can be obtained while it is apparent that long time transient may shadow the final asymptotic behavior.},
  number = {4},
  journal = {Physical Review E},
  author = {Palatella, Luigi and Pennetta, Cecilia},
  month = apr,
  year = {2011},
  pages = {041102},
  file = {articles/Palatella2011.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/BPZ7QAXG/PhysRevE.83.html}
}

@article{Touboul2010,
  title = {Can {{Power}}-{{Law Scaling}} and {{Neuronal Avalanches Arise}} from {{Stochastic Dynamics}}?},
  volume = {5},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0008982},
  abstract = {The presence of self-organized criticality in biology is often evidenced by a power-law scaling of event size distributions, which can be measured by linear regression on logarithmic axes. We show here that such a procedure does not necessarily mean that the system exhibits self-organized criticality. We first provide an analysis of multisite local field potential (LFP) recordings of brain activity and show that event size distributions defined as negative LFP peaks can be close to power-law distributions. However, this result is not robust to change in detection threshold, or when tested using more rigorous statistical analyses such as the Kolmogorov\textendash{}Smirnov test. Similar power-law scaling is observed for surrogate signals, suggesting that power-law scaling may be a generic property of thresholded stochastic processes. We next investigate this problem analytically, and show that, indeed, stochastic processes can produce spurious power-law scaling without the presence of underlying self-organized criticality. However, this power-law is only apparent in logarithmic representations, and does not survive more rigorous analysis such as the Kolmogorov\textendash{}Smirnov test. The same analysis was also performed on an artificial network known to display self-organized criticality. In this case, both the graphical representations and the rigorous statistical analysis reveal with no ambiguity that the avalanche size is distributed as a power-law. We conclude that logarithmic representations can lead to spurious power-law scaling induced by the stochastic nature of the phenomenon. This apparent power-law scaling does not constitute a proof of self-organized criticality, which should be demonstrated by more stringent statistical tests.},
  number = {2},
  journal = {PLOS ONE},
  author = {Touboul, Jonathan and Destexhe, Alain},
  month = feb,
  year = {2010},
  keywords = {Action potentials,Probability distribution,Stochastic processes,neural networks,Statistical distributions,Cats,Signal filtering,Statistical data},
  pages = {e8982},
  file = {articles/Touboul2010.pdf}
}

@book{Gillespie1992,
  address = {Boston},
  title = {Markov {{Processes}}: {{An Introduction}} for {{Physical Scientists}}},
  isbn = {978-0-12-283955-9},
  lccn = {QA274.7 .G55 1992},
  shorttitle = {Markov Processes},
  publisher = {{Academic Press}},
  author = {Gillespie, Daniel T.},
  year = {1992},
  keywords = {Markov processes},
  file = {books/Gillespie1992_Markov-Processes-An-Introduction-for-Physical-Scientists.pdf}
}

@article{Koren2007,
  title = {Leapover {{Lengths}} and {{First Passage Time Statistics}} for {{L}}$\backslash$'evy {{Flights}}},
  volume = {99},
  doi = {10.1103/PhysRevLett.99.160602},
  abstract = {Exact results for the first passage time and leapover statistics of symmetric and one-sided L\'evy flights (LFs) are derived. LFs with a stable index $\alpha$ are shown to have leapover lengths that are asymptotically power law distributed with an index $\alpha$ for one-sided LFs and, surprisingly, with an index $\alpha$/2 for symmetric LFs. The first passage time distribution scales like a power law with an index 1/2 as required by the Sparre-Andersen theorem for symmetric LFs, whereas one-sided LFs have a narrow distribution of first passage times. The exact analytic results are confirmed by extensive simulations.},
  number = {16},
  journal = {Physical Review Letters},
  author = {Koren, Tal and Lomholt, Michael A. and Chechkin, Aleksei V. and Klafter, Joseph and Metzler, Ralf},
  month = oct,
  year = {2007},
  pages = {160602},
  file = {articles/Koren2007.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/9GV59NQX/PhysRevLett.99.html}
}

@article{Nyberg2016,
  title = {A Simple Method to Calculate First-Passage Time Densities with Arbitrary Initial Conditions},
  volume = {18},
  issn = {1367-2630},
  doi = {10.1088/1367-2630/18/6/063019},
  abstract = {Numerous applications all the way from biology and physics to economics depend on the density of first crossings over a boundary. Motivated by the lack of general purpose analytical tools for computing first-passage time densities (FPTDs) for complex problems, we propose a new simple method based on the independent interval approximation (IIA). We generalise previous formulations of the IIA to include arbitrary initial conditions as well as to deal with discrete time and non-smooth continuous time processes. We derive a closed form expression for the FPTD in z and Laplace-transform space to a boundary in one dimension. Two classes of problems are analysed in detail: discrete time symmetric random walks (Markovian) and continuous time Gaussian stationary processes (Markovian and non-Markovian). Our results are in good agreement with Langevin dynamics simulations.},
  language = {en},
  number = {6},
  journal = {New Journal of Physics},
  author = {Nyberg, Markus and Ambj\"ornsson, Tobias and Lizana, Ludvig},
  year = {2016},
  pages = {063019},
  file = {articles/Nyberg2016.pdf}
}

@article{Domine1996,
  title = {First {{Passage Time Distribution}} of a {{Wiener Process}} with {{Drift}} Concerning {{Two Elastic Barriers}}},
  volume = {33},
  issn = {0021-9002},
  doi = {10.2307/3215274},
  abstract = {We solve the Fokker-Planck equation for the Wiener process with drift in the presence of elastic boundaries and a fixed start point. An explicit expression is obtained for the first passage density. The cases with pure absorbing and/or reflecting barriers arise for a special choice of a parameter constellation. These special cases are compared with results in Darling and Siegert [5] and Sweet and Hardin [15].},
  number = {1},
  journal = {Journal of Applied Probability},
  author = {Domin\'e, Marco},
  year = {1996},
  pages = {164-175},
  file = {articles/Dominé1996.pdf}
}

@article{Molini2011,
  title = {First Passage Time Statistics of {{Brownian}} Motion with Purely Time Dependent Drift and Diffusion},
  volume = {390},
  issn = {0378-4371},
  doi = {10.1016/j.physa.2011.01.024},
  abstract = {Systems where resource availability approaches a critical threshold are common to many engineering and scientific applications and often necessitate the estimation of first passage time statistics of a Brownian motion (Bm) driven by time-dependent drift and diffusion coefficients. Modeling such systems requires solving the associated Fokker-Planck equation subject to an absorbing barrier. Transitional probabilities are derived via the method of images, whose applicability to time dependent problems is shown to be limited to state-independent drift and diffusion coefficients that only depend on time and are proportional to each other. First passage time statistics, such as the survival probabilities and first passage time densities are obtained analytically. The analysis includes the study of different functional forms of the time dependent drift and diffusion, including power-law time dependence and different periodic drivers. As a case study of these theoretical results, a stochastic model of water resources availability in snowmelt dominated regions is presented, where both temperature effects and snow-precipitation input are incorporated.},
  number = {11},
  journal = {Physica A: Statistical Mechanics and its Applications},
  author = {Molini, A. and Talkner, P. and Katul, G. G. and Porporato, A.},
  month = jun,
  year = {2011},
  keywords = {Absorbing barrier,Brownian motion,Snowmelt,Time-dependent drift and diffusion},
  pages = {1841-1852},
  file = {articles/Molini2011.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/GAT8UZ48/S0378437111000884.html}
}

@article{deVivo2017,
  title = {Ultrastructural Evidence for Synaptic Scaling across the Wake/Sleep Cycle},
  volume = {355},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aah5982},
  language = {en},
  number = {6324},
  journal = {Science},
  author = {{de Vivo}, Luisa and Bellesi, Michele and Marshall, William and Bushong, Eric A. and Ellisman, Mark H. and Tononi, Giulio and Cirelli, Chiara},
  month = feb,
  year = {2017},
  pages = {507-510},
  file = {articles/de Vivo2017.pdf}
}

@article{Fujimoto2010,
  title = {A Multiplicative Stochastic Process Deriving the Probability Distribution in Exact Form},
  volume = {221},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/221/1/012008},
  journal = {Journal of Physics: Conference Series},
  author = {Fujimoto, Shouji and Ishikawa, Atushi and Tomoyose, Masashi},
  month = apr,
  year = {2010},
  pages = {012008},
  file = {articles/Fujimoto2010.pdf}
}

@article{Ibata2008,
  title = {Rapid {{Synaptic Scaling Induced}} by {{Changes}} in {{Postsynaptic Firing}}},
  volume = {57},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2008.02.031},
  abstract = {Summary
Homeostatic synaptic scaling adjusts a neuron's excitatory synaptic strengths up or down to compensate for perturbations in activity. Little is known about the molecular pathway(s) involved, nor is it clear which aspect of ``activity''\textemdash{}local synaptic signaling, postsynaptic firing, or large-scale changes in network activity\textemdash{}is required to induce synaptic scaling. Here, we selectively block either postsynaptic firing in individual neurons or a fraction of presynaptic inputs, while optically monitoring changes in synaptic strength. We find that synaptic scaling is rapidly induced by block of postsynaptic firing, but not by local synaptic blockade, and is mediated through a drop in somatic calcium influx, reduced activation of CaMKIV, and an increase in transcription. Cortical neurons thus homeostatically adjust synaptic strengths in response to changes in their own firing rate, a mechanism with the computational advantage of efficiently normalizing synaptic strengths without interfering with synapse-specific mechanisms of information storage.},
  number = {6},
  journal = {Neuron},
  author = {Ibata, Keiji and Sun, Qian and Turrigiano, Gina G.},
  month = mar,
  year = {2008},
  keywords = {CELLBIO,SIGNALING,MOLENURO},
  pages = {819-826},
  file = {articles/Ibata2008.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/2QXHE9DR/S0896627308002134.html}
}

@article{Clauset2009a,
  title = {Power-{{Law Distributions}} in {{Empirical Data}}},
  volume = {51},
  issn = {0036-1445},
  doi = {10.1137/070710111},
  abstract = {Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution\textemdash{}the part of the distribution representing large but rare events\textemdash{}and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov\textendash{}Smirnov (KS) statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data, while in others the power law is ruled out.},
  number = {4},
  journal = {SIAM Review},
  author = {Clauset, A. and Shalizi, C. and Newman, M.},
  month = nov,
  year = {2009},
  pages = {661-703},
  file = {articles/Clauset2009.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/NETDNTV4/070710111.html}
}

@article{Hoffmann2017,
  title = {Nonrandom Network Connectivity Comes in Pairs},
  volume = {1},
  doi = {10.1162/NETN_a_00004},
  abstract = {Overrepresentation of bidirectional connections in local cortical networks has been repeatedly reported and is a focus of the ongoing discussion of nonrandom connectivity. Here we show in a brief mathematical analysis that in a network in which connection probabilities are symmetric in pairs, Pij = Pji, the occurrences of bidirectional connections and nonrandom structures are inherently linked; an overabundance of reciprocally connected pairs emerges necessarily when some pairs of neurons are more likely to be connected than others. Our numerical results imply that such overrepresentation can also be sustained when connection probabilities are only approximately symmetric.},
  number = {1},
  journal = {Network Neuroscience},
  author = {Hoffmann, Felix Z. and Triesch, Jochen},
  month = jan,
  year = {2017},
  pages = {31-41},
  file = {articles/Hoffmann2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/XHXVZN82/NETN_a_00004.html}
}

@article{Radford2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.01444},
  primaryClass = {cs},
  title = {Learning to {{Generate Reviews}} and {{Discovering Sentiment}}},
  abstract = {We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.},
  journal = {arXiv:1704.01444 [cs]},
  author = {Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya},
  month = apr,
  year = {2017},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {articles/Radford2017.pdf}
}

@article{Weber2017,
  title = {Learning Place Cells, Grid Cells and Invariances: {{A}} Unifying Model},
  copyright = {\textcopyright{} 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  shorttitle = {Learning Place Cells, Grid Cells and Invariances},
  doi = {10.1101/102525},
  abstract = {Neurons in the hippocampus and adjacent brain areas show a large diversity in their tuning to location and head direction. The underlying circuit mechanisms are not fully resolved. In particular, it is unclear why certain cell types are selective to one spatial variable, but invariant to another. For example, a place cell is highly selective to location, but typically invariant to head direction. Here, we propose that all observed spatial tuning patterns -- in both their selectivity and their invariance -- are a consequence of the same mechanism: Excitatory and inhibitory synaptic plasticity that is driven by the spatial tuning statistics of synaptic inputs. Using simulations and a mathematical analysis, we show that combined excitatory and inhibitory plasticity can lead to localized, grid-like or invariant activity. Combinations of different input statistics along different spatial dimensions reproduce all major spatial tuning patterns observed in rodents. The model is robust to changes in parameters, develops patterns on behavioral time scales and makes distinctive experimental predictions. Our results suggest that the interaction of excitatory and inhibitory plasticity is a general principle for the formation of neural representations.},
  language = {en},
  journal = {bioRxiv},
  author = {Weber, Simon N. and Sprekeler, Henning},
  month = feb,
  year = {2017},
  pages = {102525},
  file = {articles/Weber2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/KTZSTZ9B/102525.html}
}

@article{Ocker2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.03132},
  primaryClass = {q-bio},
  title = {From the Statistics of Connectivity to the Statistics of Spike Times in Neuronal Networks},
  abstract = {An essential step toward understanding neural circuits is linking their structure and their dynamics. In general, this relationship can be almost arbitrarily complex. Recent theoretical work has, however, begun to identify some broad principles underlying collective spiking activity in neural circuits. The first is that local features of network connectivity can be surprisingly effective in predicting global statistics of activity across a network. The second is that, for the important case of large networks with excitatory-inhibitory balance, correlated spiking persists or vanishes depending on the spatial scales of recurrent and feedforward connectivity. We close by showing how these ideas, together with plasticity rules, can help to close the loop between network structure and activity statistics.},
  journal = {arXiv:1703.03132 [q-bio]},
  author = {Ocker, Gabriel Koch and Hu, Yu and Buice, Michael A. and Doiron, Brent and Josi\'c, Kre{\v s}imir and Rosenbaum, Robert and {Shea-Brown}, Eric},
  month = mar,
  year = {2017},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {articles/Ocker2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/DH3VCRC7/1703.html}
}

@article{Sadeh2017,
  title = {Assessing the Role of Inhibition in Stabilizing Neocortical Networks Requires Large-Scale Perturbation of the Inhibitory Population},
  copyright = {\textcopyright{} 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  doi = {10.1101/123950},
  abstract = {Neurons within cortical microcircuits are densely interconnected by recurrent excitatory synaptic connections that are thought to amplify signals (Douglas and Martin, 2007), form selective subnetworks (Ko et al., 2011) and aid feature discrimination. Strong inhibition (Haider et al., 2013) counterbalances excitation, enabling sensory features to be sharpened and represented by sparse codes (Willmore et al., 2011). This "balance" of excitation and inhibition has made it difficult to assess the strength, or gain, of the recurrent excitatory connections within cortical networks, which is key to understanding their operational regime and the computations they can perform. Networks of neurons that combine an unstable high-gain excitatory population with stabilising inhibitory feedback are known as inhibition-stabilized networks (ISNs; Tsodyks et al. 1997). Classical theoretical studies using reduced network models predict that ISNs produce paradoxical responses to perturbation, but in vivo optogenetic perturbations have failed to find evidence for ISNs in cortex (Atallah et al., 2012). We re-examined this question by investigating how rate-based and spiking cortical network models consisting of many neurons behave following perturbations, and found that results obtained from reduced network models fail to predict the effects of perturbations in more realistic networks. Our cortical network models predict that a large proportion of the inhibitory network must be perturbed in order to robustly detect an ISN regime in cortex. We propose that optogenetic suppression of inhibition under a promoter targeting all inhibitory neurons, coupled with wide-field illumination, may provide a sufficient perturbation to reveal the operating regime of cortex. Our results suggest that detailed computational models of optogenetic perturbations are necessary to interpret the results of experimental paradigms.},
  language = {en},
  journal = {bioRxiv},
  author = {Sadeh, Sadra and Silver, Robin Angus and {Mrsic-Flogel}, Thomas and Muir, Dylan},
  month = apr,
  year = {2017},
  pages = {123950},
  file = {articles/Sadeh2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/6A2S8FVW/123950.html}
}

@article{Lim2013,
  title = {Balanced Cortical Microcircuitry for Maintaining Information in Working Memory},
  volume = {16},
  copyright = {\textcopyright{} 2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  doi = {10.1038/nn.3492},
  abstract = {Persistent neural activity in the absence of a stimulus has been identified as a neural correlate of working memory, but how such activity is maintained by neocortical circuits remains unknown. We used a computational approach to show that the inhibitory and excitatory microcircuitry of neocortical memory-storing regions is sufficient to implement a corrective feedback mechanism that enables persistent activity to be maintained stably for prolonged durations. When recurrent excitatory and inhibitory inputs to memory neurons were balanced in strength and offset in time, drifts in activity triggered a corrective signal that counteracted memory decay. Circuits containing this mechanism temporally integrated their inputs, generated the irregular neural firing observed during persistent activity and were robust against common perturbations that severely disrupted previous models of short-term memory storage. These results reveal a mechanism for the accumulation and storage of memories in neocortical circuits based on principles of corrective negative feedback that are widely used in engineering applications.
View full text},
  language = {en},
  number = {9},
  journal = {Nature Neuroscience},
  author = {Lim, Sukbin and Goldman, Mark S.},
  month = sep,
  year = {2013},
  keywords = {Network models},
  pages = {1306-1314},
  file = {articles/Lim2013_2.pdf;articles/Lim2013.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/V3NGX7GD/nn.3492.html}
}

@article{Nykamp2017,
  title = {Mean-{{Field Equations For Neuronal Networks With Arbitrary Degree Distributions}}},
  copyright = {\textcopyright{} 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  doi = {10.1101/118463},
  abstract = {The emergent dynamics in networks of recurrently coupled spiking neurons depends on the interplay between single-cell dynamics and network topology. Most theoretical studies on network dynamics have assumed simple topologies, such as connections which are made randomly and independently with a fixed probability (Erdos-Renyi network) (ER), or all-to-all connected networks. However, recent findings from slice experiments suggest that the actual patterns of connectivity between cortical neurons are more structured than in the ER random network. Here we explore how introducing additional higher-order statistical structure into the connectivity can affect the dynamics in neuronal networks. Specifically, we consider networks in which the number of pre-synaptic and post-synaptic contacts for each neuron, the degrees, are drawn from a joint degree distribution. We derive mean-field equations for a single population of homogeneous neurons and for a network of excitatory and inhibitory neurons, where the neurons can have arbitrary degree distributions. Through analysis of the mean-field equations and simulation of networks of integrate-and-fire neurons, we show that such networks have potentially much richer dynamics than an equivalent ER network. Finally, we relate the degree distributions to so-called cortical motifs.},
  language = {en},
  journal = {bioRxiv},
  author = {Nykamp, Duane and Friedman, Daniel and Shaker, Sammy and Shinn, Maxwell and Vella, Michael and Compte, Albert and Roxin, Alex},
  month = mar,
  year = {2017},
  pages = {118463},
  file = {articles/Nykamp2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/KWHF85C6/118463.html}
}

@article{Litwin-Kumar2017,
  title = {Optimal {{Degrees}} of {{Synaptic Connectivity}}},
  volume = {93},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2017.01.030},
  abstract = {Summary
Synaptic connectivity varies widely across neuronal types. Cerebellar granule cells receive five orders of magnitude fewer inputs than the Purkinje cells they innervate, and cerebellum-like circuits, including the insect mushroom body, also exhibit large divergences in connectivity. In contrast, the number of inputs per neuron in cerebral cortex is more uniform and large. We investigate how the dimension of a representation formed by a population of neurons depends on how many inputs each neuron receives and what this implies for learning associations. Our theory predicts that the dimensions of the cerebellar granule-cell and Drosophila Kenyon-cell representations are maximized at degrees of synaptic connectivity that match those observed anatomically, showing that sparse connectivity is sometimes superior to dense connectivity. When input synapses are subject to supervised plasticity, however, dense wiring becomes advantageous, suggesting that the type of plasticity exhibited by a set of synapses is a major determinant of connection density.},
  number = {5},
  journal = {Neuron},
  author = {{Litwin-Kumar}, Ashok and Harris, Kameron Decker and Axel, Richard and Sompolinsky, Haim and Abbott, L. F.},
  month = mar,
  year = {2017},
  pages = {1153-1164.e7},
  file = {articles/Litwin-Kumar2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/CSQSB94F/S0896627317300545.html}
}

@article{Rumpel2016,
  title = {The Dynamic Connectome},
  volume = {7},
  issn = {1868-856X},
  doi = {10.1007/s13295-016-0026-2},
  abstract = {When trying to gain intuitions about the computations implemented in neural circuits, we often use comparisons with electronic circuits. However, one fundamental difference to hard-wired electronic circuits is that the structure of neural circuits undergoes constant remodeling. Here, we discuss recent findings highlighting the dynamic nature of neural circuits and the underlying mechanisms. The dynamics of neural circuits follows rules that explain steady state statistics of synaptic properties observed at a single time point. Interestingly, these rules allow the prediction of future network states and extend the insights gained from serial sectioning electron microscopy of brain samples, which inherently provides information from only a single time point. We argue how the connectome's dynamic nature can be reconciled with stable functioning and long-term memory storage and how it may even benefit learning.},
  language = {en},
  number = {3},
  journal = {e-Neuroforum},
  author = {Rumpel, Simon and Triesch, Jochen},
  month = sep,
  year = {2016},
  pages = {48-53},
  file = {articles/Rumpel2016_2.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/89DVA4W6/s13295-016-0026-2.html}
}

@misc{Miner2017a,
  title = {[{{Draft}}] {{The Slope}} of {{Distributions}} of {{Synaptic Lifetimes}} Can {{Serve}} as a {{Proxy Measure}} for the {{Balance Between Potentiation}} and {{Depression}} in a {{Cortical Networks}}},
  author = {Miner, Daniel},
  year = {2017},
  file = {documents/Miner2017.pdf}
}

@article{Wang2017,
  title = {Structure and Dynamics of Self-Organized Neuronal Network with an Improved {{STDP}} Rule},
  issn = {0924-090X, 1573-269X},
  doi = {10.1007/s11071-017-3348-x},
  abstract = {The chemical synapses in a neural network are known to be modulated by the neuronal firing activities through the spike-timing-dependent plasticity (STDP) rule. In this paper, we improve the multiplicative STDP rule by adding a momentum item with the aim of overcoming the low rate with which the neuronal network self-organizes into a stable complex structure. We find that the improved STDP rule with suitable momentum factors significantly speeds up the evolutionary process of the self-organized neuronal network. In addition, we explore the topological structure of self-organized neuronal network using complex network method. We show that the improved STDP rule generally results in a smaller node degree, clustering coefficient and modularity of self-organized neuronal network. Furthermore, we investigate the dynamical behaviors of self-organized neuronal network. We observe that depending on the momentum factor, the improved STDP rule has different effects on the network synchronization, neural information transmission, modularity and network complexity. Remarkably, for a specific momentum factor, the self-organized neuronal network shows the highest global efficiency of information transmission and the best combination between functional segregation and integration, which reflects the optimal dynamics as well as the topological structure. Our results provide a reasonable and efficient modulating rule of chemical synapse underlying the neuronal firing activities.},
  language = {en},
  journal = {Nonlinear Dynamics},
  author = {Wang, Rong and Wu, Ying and Wang, Li and Du, Mengmeng and Li, Jiajia},
  month = feb,
  year = {2017},
  pages = {1-14},
  file = {articles/Wang2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/MWCD6IPP/10.html}
}

@article{Bengio2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.04156},
  primaryClass = {cs},
  title = {Towards {{Biologically Plausible Deep Learning}}},
  abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
  journal = {arXiv:1502.04156 [cs]},
  author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Learning},
  file = {articles/Bengio2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/FXQH9T62/1502.html}
}

@article{Lee2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.7525},
  primaryClass = {cs},
  title = {Difference {{Target Propagation}}},
  abstract = {Back-propagation has been the workhorse of recent successes of deep learning but it relies on infinitesimal effects (partial derivatives) in order to perform credit assignment. This could become a serious issue as one considers deeper and more non-linear functions, e.g., consider the extreme case of nonlinearity where the relation between parameters and cost is actually discrete. Inspired by the biological implausibility of back-propagation, a few approaches have been proposed in the past that could play a similar credit assignment role. In this spirit, we explore a novel approach to credit assignment in deep networks that we call target propagation. The main idea is to compute targets rather than gradients, at each layer. Like gradients, they are propagated backwards. In a way that is related but different from previously proposed proxies for back-propagation which rely on a backwards network with symmetric weights, target propagation relies on auto-encoders at each layer. Unlike back-propagation, it can be applied even when units exchange stochastic bits rather than real numbers. We show that a linear correction for the imperfectness of the auto-encoders, called difference target propagation, is very effective to make target propagation actually work, leading to results comparable to back-propagation for deep networks with discrete and continuous units and denoising auto-encoders and achieving state of the art for stochastic networks.},
  journal = {arXiv:1412.7525 [cs]},
  author = {Lee, Dong-Hyun and Zhang, Saizheng and Fischer, Asja and Bengio, Yoshua},
  month = dec,
  year = {2014},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {articles/Lee2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/F7DTGFH4/1412.html}
}

@article{Keck2017,
  title = {Integrating {{Hebbian}} and Homeostatic Plasticity: The Current State of the Field and Future Research Directions},
  volume = {372},
  issn = {0962-8436, 1471-2970},
  shorttitle = {Integrating {{Hebbian}} and Homeostatic Plasticity},
  doi = {10.1098/rstb.2016.0158},
  language = {en},
  number = {1715},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  author = {Keck, Tara and Toyoizumi, Taro and Chen, Lu and Doiron, Brent and Feldman, Daniel E. and Fox, Kevin and Gerstner, Wulfram and Haydon, Philip G. and H\"ubener, Mark and Lee, Hey-Kyoung and Lisman, John E. and Rose, Tobias and Sengpiel, Frank and Stellwagen, David and Stryker, Michael P. and Turrigiano, Gina G. and {van Rossum}, Mark C.},
  month = mar,
  year = {2017},
  pages = {20160158},
  file = {articles/Keck2017.pdf}
}

@article{Koch2016,
  title = {Neural Correlates of Consciousness: Progress and Problems},
  volume = {17},
  copyright = {\textcopyright{} 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1471-003X},
  shorttitle = {Neural Correlates of Consciousness},
  doi = {10.1038/nrn.2016.22},
  abstract = {There have been a number of advances in the search for the neural correlates of consciousness \textemdash{} the minimum neural mechanisms sufficient for any one specific conscious percept. In this Review, we describe recent findings showing that the anatomical neural correlates of consciousness are primarily localized to a posterior cortical hot zone that includes sensory areas, rather than to a fronto-parietal network involved in task monitoring and reporting. We also discuss some candidate neurophysiological markers of consciousness that have proved illusory, and measures of differentiation and integration of neural activity that offer more promising quantitative indices of consciousness.
View full text},
  language = {en},
  number = {5},
  journal = {Nature Reviews Neuroscience},
  author = {Koch, Christof and Massimini, Marcello and Boly, Melanie and Tononi, Giulio},
  month = may,
  year = {2016},
  keywords = {Consciousness,Disorders of consciousness},
  pages = {307-321},
  file = {articles/Koch2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/AQV52NAF/nrn.2016.22.html}
}

@article{Toyoizumi2014,
  title = {Modeling the {{Dynamic Interaction}} of {{Hebbian}} and {{Homeostatic Plasticity}}},
  volume = {84},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2014.09.036},
  abstract = {Summary
Hebbian and homeostatic plasticity together refine neural circuitry, but their interactions are unclear. In most existing models, each form of plasticity directly modifies synaptic strength. Equilibrium is reached when the two are inducing equal and opposite changes. We show that such models cannot reproduce ocular dominance plasticity (ODP) because negative feedback from the slow homeostatic plasticity observed in ODP cannot stabilize the positive feedback of fast Hebbian plasticity. We propose a model in which synaptic strength is the product of a synapse-specific Hebbian factor and a postsynaptic-cell-specific homeostatic factor, with each factor separately arriving at a stable inactive state. This model captures ODP dynamics and has plausible biophysical substrates. We confirm model predictions experimentally that plasticity is inactive at stable states and that synaptic strength overshoots during recovery from visual deprivation. These results highlight the importance of multiple regulatory pathways for interactions of plasticity mechanisms operating over separate timescales.},
  number = {2},
  journal = {Neuron},
  author = {Toyoizumi, Taro and Kaneko, Megumi and Stryker, Michael P. and Miller, Kenneth D.},
  month = oct,
  year = {2014},
  pages = {497-510},
  file = {articles/Toyoizumi2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/QWEG8U5C/S0896627314008940.html}
}

@article{Alstott2014a,
  title = {Powerlaw: {{A Python Package}} for {{Analysis}} of {{Heavy}}-{{Tailed Distributions}}},
  volume = {9},
  issn = {1932-6203},
  shorttitle = {Powerlaw},
  doi = {10.1371/journal.pone.0085777},
  abstract = {Power laws are theoretically interesting probability distributions that are also frequently used to describe empirical data. In recent years, effective statistical methods for fitting power laws have been developed, but appropriate use of these techniques requires significant programming and statistical insight. In order to greatly decrease the barriers to using good statistical methods for fitting power law distributions, we developed the powerlaw Python package. This software package provides easy commands for basic fitting and statistical analysis of distributions. Notably, it also seeks to support a variety of user needs by being exhaustive in the options available to the user. The source code is publicly available and easily extensible.},
  number = {1},
  journal = {PLOS ONE},
  author = {Alstott, Jeff and Bullmore, Ed and Plenz, Dietmar},
  month = jan,
  year = {2014},
  keywords = {Neurons,Probability distribution,Simulation and Modeling,Statistical distributions,Data visualization,Probability density,Source code,Statistical methods},
  pages = {e85777},
  file = {articles/Alstott2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/IU3P2VTP/article.html}
}

@article{Burkitt2004,
  title = {Spike-{{Timing}}-{{Dependent Plasticity}}: {{The Relationship}} to {{Rate}}-{{Based Learning}} for {{Models}} with {{Weight Dynamics Determined}} by a {{Stable Fixed Point}}},
  volume = {16},
  issn = {0899-7667},
  shorttitle = {Spike-{{Timing}}-{{Dependent Plasticity}}},
  doi = {10.1162/089976604773135041},
  number = {5},
  journal = {Neural Computation},
  author = {Burkitt, Anthony N. and Meffin, Hamish and Grayden, David. B.},
  month = may,
  year = {2004},
  pages = {885-940},
  file = {articles/Burkitt2004.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/CG9Q4WI9/089976604773135041.html}
}

@article{Costa2017a,
  title = {Functional Consequences of Pre- and Postsynaptic Expression of Synaptic Plasticity},
  volume = {372},
  copyright = {\textcopyright{} 2017 The Authors.. Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original author and source are credited.},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2016.0153},
  abstract = {Growing experimental evidence shows that both homeostatic and Hebbian synaptic plasticity can be expressed presynaptically as well as postsynaptically. In this review, we start by discussing this evidence and methods used to determine expression loci. Next, we discuss the functional consequences of this diversity in pre- and postsynaptic expression of both homeostatic and Hebbian synaptic plasticity. In particular, we explore the functional consequences of a biologically tuned model of pre- and postsynaptically expressed spike-timing-dependent plasticity complemented with postsynaptic homeostatic control. The pre- and postsynaptic expression in this model predicts (i) more reliable receptive fields and sensory perception, (ii) rapid recovery of forgotten information (memory savings), and (iii) reduced response latencies, compared with a model with postsynaptic expression only. Finally, we discuss open questions that will require a considerable research effort to better elucidate how the specific locus of expression of homeostatic and Hebbian plasticity alters synaptic and network computations.
This article is part of the themed issue `Integrating Hebbian and homeostatic plasticity'.},
  language = {en},
  number = {1715},
  journal = {Phil. Trans. R. Soc. B},
  author = {Costa, Rui Ponte and Mizusaki, Beatriz E. P. and Sj\"ostr\"om, P. Jesper and van Rossum, Mark C. W.},
  month = mar,
  year = {2017},
  pages = {20160153},
  file = {articles/Costa2017.pdf},
  pmid = {28093547}
}

@article{Rossum2000,
  title = {Stable {{Hebbian Learning}} from {{Spike Timing}}-{{Dependent Plasticity}}},
  volume = {20},
  copyright = {Copyright \textcopyright{} 2000 Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  abstract = {We explore a synaptic plasticity model that incorporates recent findings that potentiation and depression can be induced by precisely timed pairs of synaptic events and postsynaptic spikes. In addition we include the observation that strong synapses undergo relatively less potentiation than weak synapses, whereas depression is independent of synaptic strength. After random stimulation, the synaptic weights reach an equilibrium distribution which is stable, unimodal, and has positive skew. This weight distribution compares favorably to the distributions of quantal amplitudes and of receptor number observed experimentally in central neurons and contrasts to the distribution found in plasticity models without size-dependent potentiation. Also in contrast to those models, which show strong competition between the synapses, stable plasticity is achieved with little competition. Instead, competition can be introduced by including a separate mechanism that scales synaptic strengths multiplicatively as a function of postsynaptic activity. In this model, synaptic weights change in proportion to how correlated they are with other inputs onto the same postsynaptic neuron. These results indicate that stable correlation-based plasticity can be achieved without introducing competition, suggesting that plasticity and competition need not coexist in all circuits or at all developmental stages.},
  language = {en},
  number = {23},
  journal = {Journal of Neuroscience},
  author = {van Rossum, M. C. W. and Bi, G. Q. and Turrigiano, G. G.},
  month = dec,
  year = {2000},
  keywords = {Hebbian plasticity,activity-dependent scaling,stochastic approaches,synaptic competition,synaptic weights,temporal learning},
  pages = {8812-8821},
  file = {articles/Rossum2000.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/65VHRF9H/8812.html},
  pmid = {11102489}
}

@article{Gilson2011,
  title = {Stability versus {{Neuronal Specialization}} for {{STDP}}: {{Long}}-{{Tail Weight Distributions Solve}} the {{Dilemma}}},
  volume = {6},
  issn = {1932-6203},
  shorttitle = {Stability versus {{Neuronal Specialization}} for {{STDP}}},
  doi = {10.1371/journal.pone.0025339},
  language = {en},
  number = {10},
  journal = {PLoS ONE},
  author = {Gilson, Matthieu and Fukai, Tomoki},
  editor = {Mansvelder, Huibert D.},
  month = oct,
  year = {2011},
  pages = {e25339},
  file = {articles/Gilson2011.PDF}
}

@article{Miller1994,
  title = {The {{Role}} of {{Constraints}} in {{Hebbian Learning}}},
  volume = {6},
  issn = {0899-7667},
  doi = {10.1162/neco.1994.6.1.100},
  number = {1},
  journal = {Neural Computation},
  author = {Miller, Kenneth D. and MacKay, David J. C.},
  month = jan,
  year = {1994},
  pages = {100-126},
  file = {articles/Miller1994.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/WMRME25H/neco.1994.6.1.html}
}

@article{Weigand2017,
  title = {Universal Transition from Unstructured to Structured Neural Maps},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1616163114},
  language = {en},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Weigand, Marvin and Sartori, Fabio and Cuntz, Hermann},
  month = may,
  year = {2017},
  pages = {201616163},
  file = {articles/Weigand2017.pdf}
}

@article{Fasoli2016,
  title = {The {{Complexity}} of {{Dynamics}} in {{Small Neural Circuits}}},
  volume = {12},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004992},
  abstract = {Author Summary The mesoscopic level of brain organization, describing the organization and dynamics of small circuits of neurons including from few tens to few thousands, has recently received considerable experimental attention. It is useful for describing small neural systems of invertebrates, and in mammalian neural systems it is often seen as a middle ground that is fundamental to link single neuron activity to complex functions and behavior. However, and somewhat counter-intuitively, the behavior of neural networks of small and intermediate size can be much more difficult to study mathematically than that of large networks, and appropriate mathematical methods to study the dynamics of such networks have not been developed yet. Here we consider a model of a network of firing-rate neurons with arbitrary finite size, and we study its local bifurcations using an analytical approach. This analysis, complemented by numerical studies for both the local and global bifurcations, shows the emergence of strong and previously unexplored finite-size effects that are particularly hard to detect in large networks. This study advances the tools available for the comprehension of finite-size neural circuits, going beyond the insights provided by the mean-field approximation and the current techniques for the quantification of finite-size effects.},
  number = {8},
  journal = {PLOS Computational Biology},
  author = {Fasoli, Diego and Cattani, Anna and Panzeri, Stefano},
  month = aug,
  year = {2016},
  keywords = {Membrane potential,Network analysis,Neural pathways,Neurons,neural networks,Bifurcation theory,Eigenvalues,Mesoscopic physics},
  pages = {e1004992},
  file = {articles/Fasoli2016.pdf}
}

@incollection{Bhalla2014,
  title = {Multiscale {{Modeling}} and {{Synaptic Plasticity}}},
  volume = {123},
  isbn = {978-0-12-397897-4},
  language = {en},
  booktitle = {Progress in {{Molecular Biology}} and {{Translational Science}}},
  publisher = {{Elsevier}},
  author = {Bhalla, Upinder S.},
  year = {2014},
  pages = {351-386},
  file = {book_sections/Bhalla2014.pdf},
  doi = {10.1016/B978-0-12-397897-4.00012-7}
}

@article{Sprekeler2017,
  series = {Neurobiology of Learning and Plasticity},
  title = {Functional Consequences of Inhibitory Plasticity: Homeostasis, the Excitation-Inhibition Balance and Beyond},
  volume = {43},
  issn = {0959-4388},
  shorttitle = {Functional Consequences of Inhibitory Plasticity},
  doi = {10.1016/j.conb.2017.03.014},
  abstract = {Computational neuroscience has a long-standing tradition of investigating the consequences of excitatory synaptic plasticity. In contrast, the functions of inhibitory plasticity are still largely nebulous, particularly given the bewildering diversity of interneurons in the brain. Here, we review recent computational advances that provide first suggestions for the functional roles of inhibitory plasticity, such as a maintenance of the excitation-inhibition balance, a stabilization of recurrent network dynamics and a decorrelation of sensory responses. The field is still in its infancy, but given the existing body of theory for excitatory plasticity, it is likely to mature quickly and deliver important insights into the self-organization of inhibitory circuits in the brain.},
  journal = {Current Opinion in Neurobiology},
  author = {Sprekeler, Henning},
  month = apr,
  year = {2017},
  pages = {198-203},
  file = {articles/Sprekeler2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/MR2BDMV5/S0959438817300909.html}
}

@article{Zenke2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.04200},
  primaryClass = {cs, q-bio, stat},
  title = {Improved Multitask Learning through Synaptic Intelligence},
  abstract = {Deep learning has led to remarkable advances when applied to problems where the data distribution does not change over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, and solve a diversity of tasks simultaneously. Furthermore, synapses in biological neurons are not simply real-valued scalars, but possess complex molecular machinery enabling non-trivial learning dynamics. In this study, we take a first step toward bringing this biological complexity into artificial neural networks. We introduce a model of intelligent synapses that accumulate task relevant information over time, and exploit this information to efficiently consolidate memories of old tasks to protect them from being overwritten as new tasks are learned. We apply our framework to learning sequences of related classification problems, and show that it dramatically reduces catastrophic forgetting while maintaining computational efficiency.},
  journal = {arXiv:1703.04200 [cs, q-bio, stat]},
  author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
  month = mar,
  year = {2017},
  keywords = {Quantitative Biology - Neurons and Cognition,Computer Science - Learning,Statistics - Machine Learning},
  file = {articles/Zenke2017_2.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/IJEF4HA2/1703.html}
}

@article{Atallah2012,
  title = {Parvalbumin-{{Expressing Interneurons Linearly Transform Cortical Responses}} to {{Visual Stimuli}}},
  volume = {73},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2011.12.013},
  abstract = {Summary
The response of cortical neurons to a sensory stimulus is shaped by the network in which they are embedded. Here we establish a role of parvalbumin (PV)-expressing cells, a large class of inhibitory neurons that target the soma and perisomatic compartments of pyramidal cells, in controlling cortical responses. By bidirectionally manipulating PV cell activity in visual cortex we show that these neurons strongly modulate layer 2/3 pyramidal cell spiking responses to visual stimuli while only modestly affecting their tuning properties. PV cells' impact on pyramidal cells is captured by a linear transformation, both additive and multiplicative, with a threshold. These results indicate that PV cells are ideally suited to modulate cortical gain and establish a causal relationship between a select neuron type and specific computations performed by the cortex during sensory processing.},
  number = {1},
  journal = {Neuron},
  author = {Atallah, Bassam V. and Bruns, William and Carandini, Matteo and Scanziani, Massimo},
  month = jan,
  year = {2012},
  pages = {159-170},
  file = {articles/Atallah2012.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/G2AASBC4/S0896627311010944.html}
}

@article{Doiron2016,
  title = {The Mechanics of State-Dependent Neural Correlations},
  volume = {19},
  copyright = {\textcopyright{} 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  doi = {10.1038/nn.4242},
  abstract = {Simultaneous recordings from large neural populations are becoming increasingly common. An important feature of population activity is the trial-to-trial correlated fluctuation of spike train outputs from recorded neuron pairs. Similar to the firing rate of single neurons, correlated activity can be modulated by a number of factors, from changes in arousal and attentional state to learning and task engagement. However, the physiological mechanisms that underlie these changes are not fully understood. We review recent theoretical results that identify three separate mechanisms that modulate spike train correlations: changes in input correlations, internal fluctuations and the transfer function of single neurons. We first examine these mechanisms in feedforward pathways and then show how the same approach can explain the modulation of correlations in recurrent networks. Such mechanistic constraints on the modulation of population activity will be important in statistical analyses of high-dimensional neural data.
View full text},
  language = {en},
  number = {3},
  journal = {Nature Neuroscience},
  author = {Doiron, Brent and {Litwin-Kumar}, Ashok and Rosenbaum, Robert and Ocker, Gabriel K. and Josi\'c, Kre{\v s}imir},
  month = mar,
  year = {2016},
  keywords = {Neural circuits,computational neuroscience,Biophysical models},
  pages = {383-393},
  file = {articles/Doiron2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/GMP4I8W5/nn.4242.html}
}

@book{Goodfellow2016,
  address = {Cambridge, Massachusetts},
  series = {Adaptive computation and machine learning},
  title = {Deep Learning},
  isbn = {978-0-262-03561-3},
  lccn = {Q325.5 .G66 2016},
  publisher = {{The MIT Press}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  keywords = {Machine learning},
  file = {books/Goodfellow2016_Deep-learning.pdf}
}

@article{Gallinaro2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.02912},
  primaryClass = {q-bio},
  title = {Associative Properties of Structural Plasticity Based on Firing Rate Homeostasis in a Balanced Recurrent Network of Spiking Neurons},
  abstract = {Hebbian and homeostatic plasticity have been studied extensively in the past, both experimentally and theoretically, but many aspects of their interaction remain to be elucidated. Hebbian plasticity is thought to shape neuronal connectivity during development and learning, whereas homeostatic plasticity would stabilize network activity. Here we investigate another aspect of this interaction, which is whether Hebbian associative properties can also emerge as a network effect from a plasticity rule based on homeostatic principles on the neuronal level. The maturation of cortical networks during sensory experience is an ideal case to explore this question. Excitatory neurons in the visual cortex of rodents have been shown to connect preferentially to neurons that respond to similar visual features. Since this connectivity bias is not existent at the time of eye opening, but only after some weeks of visual experience, it has been suggested that plastic mechanisms are responsible for the changes taking place during sensory stimulation. We consider a structural plasticity rule driven by a homeostasis of firing rate in a recurrent network of leaky integrate-and-fire (LIF) neurons exposed to external input that is modulated by the orientation of a visual stimulus. Our results show that feature specific connectivity, similar to what has been experimentally observed in rodent visual cortex, can emerge out of a random balanced network of LIF neurons with a plasticity rule that is not explicitly dependent on correlations between pre- and postsynaptic neuronal activity. The synaptic association of neurons responding to similar stimulus features occurs as a side-effect of controlling the activity of individual neurons. The experience dependent structural changes that are triggered by simulation are long lasting and decay only slowly when the neurons are exposed again to non modulated external input.},
  journal = {arXiv:1706.02912 [q-bio]},
  author = {Gallinaro, J\'ulia V. and Rotter, Stefan},
  month = jun,
  year = {2017},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {articles/Gallinaro2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/5KTQTZZG/1706.html}
}

@article{Hahnloser1998,
  title = {On the Piecewise Analysis of Networks of Linear Threshold Neurons},
  volume = {11},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(98)00012-4},
  abstract = {The computational abilities of recurrent networks of neurons with a linear activation function above threshold are analyzed. These networks selectively realise a linear mapping of their input. Using this property, the dynamics as well as the number and the stability of stationary states can be investigated. The important property of the boundedness of neural activities can be guaranteed by global inhibition. If used together with self-excitation, the global inhibition gives rise to a multi stable winner-take-all (WTA) mechanism. A condition for a neuron to be a potential winner of the competing dynamics is derived. The network becomes a largest input selector when the self-excitation is marginal.
Slowing down the global inhibition produces oscillations. The study of oscillations of random networks suggests that all cyclic trajectories of linear threshold networks are a result of the existence of partitions with undamped linear oscillations. Chaotic dynamics were never encountered in computer simulations and perhaps do not exist at all in small networks.},
  number = {4},
  journal = {Neural Networks},
  author = {Hahnloser, R. L. T.},
  month = jun,
  year = {1998},
  keywords = {Chaos,Monostable and multistable winner-take-all,Network Dynamics,Piecewise Linearity},
  pages = {691-697},
  file = {articles/Hahnloser1998.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/4TJ2CVM4/S0893608098000124.html}
}

@inproceedings{2015,
  title = {Cosyne},
  year = {2015},
  file = {conferences/2015_Cosyne.pdf}
}

@inproceedings{2017,
  title = {Cosyne},
  year = {2017},
  file = {conferences/2017_Cosyne.pdf}
}

@incollection{Miner2017,
  title = {Structural {{Plasticity}} and the {{Generation}} of {{Bidirectional Connectivity}}},
  isbn = {978-0-12-803784-3},
  booktitle = {The {{Rewiring Brain}}},
  publisher = {{Elsevier, Acad. Press}},
  author = {Miner, Daniel and Hoffmann, Felix Z. and Kleberg, Florence and Triesch, Jochen},
  year = {2017},
  pages = {247-260},
  file = {book_sections/Miner2017.PDF}
}

@article{Villa2016,
  title = {Inhibitory {{Synapses Are Repeatedly Assembled}} and {{Removed}} at {{Persistent Sites In Vivo}}},
  volume = {89},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.01.010},
  language = {en},
  number = {4},
  journal = {Neuron},
  author = {Villa, Katherine L. and Berry, Kalen P. and Subramanian, Jaichandar and Cha, Jae Won and Oh, Won Chan and Kwon, Hyung-Bae and Kubota, Yoshiyuki and So, Peter T.C. and Nedivi, Elly},
  month = feb,
  year = {2016},
  pages = {756-769},
  file = {articles/Villa2016.pdf}
}

@article{Rutishauser2008,
  title = {State-{{Dependent Computation Using Coupled Recurrent Networks}}},
  volume = {21},
  issn = {0899-7667},
  doi = {10.1162/neco.2008.03-08-734},
  number = {2},
  journal = {Neural Computation},
  author = {Rutishauser, Ueli and Douglas, Rodney J.},
  month = sep,
  year = {2008},
  pages = {478-509},
  file = {articles/Rutishauser2008.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/JZRCPPGT/neco.2008.html}
}

@article{Kanders2017,
  title = {Avalanche and Edge-of-Chaos Criticality Do Not Necessarily Co-Occur in Neural Networks},
  volume = {27},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.4978998},
  language = {en},
  number = {4},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  author = {Kanders, Karlis and Lorimer, Tom and Stoop, Ruedi},
  month = apr,
  year = {2017},
  pages = {047408},
  file = {articles/Kanders2017.pdf}
}

@article{Douglas2007,
  title = {Recurrent Neuronal Circuits in the Neocortex},
  volume = {17},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2007.04.024},
  number = {13},
  journal = {Current Biology},
  author = {Douglas, Rodney J. and Martin, Kevan A. C.},
  month = jul,
  year = {2007},
  pages = {R496-R500},
  file = {articles/Douglas2007.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/AQJDAK5K/S0960982207012651.html}
}

@article{Sutton1988,
  title = {Learning to Predict by the Methods of Temporal Differences},
  volume = {3},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00115009},
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
  language = {en},
  number = {1},
  journal = {Machine Learning},
  author = {Sutton, Richard S.},
  month = aug,
  year = {1988},
  pages = {9-44},
  file = {articles/Sutton1988.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/Q59QT9TI/BF00115009.html}
}

@techreport{zotero-1477,
  title = {Booktabs},
  file = {manuals/latex/booktabs.pdf},
  note = {manuals/latex}
}

@article{Ito2015,
  title = {Distinct {{Neural Representation}} in the {{Dorsolateral}}, {{Dorsomedial}}, and {{Ventral Parts}} of the {{Striatum}} during {{Fixed}}- and {{Free}}-{{Choice Tasks}}},
  volume = {35},
  number = {8},
  journal = {Journal of Neuroscience},
  author = {Ito, Makoto and Doya, Kenji},
  month = feb,
  year = {2015},
  pages = {3499-3514},
  file = {articles/Ito2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/CDXXH4BD/3499.html}
}

@article{Burak2009,
  title = {Accurate {{Path Integration}} in {{Continuous Attractor Network Models}} of {{Grid Cells}}},
  volume = {5},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000291},
  abstract = {Author Summary Even in the absence of external sensory cues, foraging rodents maintain an estimate of their position, allowing them to return home in a roughly straight line. This computation is known as dead reckoning or path integration. A discovery made three years ago in rats focused attention on the dorsolateral medial entorhinal cortex (dMEC) as a location in the rat's brain where this computation might be performed. In this area, so-called grid cells fire whenever the rat is on any vertex of a triangular grid that tiles the plane. Here we propose a model that could generate grid-cell-like responses in a neural network. The inputs to the model network convey information about the rat's velocity and heading, consistent with known inputs projecting into the dMEC. The network effectively integrates these inputs to produce a response that depends on the rat's absolute position. We show that such a neural network can integrate position accurately and can reproduce grid-cell-like responses similar to those observed experimentally. We then suggest a set of experiments that could help identify whether our suggested mechanism is responsible for the emergence of grid cells and for path integration in the rat's brain.},
  number = {2},
  journal = {PLOS Computational Biology},
  author = {Burak, Yoram and Fiete, Ila R.},
  month = feb,
  year = {2009},
  keywords = {N,P,a,c,e,f,g,i,k,l,m,o,r,s,t,u,v,w,y},
  pages = {e1000291},
  file = {articles/Burak2009.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/QNWXZGQ6/article.html}
}

@article{Muir2015,
  title = {Eigenspectrum Bounds for Semirandom Matrices with Modular and Spatial Structure for Neural Networks},
  volume = {91},
  doi = {10.1103/PhysRevE.91.042808},
  abstract = {The eigenvalue spectrum of the matrix of directed weights defining a neural network model is informative of several stability and dynamical properties of network activity. Existing results for eigenspectra of sparse asymmetric random matrices neglect spatial or other constraints in determining entries in these matrices, and so are of partial applicability to cortical-like architectures. Here we examine a parameterized class of networks that are defined by sparse connectivity, with connection weighting modulated by physical proximity (i.e., asymmetric Euclidean random matrices), modular network partitioning, and functional specificity within the excitatory population. We present a set of analytical constraints that apply to the eigenvalue spectra of associated weight matrices, highlighting the relationship between connectivity rules and classes of network dynamics.},
  number = {4},
  journal = {Physical Review E},
  author = {Muir, Dylan R. and {Mrsic-Flogel}, Thomas},
  month = apr,
  year = {2015},
  pages = {042808},
  file = {articles/Muir2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/8PTR6BCK/PhysRevE.91.html}
}

@article{Loewenstein2011,
  title = {Multiplicative {{Dynamics Underlie}} the {{Emergence}} of the {{Log}}-{{Normal Distribution}} of {{Spine Sizes}} in the {{Neocortex In Vivo}}},
  volume = {31},
  number = {26},
  journal = {Journal of Neuroscience},
  author = {Loewenstein, Yonatan and Kuras, Annerose and Rumpel, Simon},
  month = jun,
  year = {2011},
  pages = {9481-9488},
  file = {articles/Loewenstein2011.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/7M8PZBG9/9481.html}
}

@article{Tsodyks1997,
  title = {Paradoxical {{Effects}} of {{External Modulation}} of {{Inhibitory Interneurons}}},
  volume = {17},
  copyright = {Copyright \textcopyright{} 1997 Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  abstract = {The neocortex, hippocampus, and several other brain regions contain populations of excitatory principal cells with recurrent connections and strong interactions with local inhibitory interneurons. To improve our understanding of the interactions among these cell types, we modeled the dynamic behavior of this type of network, including external inputs. A surprising finding was that increasing the direct external inhibitory input to the inhibitory interneurons, without directly affecting any other part of the network, can, in some circumstances, cause the interneurons to increase their firing rates. The main prerequisite for this paradoxical response to external input is that the recurrent connections among the excitatory cells are strong enough to make the excitatory network unstable when feedback inhibition is removed. Because this requirement is met in the neocortex and several regions of the hippocampus, these observations have important implications for understanding the responses of interneurons to a variety of pharmacological and electrical manipulations. The analysis can be extended to a scenario with periodically varying external input, where it predicts a systematic relationship between the phase shift and depth of modulation for each interneuron. This prediction was tested by recording from interneurons in the CA1 region of the rat hippocampusin vivo, and the results broadly confirmed the model. These findings have further implications for the function of inhibitory and neuromodulatory circuits, which can be tested experimentally.},
  language = {en},
  number = {11},
  journal = {Journal of Neuroscience},
  author = {Tsodyks, Misha V. and Skaggs, William E. and Sejnowski, Terrence J. and McNaughton, Bruce L.},
  month = jun,
  year = {1997},
  keywords = {Hippocampus,inhibition,interneurons,network model,oscillation,theta rhythm},
  pages = {4382-4388},
  file = {articles/Tsodyks1997.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/3IXCHA8F/4382.html},
  pmid = {9151754}
}

@article{Lim2015,
  title = {Inferring Learning Rules from Distributions of Firing Rates in Cortical Neurons},
  volume = {18},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4158},
  number = {12},
  journal = {Nature Neuroscience},
  author = {Lim, Sukbin and McKee, Jillian L and Woloszyn, Luke and Amit, Yali and Freedman, David J and Sheinberg, David L and Brunel, Nicolas},
  month = nov,
  year = {2015},
  pages = {1804-1810},
  file = {articles/Lim2015.pdf}
}

@article{Doya2007a,
  title = {Reinforcement Learning: {{Computational}} Theory and Biological Mechanisms},
  volume = {1},
  issn = {1955-2068},
  shorttitle = {Reinforcement Learning},
  doi = {10.2976/1.2732246/10.2976/1},
  abstract = {Reinforcement learning is a computational framework for an active agent to learn behaviors on the basis of a scalar reward signal. The agent can be an animal, a human, or an artificial system such as a robot or a computer program. The reward can be food, water, money, or whatever measure of the performance of the agent. The theory of reinforcement learning, which was developed in an artificial intelligence community with intuitions from animal learning theory, is now giving a coherent account on the function of the basal ganglia. It now serves as the ``common language'' in which biologists, engineers, and social scientists can exchange their problems and findings. This article reviews the basic theoretical framework of reinforcement learning and discusses its recent and future contributions toward the understanding of animal behaviors and human decision making.},
  number = {1},
  journal = {HFSP Journal},
  author = {Doya, Kenji},
  month = may,
  year = {2007},
  pages = {30-40},
  file = {articles/Doya2007.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/KMV67ENT/1.html},
  pmid = {19404458}
}

@article{Barto1983,
  title = {Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems},
  volume = {SMC-13},
  issn = {0018-9472},
  doi = {10.1109/TSMC.1983.6313077},
  abstract = {It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.},
  number = {5},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  author = {Barto, A. G. and Sutton, R. S. and Anderson, C. W.},
  month = sep,
  year = {1983},
  keywords = {Neurons,Training,Adaptive systems,Biological neural networks,Pattern recognition,Problem-solving,Supervised learning,adaptive control,adaptive critic element,animal learning studies,associative search element,learning control problem,learning systems,movable cart,neural nets,neuronlike adaptive elements},
  pages = {834-846},
  file = {articles/Barto1983.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/T9DPZP85/6313077.html}
}

@article{Wilson1973,
  title = {A Mathematical Theory of the Functional Dynamics of Cortical and Thalamic Nervous Tissue},
  volume = {13},
  issn = {0023-5946, 1432-0770},
  doi = {10.1007/BF00288786},
  abstract = {It is proposed that distinct anatomical regions of cerebral cortex and of thalamic nuclei are functionally two-dimensional. On this view, the third (radial) dimension of cortical and thalamic structures is associated with a redundancy of circuits and functions so that reliable signal processing obtains in the presence of noisy or ambiguous stimuli.A mathematical model of simple cortical and thalamic nervous tissue is consequently developed, comprising two types of neurons (excitatory and inhibitory), homogeneously distributed in planar sheets, and interacting by way of recurrent lateral connexions. Following a discussion of certain anatomical and physiological restrictions on such interactions, numerical solutions of the relevant non-linear integro-differential equations are obtained. The results fall conveniently into three categories, each of which is postulated to correspond to a distinct type of tissue: sensory neo-cortex, archior prefrontal cortex, and thalamus.The different categories of solution are referred to as dynamical modes. The mode appropriate to thalamus involves a variety of non-linear oscillatory phenomena. That appropriate to archior prefrontal cortex is defined by the existence of spatially inhomogeneous stable steady states which retain contour information about prior stimuli. Finally, the mode appropriate to sensory neo-cortex involves active transient responses. It is shown that this particular mode reproduces some of the phenomenology of visual psychophysics, including spatial modulation transfer function determinations, certain metacontrast effects, and the spatial hysteresis phenomenon found in stereopsis.},
  language = {en},
  number = {2},
  journal = {Kybernetik},
  author = {Wilson, H. R. and Cowan, J. D.},
  month = sep,
  year = {1973},
  pages = {55-80},
  file = {articles/Wilson1973.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/P3I7Q8KC/BF00288786.html}
}

@article{Hennequin2016,
  title = {Inhibitory {{Plasticity}}: {{Balance}}, {{Control}}, and {{Codependence}}},
  shorttitle = {Inhibitory {{Plasticity}}},
  abstract = {Inhibitory neurons, although relatively few in number, exert powerful control over brain circuits. They stabilize network activity in the face of strong feedback excitation and actively engage in computations. Recent studies reveal the importance of a precise balance of excitation and inhibition in neural circuits, which often requires exquisite fine-tuning of inhibitory connections. Wereview inhibitory synaptic plasticity and its roles in shaping both feedforward and feedback control. We discuss the necessity of complex, codependent plasticity mechanisms to build nontrivial, functioning networks, and we end by summarizing experimental evidence of such interactions. Expected final online publication date for the Annual Review of Neuroscience Volume 40 is July 8, 2017. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
  language = {en},
  journal = {https://doi.org/10.1146/annurev-neuro-072116-031005},
  author = {Hennequin, Guillaume and Agnes, Everton J. and Vogels, Tim P.},
  month = aug,
  year = {2016},
  file = {articles/Hennequin2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/A7HUC75Z/annurev-neuro-072116-031005.html}
}

@article{Zenke2017a,
  series = {Neurobiology of Learning and Plasticity},
  title = {The Temporal Paradox of {{Hebbian}} Learning and Homeostatic Plasticity},
  volume = {43},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2017.03.015},
  abstract = {Hebbian plasticity, a synaptic mechanism which detects and amplifies co-activity between neurons, is considered a key ingredient underlying learning and memory in the brain. However, Hebbian plasticity alone is unstable, leading to runaway neuronal activity, and therefore requires stabilization by additional compensatory processes. Traditionally, a diversity of homeostatic plasticity phenomena found in neural circuits is thought to play this role. However, recent modelling work suggests that the slow evolution of homeostatic plasticity, as observed in experiments, is insufficient to prevent instabilities originating from Hebbian plasticity. To remedy this situation, we suggest that homeostatic plasticity is complemented by additional rapid compensatory processes, which rapidly stabilize neuronal activity on short timescales.},
  journal = {Current Opinion in Neurobiology},
  author = {Zenke, Friedemann and Gerstner, Wulfram and Ganguli, Surya},
  month = apr,
  year = {2017},
  pages = {166-176},
  file = {articles/Zenke2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/9QMWM388/S0959438817300910.html}
}

@article{Litwin-Kumar2016,
  title = {Inhibitory Stabilization and Visual Coding in Cortical Circuits with Multiple Interneuron Subtypes},
  volume = {115},
  copyright = {Copyright \textcopyright{} 2016 the American Physiological Society},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00732.2015},
  abstract = {Recent anatomical and functional characterization of cortical inhibitory interneurons has highlighted the diverse computations supported by different subtypes of interneurons. However, most theoretical models of cortex do not feature multiple classes of interneurons and rather assume a single homogeneous population. We study the dynamics of recurrent excitatory-inhibitory model cortical networks with parvalbumin (PV)-, somatostatin (SOM)-, and vasointestinal peptide-expressing (VIP) interneurons, with connectivity properties motivated by experimental recordings from mouse primary visual cortex. Our theory describes conditions under which the activity of such networks is stable and how perturbations of distinct neuronal subtypes recruit changes in activity through recurrent synaptic projections. We apply these conclusions to study the roles of each interneuron subtype in disinhibition, surround suppression, and subtractive or divisive modulation of orientation tuning curves. Our calculations and simulations determine the architectural and stimulus tuning conditions under which cortical activity consistent with experiment is possible. They also lead to novel predictions concerning connectivity and network dynamics that can be tested via optogenetic manipulations. Our work demonstrates that recurrent inhibitory dynamics must be taken into account to fully understand many properties of cortical dynamics observed in experiments.},
  language = {en},
  number = {3},
  journal = {Journal of Neurophysiology},
  author = {{Litwin-Kumar}, Ashok and Rosenbaum, Robert and Doiron, Brent},
  month = mar,
  year = {2016},
  pages = {1399-1409},
  file = {articles/Litwin-Kumar2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/JWQ8QEC4/1399.html},
  pmid = {26740531}
}

@techreport{zotero-1496,
  title = {Sansmath},
  file = {manuals/latex/sansmath.pdf},
  note = {manuals/latex}
}

@article{Binzegger2004,
  title = {A {{Quantitative Map}} of the {{Circuit}} of {{Cat Primary Visual Cortex}}},
  volume = {24},
  copyright = {Copyright \textcopyright{} 2004 Society for Neuroscience 0270-6474/04/248441-13.00/0},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1400-04.2004},
  abstract = {We developed a quantitative description of the circuits formed in cat area 17 by estimating the ``weight'' of the projections between different neuronal types. To achieve this, we made three-dimensional reconstructions of 39 single neurons and thalamic afferents labeled with horseradish peroxidase during intracellular recordings in vivo. These neurons served as representatives of the different types and provided the morphometrical data about the laminar distribution of the dendritic trees and synaptic boutons and the number of synapses formed by a given type of neuron. Extensive searches of the literature provided the estimates of numbers of the different neuronal types and their distribution across the cortical layers. Applying the simplification that synapses between different cell types are made in proportion to the boutons and dendrites that those cell types contribute to the neuropil in a given layer, we were able to estimate the probable source and number of synapses made between neurons in the six layers. The predicted synaptic maps were quantitatively close to the estimates derived from the experimental electron microscopic studies for the case of the main sources of excitatory and inhibitory input to the spiny stellate cells, which form a major target of layer 4 afferents. The map of the whole cortical circuit shows that there are very few ``strong'' but many ``weak'' excitatory projections, each of which may involve only a few percentage of the total complement of excitatory synapses of a single neuron.},
  language = {en},
  number = {39},
  journal = {Journal of Neuroscience},
  author = {Binzegger, Tom and Douglas, Rodney J. and Martin, Kevan A. C.},
  month = sep,
  year = {2004},
  keywords = {interlaminar connectivity,synaptic weights,cell reconstruction,cell types,connectivity rule,laminar distribution},
  pages = {8441-8453},
  file = {articles/Binzegger2004.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/4GWHDTH4/8441.html},
  pmid = {15456817}
}

@unpublished{Pfeiffer2006,
  title = {Reinforcement Learning Theory},
  author = {Pfeiffer, Michael},
  year = {2006},
  file = {manuscripts/Pfeiffer2006_Reinforcement-learning-theory.pdf}
}

@book{Spivak1996,
  address = {Houston, TX},
  title = {Answer {{Book}} to {{Calculus}}},
  isbn = {978-0-914098-90-4},
  language = {English},
  publisher = {{Publish or Perish}},
  author = {Spivak, Michael},
  year = {1996},
  file = {books/Spivak1996_Answer-Book-to-Calculus.pdf},
  note = {OCLC: 263629218}
}

@article{Hickok2007,
  title = {The Cortical Organization of Speech Processing},
  volume = {8},
  copyright = {\textcopyright{} 2007 Nature Publishing Group},
  issn = {1471-003X},
  doi = {10.1038/nrn2113},
  abstract = {Despite decades of research, the functional neuroanatomy of speech processing has been difficult to characterize. A major impediment to progress may have been the failure to consider task effects when mapping speech-related processing systems. We outline a dual-stream model of speech processing that remedies this situation. In this model, a ventral stream processes speech signals for comprehension, and a dorsal stream maps acoustic speech signals to frontal lobe articulatory networks. The model assumes that the ventral stream is largely bilaterally organized \textemdash{} although there are important computational differences between the left- and right-hemisphere systems \textemdash{} and that the dorsal stream is strongly left-hemisphere dominant.},
  language = {en},
  number = {5},
  journal = {Nature Reviews Neuroscience},
  author = {Hickok, Gregory and Poeppel, David},
  month = may,
  year = {2007},
  pages = {393-402},
  file = {articles/Hickok2007.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/TX78MHQZ/nrn2113.html}
}

@book{Sutton2016,
  title = {Reinforcement {{Learning}}: {{An Introduction}} - {{Sep2016Draft}}},
  author = {Sutton},
  year = {2016},
  file = {books/Sutton2016_Reinforcement-Learning-An-Introduction---Sep2016Draft.pdf}
}

@article{Froemke2015,
  title = {Plasticity of {{Cortical Excitatory}}-{{Inhibitory Balance}}},
  volume = {38},
  doi = {10.1146/annurev-neuro-071714-034002},
  abstract = {Synapses are highly plastic and are modified by changes in patterns of neural activity or sensory experience. Plasticity of cortical excitatory synapses is thought to be important for learning and memory, leading to alterations in sensory representations and cognitive maps. However, these changes must be coordinated across other synapses within local circuits to preserve neural coding schemes and the organization of excitatory and inhibitory inputs, i.e., excitatory-inhibitory balance. Recent studies indicate that inhibitory synapses are also plastic and are controlled directly by a large number of neuromodulators, particularly during episodes of learning. Many modulators transiently alter excitatory-inhibitory balance by decreasing inhibition, and thus disinhibition has emerged as a major mechanism by which neuromodulation might enable long-term synaptic modifications naturally. This review examines the relationships between neuromodulation and synaptic plasticity, focusing on the induction of long-term changes that collectively enhance cortical excitatory-inhibitory balance for improving perception and behavior.},
  number = {1},
  journal = {Annual Review of Neuroscience},
  author = {Froemke, Robert C.},
  year = {2015},
  pages = {195-219},
  file = {articles/Froemke2015.pdf},
  pmid = {25897875}
}

@article{Bourne2011,
  title = {Coordination of Size and Number of Excitatory and Inhibitory Synapses Results in a Balanced Structural Plasticity along Mature Hippocampal {{CA1}} Dendrites during {{LTP}}},
  volume = {21},
  issn = {1098-1063},
  doi = {10.1002/hipo.20768},
  abstract = {Enlargement of dendritic spines and synapses correlates with enhanced synaptic strength during long-term potentiation (LTP), especially in immature hippocampal neurons. Less clear is the nature of this structural synaptic plasticity on mature hippocampal neurons, and nothing is known about the structural plasticity of inhibitory synapses during LTP. Here the timing and extent of structural synaptic plasticity and changes in local protein synthesis evidenced by polyribosomes were systematically evaluated at both excitatory and inhibitory synapses on CA1 dendrites from mature rats following induction of LTP with theta-burst stimulation (TBS). Recent work suggests dendritic segments can act as functional units of plasticity. To test whether structural synaptic plasticity is similarly coordinated, we reconstructed from serial section transmission electron microscopy all of the spines and synapses along representative dendritic segments receiving control stimulation or TBS-LTP. At 5 min after TBS, polyribosomes were elevated in large spines suggesting an initial burst of local protein synthesis, and by 2 h only those spines with further enlarged synapses contained polyribosomes. Rapid induction of synaptogenesis was evidenced by an elevation in asymmetric shaft synapses and stubby spines at 5 min and more nonsynaptic filopodia at 30 min. By 2 h, the smallest synaptic spines were markedly reduced in number. This synapse loss was perfectly counterbalanced by enlargement of the remaining excitatory synapses such that the summed synaptic surface area per length of dendritic segment was constant across time and conditions. Remarkably, the inhibitory synapses showed a parallel synaptic plasticity, also demonstrating a decrease in number perfectly counterbalanced by an increase in synaptic surface area. Thus, TBS-LTP triggered spinogenesis followed by loss of small excitatory and inhibitory synapses and a subsequent enlargement of the remaining synapses by 2 h. These data suggest that dendritic segments coordinate structural plasticity across multiple synapses and maintain a homeostatic balance of excitatory and inhibitory inputs through local protein-synthesis and selective capture or redistribution of dendritic resources. \textcopyright{}2010 Wiley-Liss, Inc.},
  language = {en},
  number = {4},
  journal = {Hippocampus},
  author = {Bourne, Jennifer N. and Harris, Kristen M.},
  month = apr,
  year = {2011},
  keywords = {dendritic spine,3D reconstruction,adult,hippocampal slice,polyribosomes,serial section transmission electron microscopy,ultrastructure},
  pages = {354-373},
  file = {articles/Bourne2011.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/XZ93NQF7/abstract.html}
}

@techreport{zotero-1510,
  title = {Siunitx},
  file = {manuals/latex/siunitx.pdf},
  note = {manuals/latex}
}

@article{Timme2016,
  title = {High-{{Degree Neurons Feed Cortical Computations}}},
  volume = {12},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004858},
  abstract = {Author Summary We recorded the electrical activity of hundreds of neurons simultaneously in brain tissue from mice and we analyzed these signals using state-of-the-art tools from information theory. These tools allowed us to ascertain which neurons were transmitting information to other neurons and to characterize the computations performed by neurons using the inputs they received from two or more other neurons. We found that computations did not occur equally in all neurons throughout the networks. Surprisingly, neurons that computed large amounts of information tended to be recipients of information from neurons with a large number of outgoing connections. Interestingly, the number of incoming connections to a neuron was not related to the amount of information that neuron computed. To better understand these results, we built a network model to match the data. Unexpectedly, the model also maximized information transfer in the presence of network-wide correlations. This suggested a way that networks of cortical neurons could deal with common random background input. These results are the first to show that the amount of information computed by a neuron depends on where it is located in the surrounding network.},
  number = {5},
  journal = {PLOS Computational Biology},
  author = {Timme, Nicholas M. and Ito, Shinya and Myroshnychenko, Maxym and Nigam, Sunny and Shimono, Masanori and Yeh, Fang-Chin and Hottowy, Pawel and Litke, Alan M. and Beggs, John M.},
  month = may,
  year = {2016},
  keywords = {Action potentials,Information theory,Network analysis,Neurons,Signaling networks,entropy,neural networks,Communications},
  pages = {e1004858},
  file = {articles/Timme2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/4X6PKKT8/article.html}
}

@book{Spivak1994,
  address = {Houston, Tex},
  edition = {3. ed},
  title = {Calculus},
  isbn = {978-0-914098-89-8},
  language = {eng},
  publisher = {{Perish}},
  author = {Spivak, Michael},
  year = {1994},
  keywords = {Analysis,Infinitesimalrechnung},
  file = {books/Spivak1994_Calculus.pdf},
  note = {OCLC: 31441929}
}

@article{Sadacca2016,
  title = {The {{Behavioral Relevance}} of {{Cortical Neural Ensemble Responses Emerges Suddenly}}},
  volume = {36},
  number = {3},
  journal = {Journal of Neuroscience},
  author = {Sadacca, Brian F. and Mukherjee, Narendra and Vladusich, Tony and Li, Jennifer X. and Katz, Donald B. and Miller, Paul},
  month = jan,
  year = {2016},
  pages = {655-669},
  file = {articles/Sadacca2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/9CKQFGKQ/655.html}
}

@article{Dettner2016,
  title = {Temporal Pairwise Spike Correlations Fully Capture Single-Neuron Information},
  volume = {7},
  copyright = {\textcopyright{} 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  issn = {2041-1723},
  doi = {10.1038/ncomms13805},
  abstract = {To understand the neural code it is important to determine what spiking features contain the relevant information. Here, the authors use mathematical approaches to show that two pair-wise correlation functions, the autocorrelation function within spike trains and cross-correlation function across stimulus presentations, fully determine the neural information content.},
  language = {en},
  journal = {Nature Communications},
  author = {Dettner, Amadeus and M\"unzberg, Sabrina and Tchumatchenko, Tatjana},
  month = dec,
  year = {2016},
  pages = {13805},
  file = {articles/Dettner2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/ITHAMRSX/ncomms13805.html}
}

@article{Ding2016,
  title = {Cortical Tracking of Hierarchical Linguistic Structures in Connected Speech},
  volume = {19},
  copyright = {\textcopyright{} 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  doi = {10.1038/nn.4186},
  abstract = {The most critical attribute of human language is its unbounded combinatorial nature: smaller elements can be combined into larger structures on the basis of a grammatical system, resulting in a hierarchy of linguistic units, such as words, phrases and sentences. Mentally parsing and representing such structures, however, poses challenges for speech comprehension. In speech, hierarchical linguistic structures do not have boundaries that are clearly defined by acoustic cues and must therefore be internally and incrementally constructed during comprehension. We found that, during listening to connected speech, cortical activity of different timescales concurrently tracked the time course of abstract linguistic structures at different hierarchical levels, such as words, phrases and sentences. Notably, the neural tracking of hierarchical linguistic structures was dissociated from the encoding of acoustic cues and from the predictability of incoming words. Our results indicate that a hierarchy of neural processing timescales underlies grammar-based internal construction of hierarchical linguistic structure.
View full text},
  language = {en},
  number = {1},
  journal = {Nature Neuroscience},
  author = {Ding, Nai and Melloni, Lucia and Zhang, Hang and Tian, Xing and Poeppel, David},
  month = jan,
  year = {2016},
  keywords = {Psychology,Sensory processing,language},
  pages = {158-164},
  file = {articles/Ding2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/7BJZCX2N/nn.4186.html}
}

@article{Sharpee2016,
  title = {25th {{Annual Computational Neuroscience Meeting}}: {{CNS}}-2016},
  volume = {17},
  issn = {1471-2202},
  shorttitle = {25th {{Annual Computational Neuroscience Meeting}}},
  doi = {10.1186/s12868-016-0283-6},
  abstract = {A1 Functional advantages of cell-type heterogeneity in neural circuits},
  number = {1},
  journal = {BMC Neuroscience},
  author = {Sharpee, Tatyana O. and Destexhe, Alain and Kawato, Mitsuo and Sekuli\'c, Vladislav and Skinner, Frances K. and W\'ojcik, Daniel K. and Chintaluri, Chaitanya and Cserp\'an, Dorottya and Somogyv\'ari, Zolt\'an and Kim, Jae Kyoung and Kilpatrick, Zachary P. and Bennett, Matthew R. and Josi\'c, Kresimir and Elices, Irene and Arroyo, David and Levi, Rafael and Rodriguez, Francisco B. and Varona, Pablo and Hwang, Eunjin and Kim, Bowon and Han, Hio-Been and Kim, Tae and McKenna, James T. and Brown, Ritchie E. and McCarley, Robert W. and Choi, Jee Hyun and Rankin, James and Popp, Pamela Osborn and Rinzel, John and Tabas, Alejandro and Rupp, Andr\'e and {Balaguer-Ballester}, Emili and Maturana, Matias I. and Grayden, David B. and Cloherty, Shaun L. and Kameneva, Tatiana and Ibbotson, Michael R. and Meffin, Hamish and Koren, Veronika and Lochmann, Timm and Dragoi, Valentin and Obermayer, Klaus and Psarrou, Maria and Schilstra, Maria and Davey, Neil and {Torben-Nielsen}, Benjamin and Steuber, Volker and Ju, Huiwen and Yu, Jiao and Hines, Michael L. and Chen, Liang and Yu, Yuguo and Kim, Jimin and Leahy, Will and Shlizerman, Eli and Birgiolas, Justas and Gerkin, Richard C. and Crook, Sharon M. and Viriyopase, Atthaphon and Memmesheimer, Raoul-Martin and Gielen, Stan and Dabaghian, Yuri and DeVito, Justin and Perotti, Luca and Kim, Anmo J. and Fenk, Lisa M. and Cheng, Cheng and Maimon, Gaby and Zhao, Chang and Widmer, Yves and Sprecher, Simon and Senn, Walter and Halnes, Geir and {M\"aki-Marttunen}, Tuomo and Keller, Daniel and Pettersen, Klas H. and Andreassen, Ole A. and Einevoll, Gaute T. and Yamada, Yasunori and {Steyn-Ross}, Moira L. and {Alistair Steyn-Ross}, D. and Mejias, Jorge F. and Murray, John D. and Kennedy, Henry and Wang, Xiao-Jing and Kruscha, Alexandra and Grewe, Jan and Benda, Jan and Lindner, Benjamin and Badel, Laurent and Ohta, Kazumi and Tsuchimoto, Yoshiko and Kazama, Hokto and Kahng, B. and Tam, Nicoladie D. and Pollonini, Luca and Zouridakis, George and Soh, Jaehyun and Kim, DaeEun and Yoo, Minsu and Palmer, S. E. and Culmone, Viviana and Bojak, Ingo and Ferrario, Andrea and {Merrison-Hort}, Robert and Borisyuk, Roman and Kim, Chang Sub and Tezuka, Taro and Joo, Pangyu and Rho, Young-Ah and Burton, Shawn D. and Bard Ermentrout, G. and Jeong, Jaeseung and Urban, Nathaniel N. and Marsalek, Petr and Kim, Hoon-Hee and Moon, Seok-hyun and Lee, Do-won and Lee, Sung-beom and Lee, Ji-yong and Molkov, Yaroslav I. and Hamade, Khaldoun and Teka, Wondimu and Barnett, William H. and Kim, Taegyo and Markin, Sergey and Rybak, Ilya A. and Forro, Csaba and Dermutz, Harald and Demk\'o, L\'aszl\'o and V\"or\"os, J\'anos and Babichev, Andrey and Huang, Haiping and {Verduzco-Flores}, Sergio and Dos Santos, Filipa and Andras, Peter and Metzner, Christoph and Schweikard, Achim and Zurowski, Bartosz and Roach, James P. and Sander, Leonard M. and Zochowski, Michal R. and Skilling, Quinton M. and Ognjanovski, Nicolette and Aton, Sara J. and Zochowski, Michal and Wang, Sheng-Jun and Ouyang, Guang and Guang, Jing and Zhang, Mingsha and Michael Wong, K. Y. and Zhou, Changsong and Robinson, Peter A. and {Sanz-Leon}, Paula and Drysdale, Peter M. and Fung, Felix and Abeysuriya, Romesh G. and Rennie, Chris J. and Zhao, Xuelong and Choe, Yoonsuck and Yang, Huei-Fang and Mi, Yuanyuan and Lin, Xiaohan and Wu, Si and Liedtke, Joscha and Schottdorf, Manuel and Wolf, Fred and Yamamura, Yoriko and Wickens, Jeffery R. and Rumbell, Timothy and Ramsey, Julia and Reyes, Amy and Dragulji\'c, Danel and Hof, Patrick R. and Luebke, Jennifer and Weaver, Christina M. and He, Hu and Yang, Xu and Ma, Hailin and Xu, Zhiheng and Wang, Yuzhe and Baek, Kwangyeol and Morris, Laurel S. and Kundu, Prantik and Voon, Valerie and Agnes, Everton J. and Vogels, Tim P. and Podlaski, William F. and Giese, Martin and Kuravi, Pradeep and Vogels, Rufin and Seeholzer, Alexander and Podlaski, William and Ranjan, Rajnish and Vogels, Tim and Torres, Joaquin J. and Baroni, Fabiano and Latorre, Roberto and Gips, Bart and Lowet, Eric and Roberts, Mark J. and {de Weerd}, Peter and Jensen, Ole and {van der Eerden}, Jan and Goodarzinick, Abdorreza and Niry, Mohammad D. and Valizadeh, Alireza and Pariz, Aref and Parsi, Shervin S. and Warburton, Julia M. and Marucci, Lucia and Tamagnini, Francesco and Brown, Jon and {Tsaneva-Atanasova}, Krasimira and Kleberg, Florence I. and Triesch, Jochen and Moezzi, Bahar and Iannella, Nicolangelo and Schaworonkow, Natalie and Plogmacher, Lukas and Goldsworthy, Mitchell R. and Hordacre, Brenton and McDonnell, Mark D. and Ridding, Michael C. and Zapotocky, Martin and Smit, Daniel and Fouquet, Coralie and Trembleau, Alain and Dasgupta, Sakyasingha and Nishikawa, Isao and Aihara, Kazuyuki and Toyoizumi, Taro and Robb, Daniel T. and Mellen, Nick and Toporikova, Natalia and Tang, Rongxiang and Tang, Yi-Yuan and Liang, Guangsheng and Kiser, Seth A. and Howard, James H. and Goncharenko, Julia and Voronenko, Sergej O. and Ahamed, Tosif and Stephens, Greg and Yger, Pierre and Lefebvre, Baptiste and Spampinato, Giulia Lia Beatrice and Esposito, Elric and {et Olivier Marre}, Marcel Stimberg and Choi, Hansol and Song, Min-Ho and Chung, SueYeon and Lee, Dan D. and Sompolinsky, Haim and Phillips, Ryan S. and Smith, Jeffrey and Chatzikalymniou, Alexandra Pierri and Ferguson, Katie and Alex Cayco Gajic, N. and Clopath, Claudia and Angus Silver, R. and Gleeson, Padraig and Marin, Boris and Sadeh, Sadra and Quintana, Adrian and Cantarelli, Matteo and {Dura-Bernal}, Salvador and Lytton, William W. and Davison, Andrew and Li, Luozheng and Zhang, Wenhao and Wang, Dahui and Song, Youngjo and Park, Sol and Choi, Ilhwan and Shin, Hee-sup and Choi, Hannah and Pasupathy, Anitha and {Shea-Brown}, Eric and Huh, Dongsung and Sejnowski, Terrence J. and Vogt, Simon M. and Kumar, Arvind and Schmidt, Robert and Van Wert, Stephen and Schiff, Steven J. and Veale, Richard and Scheutz, Matthias and Lee, Sang Wan and Gallinaro, J\'ulia and Rotter, Stefan and Rubchinsky, Leonid L. and Cheung, Chung Ching and {Ratnadurai-Giridharan}, Shivakeshavan and Shomali, Safura Rashid and Ahmadabadi, Majid Nili and Shimazaki, Hideaki and Nader Rasuli, S. and Zhao, Xiaochen and Rasch, Malte J. and {etal}},
  year = {2016},
  pages = {54},
  file = {articles/Sharpee2016.pdf}
}

@book{DeSchutter2010,
  address = {Cambridge, Mass},
  series = {Computational neuroscience},
  title = {Computational Modeling Methods for Neuroscientists},
  isbn = {978-0-262-01327-7},
  lccn = {QP357.5 .C625 2010},
  publisher = {{MIT Press}},
  editor = {De Schutter, Erik},
  year = {2010},
  keywords = {Mathematical models,Models; Neurological,Neurobiology,Neurosciences,computational neuroscience,methods},
  file = {books/De Schutter2010_Computational-modeling-methods-for-neuroscientists.pdf},
  note = {OCLC: ocn310075964}
}

@article{Lucke2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.04812},
  primaryClass = {stat},
  title = {K-{{Means}} Is a {{Variational EM Approximation}} of {{Gaussian Mixture Models}}},
  abstract = {We show that k-means (Lloyd's algorithm) is equivalent to a variational EM approximation of a Gaussian Mixture Model (GMM) with isotropic Gaussians. The k-means algorithm is obtained if truncated posteriors are used as variational distributions. In contrast to the standard way to relate k-means and GMMs, we show that it is not required to consider the limit case of Gaussians with zero variance. There are a number of consequences following from our observation: (A) k-means can be shown to monotonously increase the free-energy associated with truncated distributions; (B) Using the free-energy, we can derive an explicit and compact formula of a lower GMM likelihood bound which uses the k-means objective as argument; (C) We can generalize k-means using truncated variational EM, and relate such generalizations to other k-means-like algorithms. In general, truncated variational EM provides a natural and quantitative link between k-means-like clustering and GMM clustering algorithms which may be very relevant for future theoretical as well as empirical studies.},
  journal = {arXiv:1704.04812 [stat]},
  author = {L\"ucke, J\"org and Forster, Dennis},
  month = apr,
  year = {2017},
  keywords = {62H30,Statistics - Machine Learning},
  file = {articles/Lücke2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/7VMDFTZK/1704.html}
}

@inproceedings{2016,
  title = {Cosyne},
  year = {2016},
  file = {conferences/2016_Cosyne.pdf}
}

@article{Giraud2012,
  title = {Cortical Oscillations and Speech Processing: Emerging Computational Principles and Operations},
  volume = {15},
  copyright = {\textcopyright{} 2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  shorttitle = {Cortical Oscillations and Speech Processing},
  doi = {10.1038/nn.3063},
  abstract = {Neuronal oscillations are ubiquitous in the brain and may contribute to cognition in several ways: for example, by segregating information and organizing spike timing. Recent data show that delta, theta and gamma oscillations are specifically engaged by the multi-timescale, quasi-rhythmic properties of speech and can track its dynamics. We argue that they are foundational in speech and language processing, 'packaging' incoming information into units of the appropriate temporal granularity. Such stimulus-brain alignment arguably results from auditory and motor tuning throughout the evolution of speech and language and constitutes a natural model system allowing auditory research to make a unique contribution to the issue of how neural oscillatory activity affects human cognition.},
  language = {en},
  number = {4},
  journal = {Nature Neuroscience},
  author = {Giraud, Anne-Lise and Poeppel, David},
  month = apr,
  year = {2012},
  keywords = {Cognitive neuroscience,computational neuroscience,Neurophysiology},
  pages = {511-517},
  file = {articles/Giraud2012.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/DRE3M7MV/nn.3063.html}
}

@article{Muir2014,
  title = {Anatomical {{Constraints}} on {{Lateral Competition}} in {{Columnar Cortical Architectures}}},
  volume = {26},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00613},
  number = {8},
  journal = {Neural Computation},
  author = {Muir, Dylan R. and Cook, Matthew},
  month = may,
  year = {2014},
  pages = {1624-1666},
  file = {articles/Muir2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/G6BQZED5/NECO_a_00613.html}
}

@book{Mohri2012,
  address = {Cambridge, MA},
  series = {Adaptive computation and machine learning series},
  title = {Foundations of Machine Learning},
  isbn = {978-0-262-01825-8},
  lccn = {Q325.5 .M64 2012},
  publisher = {{MIT Press}},
  author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2012},
  keywords = {Machine learning,Computer algorithms},
  file = {books/Mohri2012_Foundations-of-machine-learning.pdf}
}

@article{Rubinski2015,
  title = {Remodeling and {{Tenacity}} of {{Inhibitory Synapses}}: {{Relationships}} with {{Network Activity}} and {{Neighboring Excitatory Synapses}}},
  volume = {11},
  issn = {1553-7358},
  shorttitle = {Remodeling and {{Tenacity}} of {{Inhibitory Synapses}}},
  doi = {10.1371/journal.pcbi.1004632},
  abstract = {Author Summary Synaptic plasticity is widely believed to constitute a fundamental mechanism for altering network function. An (implicit) extension of this belief is an assumption that spontaneous changes in synaptic function should not occur to any significant degree. Where excitatory synapses are concerned, recent studies have questioned the validity of this assumption. Where inhibitory synapses are concerned, however, much less is known. Here we followed the spontaneous remodeling dynamics of inhibitory synapses for days, and analyzed these dynamics within a statistical framework previously developed for glutamatergic synapses. Like their excitatory counterparts, sizes of individual synapses fluctuated considerably. Similarly, these spontaneous fluctuations were governed by a well-defined statistical process which assures that synaptic size distributions remain constant. Contrary to the aforementioned assumption, these spontaneous fluctuations drove changes in synaptic size configurations; interestingly, however, change rates were slower for inhibitory synapses. Unlike excitatory synapses, suppressing network activity barely affected inhibitory synapse remodeling dynamics, synaptic configuration change rates or synaptic size distributions. Our findings thus point to quantitative differences in spontaneous remodeling dynamics of inhibitory and excitatory synapses, but also indicate that the processes that control their sizes and govern their remodeling dynamics are fundamentally similar.},
  number = {11},
  journal = {PLOS Computational Biology},
  author = {Rubinski, Anna and Ziv, Noam E.},
  month = nov,
  year = {2015},
  keywords = {Neuronal dendrites,Neurons,Synapses,neural networks,Statistical distributions,Fluorescence imaging,Fluorescence microscopy,Linear regression analysis},
  pages = {e1004632},
  file = {articles/Rubinski2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/ECDZS98V/article.html}
}

@book{Rudin1976,
  address = {New York},
  edition = {3d ed},
  series = {International series in pure and applied mathematics},
  title = {Principles of Mathematical Analysis},
  isbn = {978-0-07-054235-8},
  lccn = {QA300 .R8 1976},
  publisher = {{McGraw-Hill}},
  author = {Rudin, Walter},
  year = {1976},
  keywords = {Mathematical analysis},
  file = {books/Rudin1976_Principles-of-mathematical-analysis.pdf}
}

@article{Neftci2013,
  title = {Synthesizing Cognition in Neuromorphic Electronic Systems},
  volume = {110},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1212083110},
  abstract = {The quest to implement intelligent processing in electronic neuromorphic systems lacks methods for achieving reliable behavioral dynamics on substrates of inherently imprecise and noisy neurons. Here we report a solution to this problem that involves first mapping an unreliable hardware layer of spiking silicon neurons into an abstract computational layer composed of generic reliable subnetworks of model neurons and then composing the target behavioral dynamics as a ``soft state machine'' running on these reliable subnets. In the first step, the neural networks of the abstract layer are realized on the hardware substrate by mapping the neuron circuit bias voltages to the model parameters. This mapping is obtained by an automatic method in which the electronic circuit biases are calibrated against the model parameters by a series of population activity measurements. The abstract computational layer is formed by configuring neural networks as generic soft winner-take-all subnetworks that provide reliable processing by virtue of their active gain, signal restoration, and multistability. The necessary states and transitions of the desired high-level behavior are then easily embedded in the computational layer by introducing only sparse connections between some neurons of the various subnets. We demonstrate this synthesis method for a neuromorphic sensory agent that performs real-time context-dependent classification of motion patterns observed by a silicon retina.},
  language = {en},
  number = {37},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Neftci, Emre and Binas, Jonathan and Rutishauser, Ueli and Chicca, Elisabetta and Indiveri, Giacomo and Douglas, Rodney J.},
  month = oct,
  year = {2013},
  keywords = {analog very large-scale integration,artificial neural systems,decision making,sensorimotor,working memory},
  pages = {E3468-E3476},
  file = {articles/Neftci2013.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/RKXATFSF/E3468.html},
  pmid = {23878215}
}

@book{Puterman2005,
  address = {Hoboken, NJ},
  series = {Wiley series in probability and statistics},
  title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  isbn = {978-0-471-72782-8},
  shorttitle = {Markov Decision Processes},
  language = {eng},
  publisher = {{Wiley-Interscience}},
  author = {Puterman, Martin L.},
  year = {2005},
  keywords = {Markov processes,Diskreter Markov-Prozess,Dynamic programming,Statistical decision},
  file = {books/Puterman2005_Markov-decision-processes-discrete-stochastic-dynamic-programming.djvu},
  note = {OCLC: 254152847}
}

@article{Chaudhuri2016,
  title = {Computational Principles of Memory},
  volume = {19},
  copyright = {\textcopyright{} 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  doi = {10.1038/nn.4237},
  abstract = {The ability to store and later use information is essential for a variety of adaptive behaviors, including integration, learning, generalization, prediction and inference. In this Review, we survey theoretical principles that can allow the brain to construct persistent states for memory. We identify requirements that a memory system must satisfy and analyze existing models and hypothesized biological substrates in light of these requirements. We also highlight open questions, theoretical puzzles and problems shared with computer science and information theory.
View full text},
  language = {en},
  number = {3},
  journal = {Nature Neuroscience},
  author = {Chaudhuri, Rishidev and Fiete, Ila},
  month = mar,
  year = {2016},
  keywords = {Dynamical systems,Learning algorithms,Long-term memory,Short-term memory},
  pages = {394-403},
  file = {articles/Chaudhuri2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/MGHQH6U3/nn.4237.html}
}

@article{Bhalla2017,
  title = {Synaptic Input Sequence Discrimination on Behavioral Timescales Mediated by Reaction-Diffusion Chemistry in Dendrites},
  volume = {6},
  copyright = {\textcopyright{} 2017 Bhalla et al. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
  issn = {2050-084X},
  doi = {10.7554/eLife.25827},
  abstract = {The powerful computational operation of sequence recognition on behavioral timescales of approximately 1 s may emerge from synaptic activity-triggered build-up of biochemical waves in short 20 micron zones on dendrites.},
  language = {en},
  journal = {eLife},
  author = {Bhalla, Upinder Singh},
  month = apr,
  year = {2017},
  pages = {e25827},
  file = {articles/Bhalla2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/RVNR74DU/25827.html}
}

@book{Strang2010,
  address = {Wellesley, Mass},
  edition = {2. ed},
  title = {Calculus},
  isbn = {978-0-9802327-4-5},
  language = {eng},
  publisher = {{Wellesley-Cambridge Press}},
  author = {Strang, Gilbert},
  year = {2010},
  keywords = {Analysis,Lehrbuch,Mathematik},
  file = {books/Strang2010_Calculus.pdf},
  note = {OCLC: 820373854}
}

@article{Buzsaki2014,
  title = {The Log-Dynamic Brain: How Skewed Distributions Affect Network Operations},
  volume = {15},
  copyright = {\textcopyright{} 2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1471-003X},
  shorttitle = {The Log-Dynamic Brain},
  doi = {10.1038/nrn3687},
  abstract = {We often assume that the variables of functional and structural brain parameters \textemdash{} such as synaptic weights, the firing rates of individual neurons, the synchronous discharge of neural populations, the number of synaptic contacts between neurons and the size of dendritic boutons \textemdash{} have a bell-shaped distribution. However, at many physiological and anatomical levels in the brain, the distribution of numerous parameters is in fact strongly skewed with a heavy tail, suggesting that skewed (typically lognormal) distributions are fundamental to structural and functional brain organization. This insight not only has implications for how we should collect and analyse data, it may also help us to understand how the different levels of skewed distributions \textemdash{} from synapses to cognition \textemdash{} are related to each other.},
  language = {en},
  number = {4},
  journal = {Nature Reviews Neuroscience},
  author = {Buzs\'aki, Gy\"orgy and Mizuseki, Kenji},
  month = apr,
  year = {2014},
  keywords = {Cellular neuroscience,Synaptic Transmission,Spine regulation and structure},
  pages = {264-278},
  file = {articles/Buzsáki2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/9TCHJH6X/nrn3687.html}
}

@book{Ross2013,
  address = {New York},
  title = {Elementary Analysis: The Theory of Calculus},
  isbn = {978-1-4614-6270-5},
  shorttitle = {Elementary Analysis},
  publisher = {{Springer}},
  author = {Ross, Kenneth A.},
  year = {2013},
  file = {books/Ross2013_Elementary-analysis-the-theory-of-calculus.pdf}
}

@article{Kamyshanska2015,
  title = {The {{Potential Energy}} of an {{Autoencoder}}},
  volume = {37},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2014.2362140},
  abstract = {Autoencoders are popular feature learning models, that are conceptually simple, easy to train and allow for efficient inference. Recent work has shown how certain autoencoders can be associated with an energy landscape, akin to negative log-probability in a probabilistic model, which measures how well the autoencoder can represent regions in the input space. The energy landscape has been commonly inferred heuristically, by using a training criterion that relates the autoencoder to a probabilistic model such as a Restricted Boltzmann Machine (RBM). In this paper we show how most common autoencoders are naturally associated with an energy function, independent of the training procedure, and that the energy landscape can be inferred analytically by integrating the reconstruction function of the autoencoder. For autoencoders with sigmoid hidden units, the energy function is identical to the free energy of an RBM, which helps shed light onto the relationship between these two types of model. We also show that the autoencoder energy function allows us to explain common regularization procedures, such as contractive training, from the perspective of dynamical systems. As a practical application of the energy function, a generative classifier based on class-specific autoencoders is presented.},
  number = {6},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author = {Kamyshanska, H. and Memisevic, R.},
  month = jun,
  year = {2015},
  keywords = {Probability,Training,Analytical models,Autoencoders,Boltzmann machines,Data models,Principal component analysis,Probabilistic logic,RBM,Vectors,class-specific autoencoders,contractive training,energy function,energy landscape,feature learning models,generative classification,generative classifier,learning (artificial intelligence),negative log-probability,pattern classification,potential energy,probabilistic model,reconstruction function,regularization procedures,representation learning,restricted Boltzmann machine,sigmoid hidden units,training criterion,unsupervised learning},
  pages = {1261-1273},
  file = {articles/Kamyshanska2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/7EMTPMBK/6918504.html}
}

@article{Xue2014,
  title = {Equalizing Excitation-Inhibition Ratios across Visual Cortical Neurons},
  volume = {511},
  copyright = {\textcopyright{} 2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {0028-0836},
  doi = {10.1038/nature13321},
  abstract = {The relationship between synaptic excitation and inhibition (E/I ratio), two opposing forces in the mammalian cerebral cortex, affects many cortical functions such as feature selectivity and gain. Individual pyramidal cells show stable E/I ratios in time despite fluctuating cortical activity levels. This is because when excitation increases, inhibition increases proportionally through the increased recruitment of inhibitory neurons, a phenomenon referred to as excitation-inhibition balance. However, little is known about the distribution of E/I ratios across pyramidal cells. Through their highly divergent axons, inhibitory neurons indiscriminately contact most neighbouring pyramidal cells. Is inhibition homogeneously distributed or is it individually matched to the different amounts of excitation received by distinct pyramidal cells? Here we discover that pyramidal cells in layer 2/3 of mouse primary visual cortex each receive inhibition in a similar proportion to their excitation. As a consequence, E/I ratios are equalized across pyramidal cells. This matched inhibition is mediated by parvalbumin-expressing but not somatostatin-expressing inhibitory cells and results from the independent adjustment of synapses originating from individual parvalbumin-expressing cells targeting different pyramidal cells. Furthermore, this match is activity-dependent as it is disrupted by perturbing pyramidal cell activity. Thus, the equalization of E/I ratios across pyramidal cells reveals an unexpected degree of order in the spatial distribution of synaptic strengths and indicates that the relationship between the cortex/'s two opposing forces is stabilized not only in time but also in space.},
  language = {en},
  number = {7511},
  journal = {Nature},
  author = {Xue, Mingshan and Atallah, Bassam V. and Scanziani, Massimo},
  month = jul,
  year = {2014},
  keywords = {Striate cortex,Inhibition-excitation balance},
  pages = {596-600},
  file = {articles/Xue2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/65CDI5DN/nature13321.html}
}

@article{Barral2016,
  title = {Synaptic Scaling Rule Preserves Excitatory-Inhibitory Balance and Salient Neuronal Network Dynamics},
  volume = {19},
  copyright = {\textcopyright{} 2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  doi = {10.1038/nn.4415},
  abstract = {The balance between excitation and inhibition (E\textendash{}I balance) is maintained across brain regions though the network size, strength and number of synaptic connections, and connection architecture may vary substantially. We use a culture preparation to examine the homeostatic synaptic scaling rules that produce E\textendash{}I balance and in vivo-like activity. We show that synaptic strength scales with the number of connections K as \textasciitilde{} , close to the ideal theoretical value. Using optogenetic techniques, we delivered spatiotemporally patterned stimuli to neurons and confirmed key theoretical predictions: E\textendash{}I balance is maintained, active decorrelation occurs and the spiking correlation increases with firing rate. Moreover, the trial-to-trial response variability decreased during stimulation, as observed in vivo. These results\textemdash{}obtained in generic cultures, predicted by theory and observed in the intact brain\textemdash{}suggest that the synaptic scaling rule and resultant dynamics are emergent properties of networks in general.},
  language = {en},
  number = {12},
  journal = {Nature Neuroscience},
  author = {Barral, J\'er\'emie and Reyes, Alex D.},
  month = dec,
  year = {2016},
  keywords = {Dynamical systems,Network models,Neural circuits},
  pages = {1690-1696},
  file = {articles/Barral2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/3SPJ9CCQ/nn.4415.html}
}

@article{Takahashi2016,
  title = {Active Cortical Dendrites Modulate Perception},
  volume = {354},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aah6066},
  language = {en},
  number = {6319},
  journal = {Science},
  author = {Takahashi, Naoya and Oertner, Thomas G. and Hegemann, Peter and Larkum, Matthew E.},
  month = dec,
  year = {2016},
  pages = {1587-1590},
  file = {articles/Takahashi2016.pdf}
}

@article{Overath2015,
  title = {The Cortical Analysis of Speech-Specific Temporal Structure Revealed by Responses to Sound Quilts},
  volume = {18},
  copyright = {\textcopyright{} 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  doi = {10.1038/nn.4021},
  abstract = {Speech contains temporal structure that the brain must analyze to enable linguistic processing. To investigate the neural basis of this analysis, we used sound quilts, stimuli constructed by shuffling segments of a natural sound, approximately preserving its properties on short timescales while disrupting them on longer scales. We generated quilts from foreign speech to eliminate language cues and manipulated the extent of natural acoustic structure by varying the segment length. Using functional magnetic resonance imaging, we identified bilateral regions of the superior temporal sulcus (STS) whose responses varied with segment length. This effect was absent in primary auditory cortex and did not occur for quilts made from other natural sounds or acoustically matched synthetic sounds, suggesting tuning to speech-specific spectrotemporal structure. When examined parametrically, the STS response increased with segment length up to \textasciitilde{}500 ms. Our results identify a locus of speech analysis in human auditory cortex that is distinct from lexical, semantic or syntactic processes.},
  language = {en},
  number = {6},
  journal = {Nature Neuroscience},
  author = {Overath, Tobias and McDermott, Josh H. and Zarate, Jean Mary and Poeppel, David},
  month = jun,
  year = {2015},
  keywords = {Psychology,Cortex,Perception,language},
  pages = {903-911},
  file = {articles/Overath2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/4GENSKCI/nn.4021.html}
}

@article{DelPapa2017,
  title = {Criticality Meets Learning: {{Criticality}} Signatures in a Self-Organizing Recurrent Neural Network},
  volume = {12},
  issn = {1932-6203},
  shorttitle = {Criticality Meets Learning},
  doi = {10.1371/journal.pone.0178683},
  language = {en},
  number = {5},
  journal = {PLOS ONE},
  author = {Del Papa, Bruno and Priesemann, Viola and Triesch, Jochen},
  editor = {Chialvo, Dante R.},
  month = may,
  year = {2017},
  pages = {e0178683},
  file = {articles/Del Papa2017.pdf}
}

@article{Hassabis2017,
  title = {Neuroscience-{{Inspired Artificial Intelligence}}},
  volume = {95},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2017.06.011},
  language = {English},
  number = {2},
  journal = {Neuron},
  author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
  month = jul,
  year = {2017},
  keywords = {Brain,cognition,learning,neural network,artificial intelligence},
  pages = {245-258},
  file = {articles/Hassabis2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/F8D353H4/S0896-6273(17)30509-3.html}
}

@article{Topalidou2015,
  title = {A Long Journey into Reproducible Computational Neuroscience},
  volume = {9},
  issn = {1662-5188},
  doi = {10.3389/fncom.2015.00030},
  abstract = {A long journey into reproducible computational neuroscience},
  language = {English},
  journal = {Frontiers in Computational Neuroscience},
  author = {Topalidou, Meropi and Leblois, Arthur and Boraud, Thomas and Rougier, Nicolas P.},
  year = {2015},
  keywords = {reproducible science,publication process,Computational models,python language,Version control,public repository,Notebook},
  file = {articles/Topalidou2015.pdf}
}

@article{Wilson2014,
  title = {Best {{Practices}} for {{Scientific Computing}}},
  volume = {12},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1001745},
  abstract = {We describe a set of best practices for scientific software development, based on research and experience, that will improve scientists' productivity and the reliability of their software.},
  number = {1},
  journal = {PLOS Biology},
  author = {Wilson, Greg and Aruliah, D. A. and Brown, C. Titus and Hong, Neil P. Chue and Davis, Matt and Guy, Richard T. and Haddock, Steven H. D. and Huff, Kathryn D. and Mitchell, Ian M. and Plumbley, Mark D. and Waugh, Ben and White, Ethan P. and Wilson, Paul},
  month = jan,
  year = {2014},
  keywords = {Computer software,Scientists,Software development,Software tools,Programming languages,Computers,Research validity,Open source software},
  pages = {e1001745},
  file = {articles/Wilson2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/AGUNBWRN/article.html}
}

@article{DiTommaso2017,
  title = {Nextflow Enables Reproducible Computational Workflows},
  volume = {35},
  copyright = {\textcopyright{} 2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1087-0156},
  doi = {10.1038/nbt.3820},
  abstract = {To the Editor:
The increasing complexity of readouts for omics analyses goes hand-in-hand with concerns about the reproducibility of experiments that analyze 'big data'. When analyzing very large data sets, the main source of computational irreproducibility arises from a lack of good practice pertaining to software and database usage},
  language = {en},
  number = {4},
  journal = {Nature Biotechnology},
  author = {Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W. and Barja, Pablo Prieto and Palumbo, Emilio and Notredame, Cedric},
  month = apr,
  year = {2017},
  keywords = {Computational biology and bioinformatics,Data publication and archiving},
  pages = {316-319},
  file = {articles/Di Tommaso2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/7D3NUPIT/nbt.3820.html}
}

@article{Munafo2017,
  title = {A Manifesto for Reproducible Science},
  volume = {1},
  copyright = {2017 Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0021},
  abstract = {$<$p$>$Leading voices in the reproducibility landscape call for the adoption of measures to optimize key elements of the scientific process.$<$/p$>$},
  language = {en},
  number = {1},
  journal = {Nature Human Behaviour},
  author = {Munaf\`o, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and du Sert, Nathalie Percie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  month = jan,
  year = {2017},
  pages = {s41562-016-0021-016},
  file = {articles/Munafò2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/SXGIWWPN/s41562-016-0021.html}
}

@article{OConnor2017,
  title = {The {{Dockstore}}: Enabling Modular, Community-Focused Sharing of {{Docker}}-Based Genomics Tools and Workflows},
  volume = {6},
  issn = {2046-1402},
  shorttitle = {The {{Dockstore}}},
  doi = {10.12688/f1000research.10137.1},
  language = {en},
  journal = {F1000Research},
  author = {O'Connor, Brian D. and Yuen, Denis and Chung, Vincent and Duncan, Andrew G. and Liu, Xiang Kun and Patricia, Janice and Paten, Benedict and Stein, Lincoln and Ferretti, Vincent},
  month = jan,
  year = {2017},
  pages = {52},
  file = {articles/O'Connor2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/2Q9U9H3N/v1.html}
}

@article{Ludaescher2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.09958},
  primaryClass = {cs},
  title = {Capturing the "{{Whole Tale}}" of {{Computational Research}}: {{Reproducibility}} in {{Computing Environments}}},
  shorttitle = {Capturing the "{{Whole Tale}}" of {{Computational Research}}},
  abstract = {We present an overview of the recently funded "Merging Science and Cyberinfrastructure Pathways: The Whole Tale" project (NSF award \#1541450). Our approach has two nested goals: 1) deliver an environment that enables researchers to create a complete narrative of the research process including exposure of the data-to-publication lifecycle, and 2) systematically and persistently link research publications to their associated digital scholarly objects such as the data, code, and workflows. To enable this, Whole Tale will create an environment where researchers can collaborate on data, workspaces, and workflows and then publish them for future adoption or modification. Published data and applications will be consumed either directly by users using the Whole Tale environment or can be integrated into existing or future domain Science Gateways.},
  journal = {arXiv:1610.09958 [cs]},
  author = {Ludaescher, Bertram and Chard, Kyle and Gaffney, Niall and Jones, Matthew B. and Nabrzyski, Jaroslaw and Stodden, Victoria and Turk, Matthew},
  month = oct,
  year = {2016},
  keywords = {Computer Science - Digital Libraries},
  file = {articles/Ludaescher2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/XFJ52RKN/1610.html}
}

@article{Sandve2013,
  title = {Ten {{Simple Rules}} for {{Reproducible Computational Research}}},
  volume = {9},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003285},
  language = {en},
  number = {10},
  journal = {PLoS Computational Biology},
  author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
  editor = {Bourne, Philip E.},
  month = oct,
  year = {2013},
  pages = {e1003285},
  file = {articles/Sandve2013.PDF}
}

@book{Stodden2014,
  title = {Implementing Reproducible Research},
  isbn = {978-1-4665-6160-1},
  language = {English},
  author = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D and {CRC Press}},
  year = {2014},
  file = {books/Stodden2014_Implementing-reproducible-research.pdf},
  note = {OCLC: 948565410}
}

@article{Rostami2017,
  title = {[{{Re}}] {{Spike Synchronization}} and {{Rate Modulation Differentially Involved}} in {{Motor Cortical Function}}},
  volume = {3},
  doi = {10.5281/zenodo.583814},
  abstract = {A reference implementation of Spike synchronization and rate modulation di erentially involved in motor cortical function. Alexa Riehle, Sonja Gr\"un, Markus Diesmann, and Ad Aertsen (1997) Science 278:1950-1953. DOI:10.1126/science.278.5345.19 50},
  number = {1},
  journal = {ReScience},
  author = {Rostami, Vahid and Ito, Junji and Denker, Michael and Gr\"un, Sonja},
  month = may,
  year = {2017},
  keywords = {Spike time synchrony; Unitary Events method ; Python},
  file = {articles/Rostami2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/C4UKRNNJ/583814.html}
}

@article{Elliott2009,
  title = {The {{Modulation Transfer Function}} for {{Speech Intelligibility}}},
  volume = {5},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000302},
  abstract = {Author Summary The sound signal of speech is rich in temporal and frequency patterns. These fluctuations of power in time and frequency are called modulations. Despite their acoustic complexity, spoken words remain intelligible after drastic degradations in either time or frequency. To fully understand the perception of speech and to be able to reduce speech to its most essential components, we need to completely characterize how modulations in amplitude and frequency contribute together to the comprehensibility of speech. Hallmark research distorted speech in either time or frequency but described the arbitrary manipulations in terms limited to one domain or the other, without quantifying the remaining and missing portions of the signal. Here, we use a novel sound filtering technique to systematically investigate the joint features in time and frequency that are crucial for understanding speech. Both the modulation-filtering approach and the resulting characterization of speech have the potential to change the way that speech is compressed in audio engineering and how it is processed in medical applications such as cochlear implants.},
  number = {3},
  journal = {PLOS Computational Biology},
  author = {Elliott, Taffeta M. and Theunissen, Fr\'ed\'eric E.},
  month = mar,
  year = {2009},
  keywords = {Signal filtering,Speech,Speech signal processing,Audio signal processing,Modulation,Bandwidth (signal processing),Bioacoustics,Acoustic signals},
  pages = {e1000302},
  file = {articles/Elliott2009.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/43VS5X52/article.html}
}

@article{Kim2017,
  title = {Experimenting with Reproducibility in Bioinformatics},
  copyright = {\textcopyright{} 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  doi = {10.1101/143503},
  abstract = {Reproducibility or replication has been shown to be limited in many scientific fields. This question is a fundamental tenet of the scientific activity, but the related issues of reusability of scientific data are poorly documented. Here, we present a case study of our attempt to reproduce a bioinformatics method and illustrate the challenges to use a published method for which code and data were available. From this example, we address the difficulties that pave the way towards reproducibility and propose some recommendations to the research community to improve the reusability of the data.},
  language = {en},
  journal = {bioRxiv},
  author = {Kim, Yang-Min and Poline, Jean-Baptiste and Dumas, Guillaume},
  month = jun,
  year = {2017},
  pages = {143503},
  file = {articles/Kim2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/6NE2QD5B/143503.html}
}

@book{Doya2007,
  address = {Cambridge, Mass},
  series = {Computational neuroscience},
  title = {Bayesian Brain: Probabilistic Approaches to Neural Coding},
  isbn = {978-0-262-04238-3},
  lccn = {QP376 .B39 2007},
  shorttitle = {Bayesian Brain},
  publisher = {{MIT Press}},
  editor = {Doya, Kenji},
  year = {2007},
  keywords = {Brain,Neurons,Bayesian statistical decision theory},
  file = {books/Doya2007_Bayesian-brain-probabilistic-approaches-to-neural-coding.pdf}
}

@article{Wolf2014,
  title = {Dynamical Models of Cortical Circuits},
  volume = {25},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.01.017},
  language = {en},
  journal = {Current Opinion in Neurobiology},
  author = {Wolf, Fred and Engelken, Rainer and {Puelma-Touzel}, Maximilian and Weidinger, Juan Daniel Fl\'orez and Neef, Andreas},
  month = apr,
  year = {2014},
  pages = {228-236},
  file = {articles/Wolf2014.pdf}
}

@article{Vegue2017a,
  title = {On {{The Structure Of Cortical Micro}}-{{Circuits Inferred From Small Sample Sizes}}},
  copyright = {\textcopyright{} 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  doi = {10.1101/118471},
  abstract = {$<$p$>$The connectivity of cortical micro-circuits exhibits features which are inconsistent with a simple random network. Here we show that several classes of network models can account for this non-random structure despite qualitative differences in their global properties. This apparent paradox is a consequence of the small numbers of simultaneously recorded neurons in experiment: when inferred via small sample sizes many networks may be indistinguishable, despite being globally distinct. We develop a connectivity measure which successfully classifies networks even when estimated locally, with a few neurons at a time. We show that data from rat cortex is consistent with a network in which the likelihood of a connection between neurons depends on spatial distance and on non-spatial, asymmetric clustering.$<$/p$>$},
  language = {en},
  journal = {bioRxiv},
  author = {Vegue, Marina and Perin, Rodrigo and Roxin, Alex},
  month = apr,
  year = {2017},
  pages = {118471},
  file = {articles/Vegue2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/E7ID2I87/118471.html}
}

@article{Titley2017,
  title = {Toward a {{Neurocentric View}} of {{Learning}}},
  volume = {95},
  issn = {08966273},
  doi = {10.1016/j.neuron.2017.05.021},
  language = {en},
  number = {1},
  journal = {Neuron},
  author = {Titley, Heather K. and Brunel, Nicolas and Hansel, Christian},
  month = jul,
  year = {2017},
  pages = {19-32},
  file = {articles/Titley2017.pdf}
}

@article{Poirazi2001,
  title = {Impact of {{Active Dendrites}} and {{Structural Plasticity}} on the {{Memory Capacity}} of {{Neural Tissue}}},
  volume = {29},
  issn = {0896-6273},
  doi = {10.1016/S0896-6273(01)00252-5},
  abstract = {We consider the combined effects of active dendrites and structural plasticity on the storage capacity of neural tissue. We compare capacity for two different modes of dendritic integration: (1) linear, where synaptic inputs are summed across the entire dendritic arbor, and (2) nonlinear, where each dendritic compartment functions as a separately thresholded neuron-like summing unit. We calculate much larger storage capacities for cells with nonlinear subunits and show that this capacity is accessible to a structural learning rule that combines random synapse formation with activity-dependent stabilization/elimination. In a departure from the common view that memories are encoded in the overall connection strengths between neurons, our results suggest that long-term information storage in neural tissue could reside primarily in the selective addressing of synaptic contacts onto dendritic subunits.},
  number = {3},
  journal = {Neuron},
  author = {Poirazi, Panayiota and Mel, Bartlett W.},
  month = mar,
  year = {2001},
  pages = {779-796},
  file = {articles/Poirazi2001.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/A3KFKADW/S0896627301002525.html}
}

@article{Chen2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.02583},
  primaryClass = {cs},
  title = {A {{Gentle Tutorial}} of {{Recurrent Neural Network}} with {{Error Backpropagation}}},
  abstract = {We describe recurrent neural networks (RNNs), which have attracted great attention on sequential tasks, such as handwriting recognition, speech recognition and image to text. However, compared to general feedforward neural networks, RNNs have feedback loops, which makes it a little hard to understand the backpropagation step. Thus, we focus on basics, especially the error backpropagation to compute gradients with respect to model parameters. Further, we go into detail on how error backpropagation algorithm is applied on long short-term memory (LSTM) by unfolding the memory unit.},
  journal = {arXiv:1610.02583 [cs]},
  author = {Chen, Gang},
  month = oct,
  year = {2016},
  keywords = {Computer Science - Learning},
  file = {articles/Chen2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/FGNNPQUT/1610.html}
}

@article{Mishra2016,
  title = {Symmetric Spike Timing-Dependent Plasticity at {{CA3}}\textendash{{CA3}} Synapses Optimizes Storage and Recall in Autoassociative Networks},
  volume = {7},
  issn = {2041-1723},
  doi = {10.1038/ncomms11552},
  journal = {Nature Communications},
  author = {Mishra, Rajiv K. and Kim, Sooyun and Guzman, Segundo J. and Jonas, Peter},
  month = may,
  year = {2016},
  pages = {11552},
  file = {articles/Mishra2016.pdf}
}

@article{Jang2017,
  title = {Human {{Cortical Neurons}} in the {{Anterior Temporal Lobe Reinstate Spiking Activity}} during {{Verbal Memory Retrieval}}},
  volume = {27},
  issn = {09609822},
  doi = {10.1016/j.cub.2017.05.014},
  language = {en},
  number = {11},
  journal = {Current Biology},
  author = {Jang, Anthony I. and Wittig, John H. and Inati, Sara K. and Zaghloul, Kareem A.},
  month = jun,
  year = {2017},
  pages = {1700-1705.e5},
  file = {articles/Jang2017.pdf}
}

@article{Rougier2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.04393},
  primaryClass = {cs},
  title = {Sustainable Computational Science: The {{ReScience}} Initiative},
  shorttitle = {Sustainable Computational Science},
  abstract = {Computer science offers a large set of tools for prototyping, writing, running, testing, validating, sharing and reproducing results, however computational science lags behind. In the best case, authors may provide their source code as a compressed archive and they may feel confident their research is reproducible. But this is not exactly true. James Buckheit and David Donoho proposed more than two decades ago that an article about computational results is advertising, not scholarship. The actual scholarship is the full software environment, code, and data that produced the result. This implies new workflows, in particular in peer-reviews. Existing journals have been slow to adapt: source codes are rarely requested, hardly ever actually executed to check that they produce the results advertised in the article. ReScience is a peer-reviewed journal that targets computational research and encourages the explicit replication of already published research, promoting new and open-source implementations in order to ensure that the original research can be replicated from its description. To achieve this goal, the whole publishing chain is radically different from other traditional scientific journals. ReScience resides on GitHub where each new implementation of a computational study is made available together with comments, explanations, and software tests.},
  journal = {arXiv:1707.04393 [cs]},
  author = {Rougier, Nicolas P. and Hinsen, Konrad and Alexandre, Fr\'ed\'eric and Arildsen, Thomas and Barba, Lorena and Benureau, Fabien C. Y. and Brown, C. Titus and {de Buyl}, Pierre and Caglayan, Ozan and Davison, Andrew P. and Delsuc, Marc Andr\'e and Detorakis, Georgios and Diem, Alexandra K. and Drix, Damien and Enel, Pierre and Girard, Beno\^it and Guest, Olivia and Hall, Matt G. and Henriques, Rafael Neto and Hinaut, Xavier and Jaron, Kamil S. and Khamassi, Mehdi and Klein, Almar and Manninen, Tiina and Marchesi, Pietro and McGlinn, Dan and Metzner, Christoph and Petchey, Owen L. and Plesser, Hans Ekkehard and Poisot, Timoth\'ee and Ram, Karthik and Ram, Yoav and Roesch, Etienne and Rossant, Cyrille and Rostami, Vahid and Shifman, Aaron and Stachelek, Joseph and Stimberg, Marcel and Stollmeier, Frank and Vaggi, Federico and Viejo, Guillaume and Vitay, Julien and Vostinar, Anya and Yurchak, Roman and Zito, Tiziano},
  month = jul,
  year = {2017},
  keywords = {Computer Science - Digital Libraries},
  file = {articles/Rougier2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/HIGVJJ3K/1707.html}
}

@article{Iacaruso2017,
  title = {Synaptic Organization of Visual Space in Primary Visual Cortex},
  volume = {547},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature23019},
  number = {7664},
  journal = {Nature},
  author = {Iacaruso, M. Florencia and Gasler, Ioana T. and Hofer, Sonja B.},
  month = jul,
  year = {2017},
  pages = {449-452},
  file = {articles/Iacaruso2017.pdf}
}

@article{Cocchi2017,
  title = {Criticality in the Brain: {{A}} Synthesis of Neurobiology, Models and Cognition},
  issn = {0301-0082},
  shorttitle = {Criticality in the Brain},
  doi = {10.1016/j.pneurobio.2017.07.002},
  abstract = {Cognitive function requires the coordination of neural activity across many scales, from neurons and circuits to large-scale networks. As such, it is unlikely that an explanatory framework focused upon any single scale will yield a comprehensive theory of brain activity and cognitive function. Modelling and analysis methods for neuroscience should aim to accommodate multiscale phenomena. Emerging research now suggests that multi-scale processes in the brain arise from so-called critical phenomena that occur very broadly in the natural world. Criticality arises in complex systems perched between order and disorder, and is marked by fluctuations that do not have any privileged spatial or temporal scale. We review the core nature of criticality, the evidence supporting its role in neural systems and its explanatory potential in brain health and disease.},
  journal = {Progress in Neurobiology},
  author = {Cocchi, Luca and Gollo, Leonardo L. and Zalesky, Andrew and Breakspear, Michael},
  month = jul,
  year = {2017},
  keywords = {cognition,Bifurcations,metastability,multistability,dynamics,power-law},
  file = {articles/Cocchi2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/MBAP2MJ4/S0301008216301630.html}
}

@article{Kleberg2018,
  author = {Kleberg, Florence},
  year = {2018},
  file = {articles/Kleberg2017.pdf;articles/Kleberg2018.pdf}
}

@article{Morrison2008,
  title = {Phenomenological Models of Synaptic Plasticity Based on Spike Timing},
  volume = {98},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-008-0233-1},
  language = {en},
  number = {6},
  journal = {Biological Cybernetics},
  author = {Morrison, Abigail and Diesmann, Markus and Gerstner, Wulfram},
  month = jun,
  year = {2008},
  pages = {459-478},
  file = {articles/Morrison2008.pdf}
}

@article{Steyvers2004,
  title = {Word Association Spaces for Predicting Semantic Similarity Effects in Episodic Memory},
  journal = {Experimental cognitive psychology and its applications: Festschrift in honor of Lyle Bourne, Walter Kintsch, and Thomas Landauer},
  author = {Steyvers, Mark and Shiffrin, Richard M and Nelson, Douglas L},
  year = {2004},
  pages = {237--249},
  file = {articles/Steyvers2004.pdf}
}

@inproceedings{Karklin2011,
  title = {Efficient Coding of Natural Images with a Population of Noisy Linear-Nonlinear Neurons},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Karklin, Yan and Simoncelli, Eero P},
  year = {2011},
  pages = {999--1007},
  file = {conferences/Karklin2011_Efficient-coding-of-natural-images-with-a-population-of-noisy-linear-nonlinear-neurons.pdf}
}

@article{Gal2017a,
  title = {Rich Cell-Type-Specific Network Topology in Neocortical Microcircuitry},
  volume = {20},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4576},
  number = {7},
  journal = {Nature Neuroscience},
  author = {Gal, Eyal and London, Michael and Globerson, Amir and Ramaswamy, Srikanth and Reimann, Michael W and Muller, Eilif and Markram, Henry and Segev, Idan},
  month = jun,
  year = {2017},
  pages = {1004-1013},
  file = {articles/Gal2017.pdf}
}

@article{Tosi2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.00133},
  primaryClass = {q-bio},
  title = {Cortical {{Circuits}} from {{Scratch}}: {{A Metaplastic Architecture}} for the {{Emergence}} of {{Lognormal Firing Rates}} and {{Realistic Topology}}},
  shorttitle = {Cortical {{Circuits}} from {{Scratch}}},
  abstract = {Our current understanding of neuroplasticity paints a picture of a complex interconnected system of dependent processes, which shape cortical structure so as to produce an efficient information processing system. Indeed, the cooperation of these processes is associated with robust, stable, adaptable networks with characteristic features of activity and synaptic topology. However, combining the actions of these mechanisms in models has proven exceptionally difficult and to date no model has been able to do so without significant hand-tuning. Until such a model exists that can successfully combine these mechanisms to form a stable circuits with realistic features, our ability to study neuroplasticity in the context of (more realistic) dynamic networks and potentially reap whatever rewards these features and mechanisms imbue biological networks with is hindered. We introduce a model which combines five known plasticity mechanisms that act on the network as well as a unique metaplastic mechanism which acts on other plasticity mechanisms, to produce a neural circuit model which is both stable and capable of broadly reproducing many characteristic features of cortical networks. The MANA (metaplastic artificial neural architecture) represents the first model of its kind in that it is able to self-organize realistic, nonrandom features of cortical networks, from a null initial state (no synaptic connectivity or neuronal differentiation) with no hand-tuning of relevant variables. In the same vein as models like the SORN (self-organizing recurrent network) MANA represents further progress toward the reverse engineering of the brain at the network level.},
  journal = {arXiv:1706.00133 [q-bio]},
  author = {Tosi, Zach and Beggs, John},
  month = may,
  year = {2017},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {articles/Tosi2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/4DKCD5UM/1706.html}
}

@article{Sweeney2017,
  title = {Emergent Spatial Synaptic Structure from Diffusive Plasticity},
  volume = {45},
  issn = {0953816X},
  doi = {10.1111/ejn.13279},
  language = {en},
  number = {8},
  journal = {European Journal of Neuroscience},
  author = {Sweeney, Yann and Clopath, Claudia},
  editor = {Poirazi, Panayiota},
  month = apr,
  year = {2017},
  pages = {1057-1067},
  file = {articles/Sweeney2017.pdf}
}

@book{Arnold1992,
  address = {Berlin},
  edition = {3rd ed.},
  title = {Ordinary Differential Equations},
  isbn = {978-3-540-54813-3 978-0-387-54813-5},
  lccn = {QA372 .A713 1992},
  language = {eng},
  publisher = {{New York : Springer-Verlag}},
  author = {Arnol'd, V. I.},
  year = {1992},
  keywords = {Differential equations},
  file = {books/Arnolʹd1992_Ordinary-differential-equations.djvu}
}

@article{Marr1976,
  title = {From {{Understanding Computation}} to {{Understanding Neural Circuitry}}},
  abstract = {The CNS needs to be understood at four nearly independent levels of description: (1) that at which the nature of computation is expressed; (2) that at which the algorithms that implement a computation are characterized; (3) that at which an algorithm is committed to particular mechanisms; and (4) that at which the mechanisms are realized in hardware. In general, the nature of a computation is determined by the problem to be solved, the mechanisms that are used depend upon the available hardware, and the particular algorithms chosen depend on the problem and on the available mechanisms. Examples are given of theories at each level.},
  language = {en\_US},
  author = {Marr, D. and Poggio, T.},
  month = may,
  year = {1976},
  file = {articles/Marr1976.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/T9K5393F/5782.html}
}

@book{Hirsch1974,
  address = {New York},
  series = {Pure and applied mathematics; a series of monographs and textbooks},
  title = {Differential Equations, Dynamical Systems, and Linear Algebra},
  isbn = {978-0-12-349550-1},
  lccn = {QA3 QA372 .P8 vol. 60},
  number = {v. 60},
  publisher = {{Academic Press}},
  author = {Hirsch, Morris W. and Smale, Stephen},
  year = {1974},
  keywords = {Differential equations,Algebras; Linear},
  file = {books/Hirsch1974_Differential-equations,-dynamical-systems,-and-linear-algebra.pdf}
}

@book{Stein2003,
  address = {Princeton},
  series = {Princeton lectures in analysis},
  title = {Fourier Analysis: An Introduction},
  isbn = {978-0-691-11384-5},
  lccn = {QA403.5 .S74 2003},
  shorttitle = {Fourier Analysis},
  number = {1},
  publisher = {{Princeton University Press}},
  author = {Stein, Elias M. and Shakarchi, Rami},
  year = {2003},
  keywords = {Fourier analysis},
  file = {books/Stein2003_Fourier-analysis-an-introduction.pdf}
}

@book{Marr2010,
  address = {Cambridge, Mass},
  title = {Vision: A Computational Investigation into the Human Representation and Processing of Visual Information},
  isbn = {978-0-262-51462-0},
  lccn = {QP475 .M27 2010},
  shorttitle = {Vision},
  publisher = {{MIT Press}},
  author = {Marr, David},
  year = {2010},
  keywords = {Data processing,Mathematical models,Vision,Human information processing},
  file = {books/Marr2010_Vision-a-computational-investigation-into-the-human-representation-and-processing-of-visual-information.pdf},
  note = {OCLC: ocn472791457}
}

@article{Costa2017,
  title = {Synaptic {{Transmission Optimization Predicts Expression Loci}} of {{Long}}-{{Term Plasticity}}},
  volume = {96},
  issn = {08966273},
  doi = {10.1016/j.neuron.2017.09.021},
  language = {en},
  number = {1},
  journal = {Neuron},
  author = {Costa, Rui Ponte and Padamsey, Zahid and D'Amour, James A. and Emptage, Nigel J. and Froemke, Robert C. and Vogels, Tim P.},
  month = sep,
  year = {2017},
  pages = {177-189.e7},
  file = {articles/Costa3.pdf}
}

@article{Kusmierz2017,
  title = {Learning with Three Factors: Modulating {{Hebbian}} Plasticity with Errors},
  volume = {46},
  issn = {0959-4388},
  shorttitle = {Learning with Three Factors},
  doi = {10.1016/j.conb.2017.08.020},
  abstract = {Synaptic plasticity is a central theme in neuroscience. A framework of three-factor learning rules provides a powerful abstraction, helping to navigate through the abundance of models of synaptic plasticity. It is well-known that the dopamine modulation of learning is related to reward, but theoretical models predict other functional roles of the modulatory third factor; it may encode errors for supervised learning, summary statistics of the population activity for unsupervised learning or attentional feedback. Specialized structures may be needed in order to generate and propagate third factors in the neural network.},
  number = {Supplement C},
  journal = {Current Opinion in Neurobiology},
  author = {Ku\'smierz, \L{}ukasz and Isomura, Takuya and Toyoizumi, Taro},
  month = oct,
  year = {2017},
  pages = {170-177},
  file = {articles/Kuśmierz2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/NWWVGA69/S0959438817300612.html}
}

@book{InternationalConferenceonTheoryandApplicationinNonlinearDynamics2014,
  title = {International {{Conference}} on {{Theory}} and {{Application}} in {{Nonlinear Dynamics}} ({{ICAND}} 2012)},
  isbn = {978-3-319-02925-2 978-3-319-02924-5},
  abstract = {A collection of different lectures presented by experts in the field of nonlinear science provides the reader with contemporary, cutting-edge, research works that bridge the gap between theory and device realizations of nonlinear phenomena. Representative examples of topics covered include: chaos gates, social networks, communication, sensors, lasers, molecular motors, biomedical anomalies, stochastic resonance, nano-oscillators for generating microwave signals and related complex systems. A common theme among these and many other related lectures is to model, study, understand, and exploit the rich behavior exhibited by nonlinear systems to design and fabricate novel technologies with superior characteristics. Consider, for instance, the fact that a sharks sensitivity to electric fields is 400 times more powerful than the most sophisticated electric-field sensor. In spite of significant advances in material properties, in many cases it remains a daunting task to duplicate the superior signal processing capabilities of most animals. Since nonlinear systems tend to be highly sensitive to perturbations when they occur near the onset of a bifurcation, there are also lectures on the general topic of bifurcation theory and on how to exploit such bifurcations for signal enhancements purposes. This manuscript will appeal to researchers interested in both theory and implementations of nonlinear systems.},
  language = {English},
  author = {{International Conference on Theory and Application in Nonlinear Dynamics} and In, Visarath and Palacios, Antonio and Longhini, Patrick},
  year = {2014},
  file = {books/In2014_International-Conference-on-Theory-and-Application-in-Nonlinear-Dynamics-(ICAND-2012).pdf},
  note = {OCLC: 873948185}
}

@article{Rubin2015,
  title = {The {{Stabilized Supralinear Network}}: {{A Unifying Circuit Motif Underlying Multi}}-{{Input Integration}} in {{Sensory Cortex}}},
  volume = {85},
  issn = {0896-6273},
  shorttitle = {The {{Stabilized Supralinear Network}}},
  doi = {10.1016/j.neuron.2014.12.026},
  abstract = {Summary
Neurons in sensory cortex integrate multiple influences to parse objects and support perception. Across multiple cortical areas, integration is characterized by two neuronal response properties: (1) surround suppression\textemdash{}modulatory contextual stimuli suppress responses to driving stimuli; and (2) ``normalization''\textemdash{}responses to multiple driving stimuli add sublinearly. These depend on input strength: for weak driving stimuli, contextual influences facilitate or more weakly suppress and summation becomes linear or supralinear. Understanding the circuit operations underlying integration is critical to understanding cortical function and disease. We present a simple, general theory. A wealth of integrative properties, including the above, emerge robustly from four cortical circuit properties: (1) supralinear neuronal input/output functions; (2) sufficiently strong recurrent excitation; (3) feedback inhibition; and (4) simple spatial properties of intracortical connections. Integrative properties emerge dynamically as circuit properties, with excitatory and inhibitory neurons showing similar behaviors. In new recordings in visual cortex, we confirm key model predictions.},
  number = {2},
  journal = {Neuron},
  author = {Rubin, Daniel B. and Van Hooser, Stephen D. and Miller, Kenneth D.},
  month = jan,
  year = {2015},
  pages = {402-417},
  file = {articles/Rubin2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/BHNYIKCQ/S0896627314011350.html}
}

@article{Adesnik2017,
  title = {Synaptic {{Mechanisms}} of {{Feature Coding}} in the {{Visual Cortex}} of {{Awake Mice}}},
  volume = {95},
  issn = {08966273},
  doi = {10.1016/j.neuron.2017.08.014},
  language = {en},
  number = {5},
  journal = {Neuron},
  author = {Adesnik, Hillel},
  month = aug,
  year = {2017},
  pages = {1147-1159.e4},
  file = {articles/Adesnik2017.pdf}
}

@article{Sussillo2009,
  title = {Generating {{Coherent Patterns}} of {{Activity}} from {{Chaotic Neural Networks}}},
  volume = {63},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2009.07.018},
  abstract = {Summary
Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
  number = {4},
  journal = {Neuron},
  author = {Sussillo, David and Abbott, L. F.},
  month = aug,
  year = {2009},
  keywords = {SYSNEURO},
  pages = {544-557},
  file = {articles/Sussillo2009.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/57CXVZVJ/S0896627309005479.html}
}

@article{Benson2007,
  title = {Recurrence of Extreme Events with Power-Law Interarrival Times},
  volume = {34},
  issn = {1944-8007},
  doi = {10.1029/2007GL030767},
  abstract = {In various geophysical applications, power-law interarrival times are observed between extreme events. Classical extreme value theory is based on exponentially distributed interarrivals and can not be applied to these processes. We solve for the density of the maxima of a sequence of random extreme events with any distribution of random interarrivals by applying a continuous time random max model, similar to a random walk model. The equation is exact when the distributions of the exceedances and the interarrivals are known. If only the tail properties of the exceedances and interarrivals can be estimated, then limiting extreme value distributions governing the maximum observation or exceedance are used. The general extreme value densities are obtained by transforming the classical extreme value distributions via subordination. This new class of extreme value densities can be used to obtain recurrence intervals for extreme events with power-law interarrivals.},
  language = {en},
  number = {16},
  journal = {Geophysical Research Letters},
  author = {Benson, David A. and Schumer, Rina and Meerschaert, Mark M.},
  month = aug,
  year = {2007},
  keywords = {1817 Extreme events,4468 Probability distributions; heavy and fat-tailed,7223 Earthquake interaction; forecasting; and prediction,extreme values,interarrivals,power law},
  pages = {L16404},
  file = {articles/Benson2007.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/NG46GDWT/abstract.html}
}

@article{Ross-Hellauer2017a,
  title = {What Is Open Peer Review? {{A}} Systematic Review},
  volume = {6},
  issn = {2046-1402},
  shorttitle = {What Is Open Peer Review?},
  doi = {10.12688/f1000research.11369.1},
  language = {en},
  journal = {F1000Research},
  author = {{Ross-Hellauer}, Tony},
  month = apr,
  year = {2017},
  pages = {588},
  file = {articles/Ross-Hellauer2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/CQJ22AN4/v1.html}
}

@article{Rajan2006,
  title = {Eigenvalue {{Spectra}} of {{Random Matrices}} for {{Neural Networks}}},
  volume = {97},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.97.188104},
  language = {en},
  number = {18},
  journal = {Physical Review Letters},
  author = {Rajan, Kanaka and Abbott, L. F.},
  month = nov,
  year = {2006},
  file = {articles/Rajan2006.pdf}
}

@article{Nigam2016,
  title = {Rich-{{Club Organization}} in {{Effective Connectivity}} among {{Cortical Neurons}}},
  volume = {36},
  copyright = {Copyright \textcopyright{} 2016 Nigam et al.. This article is freely available online through the J Neurosci Author Open Choice option.},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2177-15.2016},
  abstract = {The performance of complex networks, like the brain, depends on how effectively their elements communicate. Despite the importance of communication, it is virtually unknown how information is transferred in local cortical networks, consisting of hundreds of closely spaced neurons. To address this, it is important to record simultaneously from hundreds of neurons at a spacing that matches typical axonal connection distances, and at a temporal resolution that matches synaptic delays. We used a 512-electrode array (60 $\mu$m spacing) to record spontaneous activity at 20 kHz from up to 500 neurons simultaneously in slice cultures of mouse somatosensory cortex for 1 h at a time. We applied a previously validated version of transfer entropy to quantify information transfer. Similar to in vivo reports, we found an approximately lognormal distribution of firing rates. Pairwise information transfer strengths also were nearly lognormally distributed, similar to reports of synaptic strengths. Some neurons transferred and received much more information than others, which is consistent with previous predictions. Neurons with the highest outgoing and incoming information transfer were more strongly connected to each other than chance, thus forming a ``rich club.'' We found similar results in networks recorded in vivo from rodent cortex, suggesting the generality of these findings. A rich-club structure has been found previously in large-scale human brain networks and is thought to facilitate communication between cortical regions. The discovery of a small, but information-rich, subset of neurons within cortical regions suggests that this population will play a vital role in communication, learning, and memory.
SIGNIFICANCE STATEMENT Many studies have focused on communication networks between cortical brain regions. In contrast, very few studies have examined communication networks within a cortical region. This is the first study to combine such a large number of neurons (several hundred at a time) with such high temporal resolution (so we can know the direction of communication between neurons) for mapping networks within cortex. We found that information was not transferred equally through all neurons. Instead, $\sim$70\% of the information passed through only 20\% of the neurons. Network models suggest that this highly concentrated pattern of information transfer would be both efficient and robust to damage. Therefore, this work may help in understanding how the cortex processes information and responds to neurodegenerative diseases.},
  language = {en},
  number = {3},
  journal = {Journal of Neuroscience},
  author = {Nigam, Sunny and Shimono, Masanori and Ito, Shinya and Yeh, Fang-Chin and Timme, Nicholas and Myroshnychenko, Maxym and Lapish, Christopher C. and Tosi, Zachary and Hottowy, Pawel and Smith, Wesley C. and Masmanidis, Sotiris C. and Litke, Alan M. and Sporns, Olaf and Beggs, John M.},
  month = jan,
  year = {2016},
  keywords = {effective connectivity,information transfer,microcircuits,rich club,transfer entropy},
  pages = {670-684},
  file = {articles/Nigam2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/LPCIPJFN/670.html},
  pmid = {26791200}
}

@book{Papoulis1991,
  address = {New York},
  edition = {3rd ed},
  series = {McGraw-Hill series in electrical engineering},
  title = {Probability, Random Variables, and Stochastic Processes},
  isbn = {978-0-07-048477-1},
  lccn = {QA273 .P2 1991},
  publisher = {{McGraw-Hill}},
  author = {Papoulis, Athanasios},
  year = {1991},
  keywords = {Stochastic processes,Random variables,Probabilities},
  file = {books/Papoulis1991_Probability,-random-variables,-and-stochastic-processes.djvu;books/Papoulis1991_solutions_manual.djvu}
}

@article{Sompolinsky1988,
  title = {Chaos in {{Random Neural Networks}}},
  volume = {61},
  doi = {10.1103/PhysRevLett.61.259},
  abstract = {A continuous-time dynamic model of a network of N nonlinear elements interacting via random asymmetric couplings is studied. A self-consistent mean-field theory, exact in the N$\rightarrow\infty$ limit, predicts a transition from a stationary phase to a chaotic phase occurring at a critical value of the gain parameter. The autocorrelations of the chaotic flow as well as the maximal Lyapunov exponent are calculated.},
  number = {3},
  journal = {Physical Review Letters},
  author = {Sompolinsky, H. and Crisanti, A. and Sommers, H. J.},
  month = jul,
  year = {1988},
  pages = {259-262},
  file = {articles/Sompolinsky1988.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/RR2GQ5I6/PhysRevLett.61.html}
}

@article{Goedeke2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.01880},
  primaryClass = {nlin, q-bio},
  title = {Noise Dynamically Suppresses Chaos in Random Neural Networks},
  abstract = {Noise is ubiquitous in neural systems due to intrinsic stochasticity or external drive. For deterministic dynamics, randomly coupled neural networks display a transition to chaos at a critical coupling strength. Here, we investigate the effect of additive white noise on the onset of chaos. We develop the dynamical mean-field theory yielding the statistics of the activity and the maximum Lyapunov exponent. An exact condition determines the transition from stable to chaotic dynamics. Noise suppresses chaos by a dynamic mechanism, shifting the transition to significantly larger coupling strengths than predicted by local stability analysis. A regime emerges, where expansive dynamics and stable long-term behavior coexist. Furthermore, the time scale of the temporal correlations does not diverge at the transition, but peaks slightly above the critical coupling strength.},
  journal = {arXiv:1603.01880 [nlin, q-bio]},
  author = {Goedeke, Sven and Schuecker, Jannis and Helias, Moritz},
  month = mar,
  year = {2016},
  keywords = {Quantitative Biology - Neurons and Cognition,Nonlinear Sciences - Chaotic Dynamics},
  file = {articles/Goedeke2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/UBDLTH2N/1603.html}
}

@article{Richardson2007,
  title = {Firing-Rate Response of Linear and Nonlinear Integrate-and-Fire Neurons to Modulated Current-Based and Conductance-Based Synaptic Drive},
  volume = {76},
  doi = {10.1103/PhysRevE.76.021919},
  abstract = {Integrate-and-fire models are mainstays of the study of single-neuron response properties and emergent states of recurrent networks of spiking neurons. They also provide an analytical base for perturbative approaches that treat important biological details, such as synaptic filtering, synaptic conductance increase, and voltage-activated currents. Steady-state firing rates of both linear and nonlinear integrate-and-fire models, receiving fluctuating synaptic drive, can be calculated from the time-independent Fokker-Planck equation. The dynamic firing-rate response is less easy to extract, even at the first-order level of a weak modulation of the model parameters, but is an important determinant of neuronal response and network stability. For the linear integrate-and-fire model the response to modulations of current-based synaptic drive can be written in terms of hypergeometric functions. For the nonlinear exponential and quadratic models no such analytical forms for the response are available. Here it is demonstrated that a rather simple numerical method can be used to obtain the steady-state and dynamic response for both linear and nonlinear models to parameter modulation in the presence of current-based or conductance-based synaptic fluctuations. To complement the full numerical solution, generalized analytical forms for the high-frequency response are provided. A special case is also identified\textemdash{}time-constant modulation\textemdash{}for which the response to an arbitrarily strong modulation can be calculated exactly.},
  number = {2},
  journal = {Physical Review E},
  author = {Richardson, Magnus J. E.},
  month = aug,
  year = {2007},
  pages = {021919},
  file = {articles/Richardson2007.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/7FP6ISAU/PhysRevE.76.html}
}

@article{Kreutz-Delgado2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1501.04032},
  primaryClass = {q-bio},
  title = {Mean {{Time}}-to-{{Fire}} for the {{Noisy LIF Neuron}} - {{A Detailed Derivation}} of the {{Siegert Formula}}},
  abstract = {When stimulated by a very large number of Poisson-like presynaptic current input spikes, the temporal dynamics of the soma membrane potential \$V(t)\$ of a leaky integrate-and-fire (LIF) neuron is typically modeled in the diffusion limit and treated as a Ornstein-Uhlenbeck process (OUP). When the potential reaches a threshold value \$$\backslash$theta\$, \$V(t) = $\backslash$theta\$, the LIF neuron fires and the membrane potential is reset to a resting value, \$V\_0 $<$ $\backslash$theta\$, and clamped to this value for a specified (non-stochastic) absolute refractory period \$T\_r $\backslash$ge 0\$, after which the cycle is repeated. The time between firings is given by the random variable \$T\_f = T\_r+ T\$ where \$T\$ is the random time which elapses between the "unpinning" of the membrane potential clamp and the next, subsequent firing of the neuron. The mean time-to-fire, \$$\backslash$widehat\{T\}\_f = $\backslash$text\{E\}(T\_f) = T\_r + $\backslash$text\{E\}(T) = T\_r + $\backslash$widehat\{T\}\$, provides a measure \$$\backslash$rho\$ of the average firing rate of the neuron, $\backslash$[ $\backslash$rho = $\backslash$widehat\{T\}\_f\^\{-1\} = $\backslash$frac\{1\}\{T\_r + $\backslash$widehat\{T\}\} . $\backslash$] This note briefly discusses some aspects of the OUP model and derives the Siegert formula giving the firing rate, \$$\backslash$rho = $\backslash$rho(I\_0)\$ as a function of an injected current, \$I\_0\$. This is a well-known classical result and no claim to originality is made. The derivation of the firing rate given in this report, which closely follows the derivation outlined in the textbook by Gardiner, minimizes the required mathematical background and is done in some pedagogic detail to facilitate study by graduate students and others who are new to the subject. Knowledge of the material presented in the first five chapters of Gardiner should provide an adequate background for following the derivation given in this note.},
  journal = {arXiv:1501.04032 [q-bio]},
  author = {{Kreutz-Delgado}, Ken},
  month = jan,
  year = {2015},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {articles/Kreutz-Delgado2015.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/7LKHS7LM/1501.html}
}

@article{Tchumatchenko2011,
  title = {Representation of {{Dynamical Stimuli}} in {{Populations}} of {{Threshold Neurons}}},
  volume = {7},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002239},
  abstract = {Author Summary Sensory stimuli in our environment are represented in the brain as input current changes to neurons. For example, a periodic bar pattern in the visual field leads to periodic current modulations in the visual cortex. Therefore, models describing the ability of neurons to represent incoming stimuli can offer important clues about how sensory stimuli are processed by the brain. As anyone who has used an old-fashioned radio can attest, there is not just one but multiple ways to encode a signal, e.g. the familiar AM and FM channels. But what are the potential encoding channels in the cortex? A signal could modify the neuronal input current in two distinct ways: it could act either on the mean or the variance of the current. Using a minimal model framework, which can reproduce many features of neuronal activity, we find that both encoding schemes could be equally potent in transmitting slow and fast signals. This allows us to describe how input signals of any functional form give rise to collective firing rate changes in populations of neurons.},
  number = {10},
  journal = {PLOS Computational Biology},
  author = {Tchumatchenko, Tatjana and Wolf, Fred},
  month = oct,
  year = {2011},
  keywords = {Action potentials,Neurons,Signaling networks,Single neuron function,White noise,Dynamic response,Frequency response,Nonlinear dynamics},
  pages = {e1002239},
  file = {articles/Tchumatchenko2011.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/QR2SBQJX/article.html}
}

@article{Shoham2006,
  title = {How Silent Is the Brain: Is There a ``Dark Matter'' Problem in Neuroscience?},
  volume = {192},
  issn = {0340-7594, 1432-1351},
  shorttitle = {How Silent Is the Brain},
  doi = {10.1007/s00359-006-0117-6},
  abstract = {Evidence from a variety of recording methods suggests that many areas of the brain are far more sparsely active than commonly thought. Here, we review experimental findings pointing to the existence of neurons which fire action potentials rarely or only to very specific stimuli. Because such neurons would be difficult to detect with the most common method of monitoring neural activity in vivo\textemdash{}extracellular electrode recording\textemdash{}they could be referred to as ``dark neurons,'' in analogy to the astrophysical observation that much of the matter in the universe is undetectable, or dark. In addition to discussing the evidence for largely silent neurons, we review technical advances that will ultimately answer the question: how silent is the brain?},
  language = {en},
  number = {8},
  journal = {Journal of Comparative Physiology A},
  author = {Shoham, Shy and O'Connor, Daniel H. and Segev, Ronen},
  month = aug,
  year = {2006},
  pages = {777-784},
  file = {articles/Shoham2006.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/36K47PWA/10.html}
}

@article{Wilting2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.07035},
  primaryClass = {physics, q-bio},
  title = {Branching into the {{Unknown}}: {{Inferring}} Collective Dynamical States from Subsampled Systems},
  shorttitle = {Branching into the {{Unknown}}},
  abstract = {When studying the dynamics of complex systems, one can rarely sample the state of all components. We show that this spatial subsampling typically leads to severe underestimation of the risk of instability in systems with propagation of events. We analytically derived a subsampling-invariant estimator and applied it to non-linear network simulations and case reports of various diseases, recovering a close relation between vaccination rate and spreading behavior. The estimator can be particularly useful in countries with unreliable case reports, and promises early warning if e.g. antibiotic resistant bacteria increase their infectiousness. In neuroscience, subsampling has led to contradictory hypotheses about the collective spiking dynamics: asynchronous-irregular or critical. With the novel estimator, we demonstrated for rat, cat and monkey that collective dynamics lives in a narrow subspace between the two. Functionally, this subspace can combine the different computational properties associated with the two states.},
  journal = {arXiv:1608.07035 [physics, q-bio]},
  author = {Wilting, Jens and Priesemann, Viola},
  month = aug,
  year = {2016},
  keywords = {Quantitative Biology - Neurons and Cognition,Physics - Data Analysis; Statistics and Probability},
  file = {articles/Wilting2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/M4LNQYKS/1608.html}
}

@article{Bertschinger2004,
  title = {Real-{{Time Computation}} at the {{Edge}} of {{Chaos}} in {{Recurrent Neural Networks}}},
  volume = {16},
  issn = {0899-7667},
  doi = {10.1162/089976604323057443},
  number = {7},
  journal = {Neural Computation},
  author = {Bertschinger, Nils and Natschl\"ager, Thomas},
  month = jul,
  year = {2004},
  pages = {1413-1436},
  file = {articles/Bertschinger2004.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/R2M9DZHZ/089976604323057443.html}
}

@article{Kuang2016,
  title = {Planning {{Movements}} in {{Visual}} and {{Physical Space}} in {{Monkey Posterior Parietal Cortex}}},
  volume = {26},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhu312},
  abstract = {Neurons in the posterior parietal cortex respond selectively for spatial parameters of planned goal-directed movements. Yet, it is still unclear which aspects of the movement the neurons encode: the spatial parameters of the upcoming physical movement (physical goal), or the upcoming visual limb movement (visual goal). To test this, we recorded neuronal activity from the parietal reach region while monkeys planned reaches under either normal or prism-reversed viewing conditions. We found predominant encoding of physical goals while fewer neurons were selective for visual goals during planning. In contrast, local field potentials recorded in the same brain region exhibited predominant visual goal encoding, similar to previous imaging data from humans. The visual goal encoding in individual neurons was neither related to immediate visual input nor to visual memory, but to the future visual movement. Our finding suggests that action planning in parietal cortex is not exclusively a precursor of impending physical movements, as reflected by the predominant physical goal encoding, but also contains spatial kinematic parameters of upcoming visual movement, as reflected by co-existing visual goal encoding in neuronal spiking. The co-existence of visual and physical goals adds a complementary perspective to the current understanding of parietal spatial computations in primates.},
  number = {2},
  journal = {Cerebral Cortex},
  author = {Kuang, Shenbing and Morel, Pierre and Gail, Alexander},
  month = feb,
  year = {2016},
  pages = {731-747},
  file = {articles/Kuang2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/YK8ICVI3/Planning-Movements-in-Visual-and-Physical-Space-in.html}
}

@article{Yim2014,
  title = {Impact of Correlated Inputs to Neurons: Modeling Observations from in Vivo Intracellular Recordings},
  volume = {37},
  issn = {0929-5313, 1573-6873},
  shorttitle = {Impact of Correlated Inputs to Neurons},
  doi = {10.1007/s10827-014-0502-z},
  language = {en},
  number = {2},
  journal = {Journal of Computational Neuroscience},
  author = {Yim, Man Yi and Kumar, Arvind and Aertsen, Ad and Rotter, Stefan},
  month = oct,
  year = {2014},
  pages = {293-304},
  file = {articles/Yim2014.pdf}
}

@article{Stachenfeld2017,
  title = {The Hippocampus as a Predictive Map},
  volume = {advance online publication},
  copyright = {\textcopyright{} 2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  issn = {1097-6256},
  doi = {10.1038/nn.4650},
  abstract = {A cognitive map has long been the dominant metaphor for hippocampal function, embracing the idea that place cells encode a geometric representation of space. However, evidence for predictive coding, reward sensitivity and policy dependence in place cells suggests that the representation is not purely spatial. We approach this puzzle from a reinforcement learning perspective: what kind of spatial representation is most useful for maximizing future reward? We show that the answer takes the form of a predictive representation. This representation captures many aspects of place cell responses that fall outside the traditional view of a cognitive map. Furthermore, we argue that entorhinal grid cells encode a low-dimensionality basis set for the predictive representation, useful for suppressing noise in predictions and extracting multiscale structure for hierarchical planning.},
  language = {en},
  journal = {Nature Neuroscience},
  author = {Stachenfeld, Kimberly L. and Botvinick, Matthew M. and Gershman, Samuel J.},
  month = oct,
  year = {2017},
  keywords = {Hippocampus,Learning and memory,Reward},
  file = {articles/Stachenfeld2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/EY4XXHIE/nn.4650.html}
}

@article{Triesch2017,
  title = {Competition for a Limited Supply of Synaptic Building Blocks Predicts Multiplicative Synaptic Normalization and Heterosynaptic Plasticity},
  copyright = {\textcopyright{} 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  doi = {10.1101/166819},
  abstract = {We present a mathematical model of synaptic normalization and heterosynaptic plasticity based on competition for limited synaptic resources. In the model, afferent synapses on a part of the dendritic tree of a neuron compete for a limited supply of synaptic building blocks such as AMPA receptors or other postsynaptic components, which are distributed across the dendritic tree. These building blocks form a pool of parts that are ready for incorporation into synapses. Using minimal assumptions, the model produces fast multiplicative normalization behavior and leads to a homeostatic form of heterosynaptic plasticity. It therefore supports the use of such rules in neural network models. Furthermore, the model predicts that the amount of heterosynaptic plasticity is small when many building blocks are available in the pool. The model also suggests that local production and/or assembly of postsynaptic building blocks across the dendritic tree may be necessary to maintain a neuron's proper function, because it facilitates their homogeneous distribution across the dendritic tree. Because of its simplicity and analytical tractability, the model provides a convenient starting point for the development of more detailed models of the molecular mechanisms underlying different forms of synaptic plasticity.},
  language = {en},
  journal = {bioRxiv},
  author = {Triesch, Jochen and Hafner, Anne-Sophie},
  month = jul,
  year = {2017},
  pages = {166819},
  file = {articles/Triesch2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/MR7KPA3E/166819.html}
}

@misc{zotero-1916,
  title = {Brian2 2.0.2.1},
  file = {manuals/brian2/brian2_2.0.2.1/index.html}
}

@article{Pereira2017,
  title = {Attractor Dynamics in Networks with Learning Rules Inferred from in Vivo Data},
  copyright = {\textcopyright{} 2017, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  doi = {10.1101/199521},
  abstract = {The attractor neural network scenario is a popular scenario for memory storage in association cortex, but there is still a large gap between models based on this scenario and experimental data. We study a recurrent network model in which both learning rules and distribution of stored patterns are inferred from distributions of visual responses for novel and familiar images in inferior temporal cortex (ITC). Unlike classical attractor neural network models, our model exhibits graded activity in retrieval states, with distributions of firing rates that are close to lognormal. Inferred learning rules are close to maximizing the number of stored patterns within a family of unsupervised Hebbian learning rules, suggesting learning rules in ITC are optimized to store a large number of attractor states. Finally, we show that there exists two types of retrieval states: one in which firing rates are constant in time, another in which firing rates fluctuate chaotically.},
  language = {en},
  journal = {bioRxiv},
  author = {Pereira, Ulises and Brunel, Nicolas},
  month = oct,
  year = {2017},
  pages = {199521},
  file = {articles/Pereira2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/GF8KZE6D/199521.html}
}

@book{1992,
  address = {Oxford},
  edition = {Repr},
  title = {The {{Oxford}}-{{Duden}} Pictorial {{English}} Dictionary},
  isbn = {978-0-19-864155-1},
  language = {eng},
  publisher = {{Univ. Press}},
  year = {1992},
  file = {books/1992_The-Oxford-Duden-pictorial-English-dictionary.pdf},
  note = {OCLC: 258013646}
}

@article{Cohen2013,
  title = {Metabolic {{Turnover}} of {{Synaptic Proteins}}: {{Kinetics}}, {{Interdependencies}} and {{Implications}} for {{Synaptic Maintenance}}},
  volume = {8},
  issn = {1932-6203},
  shorttitle = {Metabolic {{Turnover}} of {{Synaptic Proteins}}},
  doi = {10.1371/journal.pone.0063191},
  abstract = {Chemical synapses contain multitudes of proteins, which in common with all proteins, have finite lifetimes and therefore need to be continuously replaced. Given the huge numbers of synaptic connections typical neurons form, the demand to maintain the protein contents of these connections might be expected to place considerable metabolic demands on each neuron. Moreover, synaptic proteostasis might differ according to distance from global protein synthesis sites, the availability of distributed protein synthesis facilities, trafficking rates and synaptic protein dynamics. To date, the turnover kinetics of synaptic proteins have not been studied or analyzed systematically, and thus metabolic demands or the aforementioned relationships remain largely unknown. In the current study we used dynamic Stable Isotope Labeling with Amino acids in Cell culture (SILAC), mass spectrometry (MS), Fluorescent Non\textendash{}Canonical Amino acid Tagging (FUNCAT), quantitative immunohistochemistry and bioinformatics to systematically measure the metabolic half-lives of hundreds of synaptic proteins, examine how these depend on their pre/postsynaptic affiliation or their association with particular molecular complexes, and assess the metabolic load of synaptic proteostasis. We found that nearly all synaptic proteins identified here exhibited half-lifetimes in the range of 2\textendash{}5 days. Unexpectedly, metabolic turnover rates were not significantly different for presynaptic and postsynaptic proteins, or for proteins for which mRNAs are consistently found in dendrites. Some functionally or structurally related proteins exhibited very similar turnover rates, indicating that their biogenesis and degradation might be coupled, a possibility further supported by bioinformatics-based analyses. The relatively low turnover rates measured here ($\sim$0.7\% of synaptic protein content per hour) are in good agreement with imaging-based studies of synaptic protein trafficking, yet indicate that the metabolic load synaptic protein turnover places on individual neurons is very substantial.},
  number = {5},
  journal = {PLOS ONE},
  author = {Cohen, Laurie D. and Zuchman, Rina and Sorokina, Oksana and M\"uller, Anke and Dieterich, Daniela C. and Armstrong, J. Douglas and Ziv, Tamar and Ziv, Noam E.},
  month = may,
  year = {2013},
  keywords = {Neuronal dendrites,Neurons,Synapses,Lysine,Protein metabolism,Protein synthesis,Stable isotope labeling by amino acids in cell culture,Synaptic vesicles},
  pages = {e63191},
  file = {articles/Cohen2013.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/VPVGJ6HV/article.html}
}

@article{Gruning2017,
  title = {Practical Computational Reproducibility in the Life Sciences},
  copyright = {\textcopyright{} 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  doi = {10.1101/200683},
  abstract = {$<$p$>$Many areas of research suffer from poor reproducibility. This problem is particularly acute in computationally intensive domains where results rely on a series of complex methodological decisions that are not well captured by traditional publication approaches. Various guidelines have emerged for achieving reproducibility, but practical implementation of these practices remains difficult. This is because reproducing published computational analyses requires installing many software tools plus associated libraries, connecting tools together into the complete pipeline, and specifying parameters. Here we present a suite of recently emerged technologies which make computational reproducibility not just possible, but, finally, practical in both time and effort. By combining a system for building highly portable packages of bioinformatics software, containerization and virtualization technologies for isolating reusable execution environments for these packages, and an integrated workflow system that automatically orchestrates the composition of these packages for entire pipelines, an unprecedented level of computational reproducibility can be achieved.$<$/p$>$},
  language = {en},
  journal = {bioRxiv},
  author = {Gr\"uning, Bj\"orn and Chilton, John and K\"oster, Johannes and Dale, Ryan and Goecks, Jeremy and Backofen, Rolf and Nekrutenko, Anton and Taylor, James},
  month = oct,
  year = {2017},
  pages = {200683},
  file = {articles/Grüning2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/UX3QSBAA/200683.html}
}

@book{Frohlich2016,
  address = {Amsterdam ; Boston},
  title = {Network Neuroscience},
  isbn = {978-0-12-801560-5},
  lccn = {QP363.3 .F75 2016},
  publisher = {{Academic Press}},
  author = {Fr\"ohlich, Flavio},
  year = {2016},
  keywords = {Nerve Net,Neural networks (Neurobiology),Neural circuitry},
  file = {books/Fröhlich2016_Network-neuroscience.pdf},
  note = {OCLC: ocn969336526}
}

@article{Piccolo2016,
  title = {Tools and Techniques for Computational Reproducibility},
  volume = {5},
  issn = {2047-217X},
  doi = {10.1186/s13742-016-0135-4},
  abstract = {When reporting research findings, scientists document the steps they followed so that others can verify and build upon the research. When those steps have been described in sufficient detail that others can retrace the steps and obtain similar results, the research is said to be reproducible. Computers play a vital role in many research disciplines and present both opportunities and challenges for reproducibility. Computers can be programmed to execute analysis tasks, and those programs can be repeated and shared with others. The deterministic nature of most computer programs means that the same analysis tasks, applied to the same data, will often produce the same outputs. However, in practice, computational findings often cannot be reproduced because of complexities in how software is packaged, installed, and executed\textemdash{}and because of limitations associated with how scientists document analysis steps. Many tools and techniques are available to help overcome these challenges; here we describe seven such strategies. With a broad scientific audience in mind, we describe the strengths and limitations of each approach, as well as the circumstances under which each might be applied. No single strategy is sufficient for every scenario; thus we emphasize that it is often useful to combine approaches.},
  journal = {GigaScience},
  author = {Piccolo, Stephen R. and Frampton, Michael B.},
  month = jul,
  year = {2016},
  keywords = {Computational reproducibility,Literate programming,Practice of science,Software containers,Software frameworks,Virtualization},
  pages = {30},
  file = {articles/Piccolo2016.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/5BKW2A9Z/s13742-016-0135-4.html}
}

@article{Cavallari2014,
  title = {Comparison of the Dynamics of Neural Interactions between Current-Based and Conductance-Based Integrate-and-Fire Recurrent Networks},
  volume = {8},
  issn = {1662-5110},
  doi = {10.3389/fncir.2014.00012},
  abstract = {Models of networks of Leaky Integrate-and-Fire (LIF) neurons are a widely used tool for theoretical investigations of brain function. These models have been used both with current- and conductance-based synapses. However, the differences in the dynamics expressed by these two approaches have been so far mainly studied at the single neuron level. To investigate how these synaptic models affect network activity, we compared the single neuron and neural population dynamics of conductance-based networks (COBNs) and current-based networks (CUBNs) of LIF neurons. These networks were endowed with sparse excitatory and inhibitory recurrent connections, and were tested in conditions including both low- and high-conductance states. We developed a novel procedure to obtain comparable networks by properly tuning the synaptic parameters not shared by the models. The so defined comparable networks displayed an excellent and robust match of first order statistics (average single neuron firing rates and average frequency spectrum of network activity). However, these comparable networks showed profound differences in the second order statistics of neural population interactions and in the modulation of these properties by external inputs. The correlation between inhibitory and excitatory synaptic currents and the cross-neuron correlation between synaptic inputs, membrane potentials and spike trains were stronger and more stimulus-modulated in the COBN. Because of these properties, the spike train correlation carried more information about the strength of the input in the COBN, although the firing rates were equally informative in both network models. Moreover, the network activity of COBN showed stronger synchronization in the gamma band, and spectral information about the input higher and spread over a broader range of frequencies. These results suggest that the second order statistics of network dynamics depend strongly on the choice of synaptic model.},
  journal = {Frontiers in Neural Circuits},
  author = {Cavallari, Stefano and Panzeri, Stefano and Mazzoni, Alberto},
  month = mar,
  year = {2014},
  file = {articles/Cavallari2014.pdf}
}

@article{Goecks2010,
  title = {Galaxy: A Comprehensive Approach for Supporting Accessible, Reproducible, and Transparent Computational Research in the Life Sciences},
  volume = {11},
  issn = {1474-760X},
  shorttitle = {Galaxy},
  doi = {10.1186/gb-2010-11-8-r86},
  abstract = {Increased reliance on computational approaches in the life sciences has revealed grave concerns about how accessible and reproducible computation-reliant results truly are. Galaxy                   http://usegalaxy.org                                  , an open web-based platform for genomic research, addresses these problems. Galaxy automatically tracks and manages data provenance and provides support for capturing the context and intent of computational methods. Galaxy Pages are interactive, web-based documents that provide users with a medium to communicate a complete computational analysis.},
  journal = {Genome Biology},
  author = {Goecks, Jeremy and Nekrutenko, Anton and Taylor, James},
  month = aug,
  year = {2010},
  pages = {R86},
  file = {articles/Goecks2010.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/5I7MCLSK/gb-2010-11-8-r86.html}
}

@article{Nishimoto2011,
  title = {Reconstructing {{Visual Experiences}} from {{Brain Activity Evoked}} by {{Natural Movies}}},
  volume = {21},
  issn = {0960-9822},
  doi = {10.1016/j.cub.2011.08.031},
  language = {English},
  number = {19},
  journal = {Current Biology},
  author = {Nishimoto, Shinji and Vu, An T. and Naselaris, Thomas and Benjamini, Yuval and Yu, Bin and Gallant, Jack L.},
  month = oct,
  year = {2011},
  pages = {1641-1646},
  file = {articles/Nishimoto2011.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/XGJRHECM/S0960-9822(11)00937-7.html},
  pmid = {21945275}
}

@techreport{zotero-1949,
  title = {Dataref},
  file = {manuals/latex/dataref.pdf},
  note = {manuals/latex}
}

@book{Cook2017,
  address = {Berkeley, CA},
  title = {Docker for {{Data Science Building Scalable}} and {{Extensible Data Infrastructure Around}} the {{Jupyter Notebook Server}}},
  isbn = {978-1-4842-3012-1},
  language = {English},
  publisher = {{Apress}},
  author = {Cook, Joshua},
  year = {2017},
  file = {books/Cook2017_Docker-for-Data-Science-Building-Scalable-and-Extensible-Data-Infrastructure-Around-the-Jupyter-Notebook-Server.pdf},
  note = {OCLC: 1006697547}
}

@inproceedings{Hamerly2004,
  title = {Learning the k in K-Means},
  author = {Hamerly, Greg and Elkan, Charles},
  year = {2004},
  pages = {281-288},
  file = {conferences/Hamerly2004_Learning-the-k-in-k-means.pdf}
}

@article{Berry2017,
  title = {Spine {{Dynamics}}: {{Are They All}} the {{Same}}?},
  volume = {96},
  issn = {08966273},
  shorttitle = {Spine {{Dynamics}}},
  doi = {10.1016/j.neuron.2017.08.008},
  language = {en},
  number = {1},
  journal = {Neuron},
  author = {Berry, Kalen P. and Nedivi, Elly},
  month = sep,
  year = {2017},
  pages = {43-55},
  file = {articles/Berry2017.pdf}
}

@article{Ho2011,
  title = {The {{Cell Biology}} of {{Synaptic Plasticity}}},
  volume = {334},
  copyright = {Copyright \textcopyright{} 2011, American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1209236},
  abstract = {Synaptic plasticity is the experience-dependent change in connectivity between neurons that is believed to underlie learning and memory. Here, we discuss the cellular and molecular processes that are altered when a neuron responds to external stimuli, and how these alterations lead to an increase or decrease in synaptic connectivity. Modification of synaptic components and changes in gene expression are necessary for many forms of plasticity. We focus on excitatory neurons in the mammalian hippocampus, one of the best-studied model systems of learning-related plasticity.},
  language = {en},
  number = {6056},
  journal = {Science},
  author = {Ho, Victoria M. and Lee, Ji-Ann and Martin, Kelsey C.},
  month = nov,
  year = {2011},
  pages = {623-628},
  file = {articles/Ho2011.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/IYQCMT8S/623.html},
  pmid = {22053042}
}

@article{Larkman1992,
  title = {Presynaptic Release Probability Influences the Locus of Long-Term Potentiation},
  volume = {360},
  copyright = {1992 Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/360070a0},
  abstract = {Presynaptic release probability influences the locus of long-term potentiation},
  language = {En},
  number = {6399},
  journal = {Nature},
  author = {Larkman, Alan and Jack, Julian and Stratford, Ken and Hannay, Timo},
  month = nov,
  year = {1992},
  pages = {70},
  file = {articles/Larkman1992.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/4R8BWX2P/360070a0.html}
}

@article{Silver2003,
  title = {High-{{Probability Uniquantal Transmission}} at {{Excitatory Synapses}} in {{Barrel Cortex}}},
  volume = {302},
  copyright = {American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1087160},
  abstract = {The number of vesicles released at excitatory synapses and the number of release sites per synaptic connection are key determinants of information processing in the cortex, yet they remain uncertain. Here we show that the number of functional release sites and the number of anatomically identified synaptic contacts are equal at connections between spiny stellate and pyramidal cells in rat barrel cortex. Moreover, our results indicate that the amount of transmitter released per synaptic contact is independent of release probability and the intrinsic release probability is high. These properties suggest that connections between layer 4and layer 2/3 are tuned for reliable transmission of spatially distributed, timing-based signals.},
  language = {en},
  number = {5652},
  journal = {Science},
  author = {Silver, R. Angus and L\"ubke, Joachim and Sakmann, Bert and Feldmeyer, Dirk},
  month = dec,
  year = {2003},
  pages = {1981-1984},
  file = {articles/Silver2003.pdf},
  pmid = {14671309}
}

@article{Edwards2007,
  title = {The {{Neurotransmitter Cycle}} and {{Quantal Size}}},
  volume = {55},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2007.09.001},
  abstract = {Changes in the response to release of a single synaptic vesicle have generally been attributed to postsynaptic modification of receptor sensitivity, but considerable evidence now demonstrates that alterations in vesicle filling also contribute to changes in quantal size. Receptors are not saturated at many synapses, and changes in the amount of transmitter per vesicle contribute to the physiological regulation of release. On the other hand, the presynaptic factors that determine quantal size remain poorly understood. Aside from regulation of the fusion pore, these mechanisms fall into two general categories: those that affect the accumulation of transmitter inside a vesicle and those that affect vesicle size. This review will summarize current understanding of the neurotransmitter cycle and indicate basic, unanswered questions about the presynaptic regulation of quantal size.},
  number = {6},
  journal = {Neuron},
  author = {Edwards, Robert H.},
  month = sep,
  year = {2007},
  pages = {835-858},
  file = {articles/Edwards2007.pdf}
}

@article{Damour2015,
  title = {Inhibitory and {{Excitatory Spike}}-{{Timing}}-{{Dependent Plasticity}} in the {{Auditory Cortex}}},
  volume = {86},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2015.03.014},
  language = {English},
  number = {2},
  journal = {Neuron},
  author = {D'amour, James A. and Froemke, Robert C.},
  month = apr,
  year = {2015},
  pages = {514-528},
  file = {articles/D’amour2015.pdf},
  pmid = {25843405}
}

@article{Sjostrom2007,
  series = {LTP: Forty Unforgettable Years. A Festschrift in Honour of Professor Tim Bliss FRS},
  title = {Multiple Forms of Long-Term Plasticity at Unitary Neocortical Layer 5 Synapses},
  volume = {52},
  issn = {0028-3908},
  doi = {10.1016/j.neuropharm.2006.07.021},
  abstract = {Long-term potentiation and depression (LTP and LTD) are cellular plasticity phenomena expressed at a variety of central synapses, and are thought to contribute to learning and developmental changes in circuitry. Recurrent neocortical layer-5 synapses are thought to express a presynaptic form of LTP that influences the short-term plasticity of the synapse. Here we show that changes in synaptic strength elicited by pairing high frequency pre- and postsynaptic firing at this synapse result from a mixture of presynaptic and postsynaptic forms of plasticity, as assessed by the analysis of changes in coefficient of variation, short-term plasticity, and NMDA:AMPA current ratios. Pharmacological dissection of this plasticity revealed that block of presynaptic LTD with an endocannabinoid inhibitor enhanced LTP, while the apparently presynaptic component of LTP could be prevented by induction in the presence of blockers of nitric oxide. These data suggest that correlated high-frequency firing at layer-5 synapses simultaneously induces a mixture of presynaptic LTD, presynaptic LTP, and postsynaptic LTP.},
  number = {1},
  journal = {Neuropharmacology},
  author = {Sj\"ostr\"om, Per Jesper and Turrigiano, Gina G. and Nelson, Sacha B.},
  month = jan,
  year = {2007},
  keywords = {Endocannabinoid,Long-term depression,Long-term potentiation,Neocortical layer 5,Nitric oxide},
  pages = {176-184},
  file = {articles/Sjöström2007.pdf}
}

@article{Elliott2017,
  title = {First {{Passage Time Memory Lifetimes}} for {{Simple}}, {{Multistate Synapses}}},
  volume = {29},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01016},
  abstract = {Memory models based on synapses with discrete and bounded strengths store new memories by forgetting old ones. Memory lifetimes in such memory systems may be defined in a variety of ways. A mean first passage time (MFPT) definition overcomes much of the arbitrariness and many of the problems associated with the more usual signal-to-noise ratio (SNR) definition. We have previously computed MFPT lifetimes for simple, binary-strength synapses that lack internal, plasticity-related states. In simulation we have also seen that for multistate synapses, optimality conditions based on SNR lifetimes are absent with MFPT lifetimes, suggesting that such conditions may be artifactual. Here we extend our earlier work by computing the entire first passage time (FPT) distribution for simple, multistate synapses, from which all statistics, including the MFPT lifetime, may be extracted. For this, we develop a Fokker-Planck equation using the jump moments for perceptron activation. Two models are considered that satisfy a particular eigenvector condition that this approach requires. In these models, MFPT lifetimes do not exhibit optimality conditions, while in one but not the other, SNR lifetimes do exhibit optimality. Thus, not only are such optimality conditions artifacts of the SNR approach, but they are also strongly model dependent. By examining the variance in the FPT distribution, we may identify regions in which memory storage is subject to high variability, although MFPT lifetimes are nevertheless robustly positive. In such regions, SNR lifetimes are typically (defined to be) zero. FPT-defined memory lifetimes therefore provide an analytically superior approach and also have the virtue of being directly related to a neuron's firing properties.},
  number = {12},
  journal = {Neural Computation},
  author = {Elliott, Terry},
  month = sep,
  year = {2017},
  pages = {3219-3259},
  file = {articles/Elliott2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/EZHHGD6S/neco_a_01016.html}
}

@article{Bremges2015,
  title = {Deeply Sequenced Metagenome and Metatranscriptome of a Biogas-Producing Microbial Community from an Agricultural Production-Scale Biogas Plant},
  volume = {4},
  issn = {2047-217X},
  doi = {10.1186/s13742-015-0073-6},
  abstract = {The production of biogas takes place under anaerobic conditions and involves microbial decomposition of organic matter. Most of the participating microbes are still unknown and non-cultivable. Accordingly, shotgun metagenome sequencing currently is the method of choice to obtain insights into community composition and the genetic repertoire.},
  journal = {GigaScience},
  author = {Bremges, Andreas and Maus, Irena and Belmann, Peter and Eikmeyer, Felix and Winkler, Anika and Albersmeier, Andreas and P\"uhler, Alfred and Schl\"uter, Andreas and Sczyrba, Alexander},
  month = jul,
  year = {2015},
  keywords = {Anaerobic digestion,Assembly,Biogas,Metagenomics,Metatranscriptomics,Methanogenesis,Sequencing,Wet fermentation},
  pages = {33},
  file = {articles/Bremges2015.pdf}
}

@article{Clarkson2015,
  title = {The Archaeology, Chronology and Stratigraphy of {{Madjedbebe}} ({{Malakunanja II}}): {{A}} Site in Northern {{Australia}} with Early Occupation},
  volume = {83},
  issn = {0047-2484},
  shorttitle = {The Archaeology, Chronology and Stratigraphy of {{Madjedbebe}} ({{Malakunanja II}})},
  doi = {10.1016/j.jhevol.2015.03.014},
  abstract = {Published ages of $>$50~ka for occupation at Madjedbebe (Malakunanja II) in Australia's north have kept the site prominent in discussions about the colonisation of Sahul. The site also contains one of the largest stone artefact assemblages in Sahul for this early period. However, the stone artefacts and other important archaeological components of the site have never been described in detail, leading to persistent doubts about its stratigraphic integrity. We report on our analysis of the stone artefacts and faunal and other materials recovered during the 1989 excavations, as well as the stratigraphy and depositional history recorded by the original excavators. We demonstrate that the technology and raw materials of the early assemblage are distinctive from those in the overlying layers. Silcrete and quartzite artefacts are common in the early assemblage, which also includes edge-ground axe fragments and ground haematite. The lower flaked stone assemblage is distinctive, comprising a mix of long convergent flakes, some radial flakes with faceted platforms, and many small thin silcrete flakes that we interpret as thinning flakes. Residue and use-wear analysis indicate occasional grinding of haematite and woodworking, as well as frequent abrading of platform edges on thinning flakes. We conclude that previous claims of extensive displacement of artefacts and post-depositional disturbance may have been overstated. The stone artefacts and stratigraphic details support previous claims for human occupation 50\textendash{}60~ka and show that human occupation during this time differed from later periods. We discuss the implications of these new data for understanding the first human colonisation of Sahul.},
  number = {Supplement C},
  journal = {Journal of Human Evolution},
  author = {Clarkson, Chris and Smith, Mike and Marwick, Ben and Fullagar, Richard and Wallis, Lynley A. and Faulkner, Patrick and Manne, Tiina and Hayes, Elspeth and Roberts, Richard G. and Jacobs, Zenobia and Carah, Xavier and Lowe, Kelsey M. and Matthews, Jacqueline and Florin, S. Anna},
  month = jun,
  year = {2015},
  keywords = {Australia,Chronology,Colonisation,Grindstones,Lithic technology,Stone axes},
  pages = {46-64},
  file = {articles/Clarkson2015.pdf}
}

@article{Gleeson2017,
  title = {A {{Commitment}} to {{Open Source}} in {{Neuroscience}}},
  volume = {96},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2017.10.013},
  abstract = {Modern neuroscience increasingly relies on custom-developed software, but much of this is not being made available to the wider community. A group of researchers are pledging to make code they produce for data analysis and modeling open source, and are actively encouraging their colleagues to follow suit.},
  number = {5},
  journal = {Neuron},
  author = {Gleeson, Padraig and Davison, Andrew P. and Silver, R. Angus and Ascoli, Giorgio A.},
  month = dec,
  year = {2017},
  pages = {964-965},
  file = {articles/Gleeson2017.pdf}
}

@inproceedings{Gadea2016,
  title = {A Microservices Architecture for Collaborative Document Editing Enhanced with Face Recognition},
  doi = {10.1109/SACI.2016.7507409},
  abstract = {Modern web applications can now provide rich and dynamic user experiences, such as allowing multiple users to collaboratively edit rich-text documents in real-time from multiple devices. Application architectures are evolving to support the development and deployment of such interactive functionality by decoupling software components into microservices. This paper introduces the architecture and the implementation of a collaborative rich-text editor that makes use of microservices to enable and enhance its scalable co-editing functionality. This includes microservices for synchronizing unstructured text using operational transformations, for chat functionality, and for detecting and recognizing faces in images added to the editor. The architecture makes use of Docker to allow for the development and testing of individual services as separate containers enabling seamless deployment across the available network of computers and other computing devices. The system will be demonstrated by showing how microservices make it possible for multiple users to co-edit a document where images containing faces are added and recognized as part of the document content, thereby supporting the document creation process.},
  booktitle = {2016 {{IEEE}} 11th {{International Symposium}} on {{Applied Computational Intelligence}} and {{Informatics}} ({{SACI}})},
  author = {Gadea, C. and Trifan, M. and Ionescu, D. and Cordea, M. and Ionescu, B.},
  month = may,
  year = {2016},
  keywords = {chat functionality,Collaboration,collaborative document editing,collaborative editing,Computer architecture,Databases,docker,Docker,document creation process,document image processing,Face,face detection,face recognition,Face recognition,Google,groupware,Internet,microservices,microservices architecture,operational transformation,Real-time systems,real-time web,rich-text document editing,software architecture,text analysis,user experience,Web applications},
  pages = {441-446},
  file = {conferences/Gadea2016_A-microservices-architecture-for-collaborative-document-editing-enhanced-with-face-recognition.pdf}
}

@inproceedings{Cito2016,
  series = {Lecture Notes in Computer Science},
  title = {Using {{Docker Containers}} to {{Improve Reproducibility}} in {{Software}} and {{Web Engineering Research}}},
  isbn = {978-3-319-38790-1 978-3-319-38791-8},
  doi = {10.1007/978-3-319-38791-8_58},
  abstract = {The ability to replicate and reproduce scientific results has become an increasingly important topic for many academic disciplines. In computer science and, more specifically, software and web engineering, contributions of scientific work rely on developed algorithms, tools and prototypes, quantitative evaluations, and other computational analyses. Published code and data come with many undocumented assumptions, dependencies, and configurations that are internal knowledge and make reproducibility hard to achieve. This tutorial presents how Docker containers can overcome these issues and aid the reproducibility of research artifacts in software and web engineering and discusses their applications in the field.},
  language = {en},
  booktitle = {Web {{Engineering}}},
  publisher = {{Springer, Cham}},
  author = {Cito, J\"urgen and Ferme, Vincenzo and Gall, Harald C.},
  month = jun,
  year = {2016},
  pages = {609-612},
  file = {conferences/Cito2016_Using-Docker-Containers-to-Improve-Reproducibility-in-Software-and-Web-Engineering-Research.pdf}
}

@article{Eglen2015,
  title = {Bivariate Spatial Point Patterns in the Retina: A Reproducible Review},
  copyright = {\textcopyright{} 2015, Published by Cold Spring Harbor Laboratory Press. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
  shorttitle = {Bivariate Spatial Point Patterns in the Retina},
  doi = {10.1101/029348},
  abstract = {$<$p$>$In this article I present a reproducible review of recent research to investigate the spatial positioning of neurons in the nervous system. In particular, I focus on the relative spatial positioning of pairs of cell types within the retina. I examine three different cases by which two types of neurons might be arranged relative to each other. (1) Cells of different type might be effectively independent of each other. (2) Cells of one type are randomly assigned one of two labels to create two related populations. (3) Interactions between cells of different type generate functional dependencies. I show briefly how spatial statistic techniques can be applied to investigate the nature of spatial interactions between two cell types. Finally, I have termed this article a ‘reproducible review9 because all the data and computer code are integrated into the manuscript so that others can repeat the analysis presented here. I close the review with a discussion of this concept.$<$/p$>$},
  language = {en},
  journal = {bioRxiv},
  author = {Eglen, Stephen J.},
  month = oct,
  year = {2015},
  pages = {029348},
  file = {articles/Eglen2015.pdf}
}

@article{Waskom2014,
  title = {Frontoparietal {{Representations}} of {{Task Context Support}} the {{Flexible Control}} of {{Goal}}-{{Directed Cognition}}},
  volume = {34},
  copyright = {Copyright \textcopyright{} 2014 the authors 0270-6474/14/3410743-13\$15.00/0. This article is freely available online through the J Neurosci Author Open Choice option.},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5282-13.2014},
  abstract = {Cognitive control allows stimulus-response processing to be aligned with internal goals and is thus central to intelligent, purposeful behavior. Control is thought to depend in part on the active representation of task information in prefrontal cortex (PFC), which provides a source of contextual bias on perception, decision making, and action. In the present study, we investigated the organization, influences, and consequences of context representation as human subjects performed a cued sorting task that required them to flexibly judge the relationship between pairs of multivalent stimuli. Using a connectivity-based parcellation of PFC and multivariate decoding analyses, we determined that context is specifically and transiently represented in a region spanning the inferior frontal sulcus during context-dependent decision making. We also found strong evidence that decision context is represented within the intraparietal sulcus, an area previously shown to be functionally networked with the inferior frontal sulcus at rest and during task performance. Rule-guided allocation of attention to different stimulus dimensions produced discriminable patterns of activation in visual cortex, providing a signature of top-down bias over perception. Furthermore, demands on cognitive control arising from the task structure modulated context representation, which was found to be strongest after a shift in task rules. When context representation in frontoparietal areas increased in strength, as measured by the discriminability of high-dimensional activation patterns, the bias on attended stimulus features was enhanced. These results provide novel evidence that illuminates the mechanisms by which humans flexibly guide behavior in complex environments.},
  language = {en},
  number = {32},
  journal = {Journal of Neuroscience},
  author = {Waskom, Michael L. and Kumaran, Dharshan and Gordon, Alan M. and Rissman, Jesse and Wagner, Anthony D.},
  month = aug,
  year = {2014},
  keywords = {decision making,attention,cognitive control,prefrontal cortex},
  pages = {10743-10755},
  file = {articles/Waskom2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/IVU9PLLE/10743.html},
  pmid = {25100605}
}

@article{Ghosh2017,
  title = {A Very Simple, Re-Executable Neuroimaging Publication},
  volume = {6},
  issn = {2046-1402},
  doi = {10.12688/f1000research.10783.2},
  language = {en},
  journal = {F1000Research},
  author = {Ghosh, Satrajit S. and Poline, Jean-Baptiste and Keator, David B. and Halchenko, Yaroslav O. and Thomas, Adam G. and Kessler, Daniel A. and Kennedy, David N.},
  month = jun,
  year = {2017},
  pages = {124},
  file = {articles/Ghosh2017.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/KYUG89FA/v2.html}
}

@article{Wong2010,
  title = {Points of View: {{Design}} of Data Figures},
  volume = {7},
  issn = {1548-7091, 1548-7105},
  shorttitle = {Points of View},
  doi = {10.1038/nmeth0910-665},
  number = {9},
  journal = {Nature Methods},
  author = {Wong, Bang},
  month = sep,
  year = {2010},
  pages = {665-665},
  file = {articles/Wong2010.pdf}
}

@article{Spoerer2017,
  title = {Recurrent {{Convolutional Neural Networks}}: {{A Better Model}} of {{Biological Object Recognition}}},
  volume = {8},
  issn = {1664-1078},
  shorttitle = {Recurrent {{Convolutional Neural Networks}}},
  doi = {10.3389/fpsyg.2017.01551},
  abstract = {Feedforward neural networks provide the dominant model of how the brain performs visual object recognition. However, these networks lack the lateral and feedback connections, and the resulting recurrent neuronal dynamics, of the ventral visual pathway in the human and nonhuman primate brain. Here we investigate recurrent convolutional neural networks with bottom-up (B), lateral (L), and top-down (T) connections. Combining these types of connections yields four architectures (B, BT, BL, and BLT), which we systematically test and compare. We hypothesized that recurrent dynamics might improve recognition performance in the challenging scenario of partial occlusion. We introduce two novel occluded object recognition tasks to test the efficacy of the models, $\backslash$emph\{digit clutter\} (where multiple target digits occlude one another) and $\backslash$emph\{digit debris\} (where target digits are occluded by digit fragments). We find that recurrent neural networks outperform feedforward control models (approximately matched in parametric complexity) at recognising objects, both in the absence of occlusion and in all occlusion conditions. Recurrent networks were also found to be more robust to the inclusion of additive Gaussian noise. Recurrent neural networks are better in two respects: (1) they are more neurobiologically realistic than their feedforward counterparts; (2) they are better in terms of their ability to recognise objects, especially under challenging conditions. This work shows that computer vision can benefit from using recurrent convolutional architectures and suggests that the ubiquitous recurrent connections in biological brains are essential for task performance.},
  language = {English},
  journal = {Frontiers in Psychology},
  author = {Spoerer, Courtney J. and McClure, Patrick and Kriegeskorte, Nikolaus},
  year = {2017},
  keywords = {Convolutional Neural Network,object recognition,occlusion,recurrent neural network,top-down processing},
  file = {articles/Spoerer2017.pdf}
}

@article{Wernle2017,
  title = {Integration of Grid Maps in Merged Environments},
  copyright = {2017 The Author(s)},
  issn = {1546-1726},
  doi = {10.1038/s41593-017-0036-6},
  abstract = {$<$p$>$The authors investigate grid cell dynamics after removal of a border between two environments. Near the transition between environments, grid fields changed location, resulting in local spatial periodicity and continuity between the original maps.$<$/p$>$},
  language = {En},
  journal = {Nature Neuroscience},
  author = {Wernle, Tanja and Waaga, Torgeir and M\o{}rreaunet, Maria and Treves, Alessandro and Moser, May-Britt and Moser, Edvard I.},
  month = dec,
  year = {2017},
  pages = {1},
  file = {articles/Wernle2017.pdf}
}

@article{Duarte2017,
  title = {Leveraging Heterogeneity for Neural Computation with Fading Memory in Layer 2/3 Cortical Microcircuits},
  copyright = {\textcopyright{} 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  doi = {10.1101/230821},
  abstract = {Complexity and heterogeneity are intrinsic to neurobiological systems, manifest in every process, at every scale, and are inextricably linked to the systems' emergent collective behaviours and function. However, the majority of studies addressing the dynamics and computational properties of biologically inspired cortical microcircuits tend to assume (often for the sake of analytical tractability) a great degree of homogeneity in both neuronal and synaptic/connectivity parameters. While simplification and reductionism are necessary to understand the brain's functional principles, disregarding the existence of the multiple heterogeneities in the cortical composition, which may be at the core of its computational proficiency, will inevitably fail to account for important phenomena and limit the scope and generalizability of cortical models. We address these issues by studying the individual and composite functional roles of heterogeneities in neuronal, synaptic and structural properties in a biophysically plausible layer 2/3 microcircuit model, built and constrained by multiple sources of empirical data. This approach was made possible by the emergence of large-scale, well curated databases, as well as the substantial improvements in experimental methodologies achieved over the last few years. Our results show that variability in single neuron parameters is the dominant source of functional specialization, leading to highly proficient microcircuits with much higher computational power than their homogeneous counterparts. We further show that fully heterogeneous circuits, which are closest to the biophysical reality, owe their response properties to the differential contribution of different sources of heterogeneity.},
  language = {en},
  journal = {bioRxiv},
  author = {Duarte, Renato and Morrison, Abigail},
  month = dec,
  year = {2017},
  pages = {230821},
  file = {articles/Duarte2017.pdf}
}

@book{Chernick2008,
  address = {Hoboken, N.J},
  edition = {2nd ed},
  series = {Wiley series in probability and statistics},
  title = {Bootstrap Methods: A Guide for Practitioners and Researchers},
  isbn = {978-0-471-75621-7},
  lccn = {QA276.8 .C479 2008},
  shorttitle = {Bootstrap Methods},
  publisher = {{Wiley-Interscience}},
  author = {Chernick, Michael R.},
  year = {2008},
  keywords = {Bootstrap (Statistics)},
  file = {books/Chernick2008_Bootstrap-methods-a-guide-for-practitioners-and-researchers.pdf},
  note = {OCLC: ocn156785095}
}

@book{Diggle2011,
  address = {Oxford},
  title = {Statistics and Scientific Method: An Introduction for Students and Researchers},
  isbn = {978-0-19-954318-2 978-0-19-954319-9},
  shorttitle = {Statistics and Scientific Method},
  abstract = {"Most introductory statistics text-books are written either in a highly mathematical style for an intended readership of mathematics undergraduate students, or in a recipe-book style for an intended audience of non-mathematically inclined undergraduate or postgraduate students, typically in a single discipline; hence, "statistics for biologists", "statistics for psychologists", and so on. An antidote to technique-oriented service courses, Statistics and Scientific Method is different. It studiously avoids the recipe-book style and keeps algebraic details of specific statistical methods to the minimum extent necessary to understand the underlying concepts. Instead, the text aims to give the reader a clear understanding of how core statistical ideas of experimental design, modelling and data analysis are integral to the scientific method. Aimed primarily at beginning postgraduate students across a range of scientific disciplines (albeit with a bias towards the biological, environmental and health sciences), it therefore assumes some maturity of understanding of scientific method, but does not require any prior knowledge of statistics, or any mathematical knowledge beyond basic algebra and a willingness to come to terms with mathematical notation. Any statistical analysis of a realistically sized data-set requires the use of specially written computer software. An Appendix introduces the reader to our open-source software of choice, R, whilst the book's web-page includes downloadable data and R code that enables the reader to reproduce all of the analyses in the book and, with easy modifications, to adapt the code to analyse their own data if they wish. However, the book is not intended to be a textbook on statistical computing, and all of the material in the book can be understood without using either R or any other computer software"--},
  language = {eng},
  publisher = {{Oxford Univ. Press}},
  author = {Diggle, Peter and Chetwynd, Amanda},
  year = {2011},
  file = {books/Diggle2011_Statistics-and-scientific-method-an-introduction-for-students-and-researchers.pdf},
  note = {OCLC: 753366927}
}

@book{Karris2007,
  address = {Fremont, CA},
  title = {Mathematics for Business, Science, and Technology: With {{MATLAB}} and Spreadsheet Applications},
  isbn = {978-1-934404-01-0},
  shorttitle = {Mathematics for Business, Science, and Technology},
  language = {English},
  publisher = {{Orchard Publications}},
  author = {Karris, Steven T},
  year = {2007},
  file = {books/Karris2007_Mathematics-for-business,-science,-and-technology-with-MATLAB-and-spreadsheet-applications.pdf},
  note = {OCLC: 896815975}
}

@article{Mengiste2017,
  title = {Computational {{Approaches}} to the {{Degeneration}} of {{Brain Networks}} and {{Other Complex Networks}}},
  abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 47 universities and research institutions.},
  language = {eng},
  journal = {DIVA},
  author = {Mengiste, Simachew Abebe},
  year = {2017},
  file = {articles/Mengiste2017.pdf}
}

@article{Cowan2012,
  title = {Nodal {{Dynamics}}, {{Not Degree Distributions}}, {{Determine}} the {{Structural Controllability}} of {{Complex Networks}}},
  volume = {7},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0038398},
  abstract = {Structural controllability has been proposed as an analytical framework for making predictions regarding the control of complex networks across myriad disciplines in the physical and life sciences (Liu et al., Nature:473(7346):167\textendash{}173, 2011). Although the integration of control theory and network analysis is important, we argue that the application of the structural controllability framework to most if not all real-world networks leads to the conclusion that a single control input, applied to the power dominating set, is all that is needed for structural controllability. This result is consistent with the well-known fact that controllability and its dual observability are generic properties of systems. We argue that more important than issues of structural controllability are the questions of whether a system is almost uncontrollable, whether it is almost unobservable, and whether it possesses almost pole-zero cancellations.},
  number = {6},
  journal = {PLOS ONE},
  author = {Cowan, Noah J. and Chastain, Erick J. and Vilhena, Daril A. and Freudenberg, James S. and Bergstrom, Carl T.},
  month = jun,
  year = {2012},
  keywords = {Dynamical systems,Signal processing,Eigenvalues,Control theory,Directed graphs,Engineering and technology,Food web structure,Transfer functions},
  pages = {e38398},
  file = {articles/Cowan2012.pdf}
}

@book{Abu-Mostafa2012,
  address = {S.l.},
  title = {Learning from Data: A Short Course},
  isbn = {978-1-60049-006-4},
  shorttitle = {Learning from Data},
  language = {eng},
  publisher = {{AMLbook.com}},
  author = {{Abu-Mostafa}, Yaser S. and {Magdon-Ismail}, Malik and Lin, Hsuan-Tien},
  year = {2012},
  file = {books/Abu-Mostafa2012_Learning-from-data-a-short-course.pdf},
  note = {OCLC: 808441289}
}

@article{Marcus2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.00631},
  primaryClass = {cs, stat},
  title = {Deep {{Learning}}: {{A Critical Appraisal}}},
  shorttitle = {Deep {{Learning}}},
  abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.},
  journal = {arXiv:1801.00631 [cs, stat]},
  author = {Marcus, Gary},
  month = jan,
  year = {2018},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,97R40,Computer Science - Artificial Intelligence,I.2.0,I.2.6},
  file = {articles/Marcus2018.pdf}
}

@article{Harris1989,
  title = {Dendritic Spines of {{CA}} 1 Pyramidal Cells in the Rat Hippocampus: Serial Electron Microscopy with Reference to Their Biophysical Characteristics},
  volume = {9},
  copyright = {\textcopyright{} 1989 by Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  shorttitle = {Dendritic Spines of {{CA}} 1 Pyramidal Cells in the Rat Hippocampus},
  abstract = {Serial electron microscopy and 3-D reconstructions of dendritic spines from hippocampal area CA 1 dendrites were obtained to evaluate 2 questions about relationships between spine geometry and synaptic efficacy. First, under what biophysical conditions are the spine necks likely to reduce the magnitude of charge transferred from the synapses on the spine heads to the recipient dendrite? Simulation software provided by Charles Wilson (1984) was used to determine that if synaptic conductance is 1 nS or less, only 1\% of the hippocampal spine necks are sufficiently thin and long to reduce charge transfer by more than 10\%. If synaptic conductance approaches 5 nS, however, 33\% of the hippocampal spine necks are sufficiently thin and long to reduce charge transfer by more than 10\%. Second, is spine geometry associated with other anatomical indicators of synaptic efficacy, including the area of the postsynaptic density and the number of vesicles in the presynaptic axon? Reconstructed spines were graphically edited into head and neck compartments, and their dimensions were measured, the areas of the postsynaptic densities (PSD) were measured, and all of the vesicles in the presynaptic axonal varicosities were counted. The dimensions of the spine head were well correlated with the area of PSD and the number of vesicles in the presynaptic axonal varicosity. Spine neck diameter and length were not correlated with PSD area, head volume, or the number of vesicles. These results suggest that the dimensions of the spine head, but not the spine neck, reflect differences in synaptic efficacy. We suggest that the constricted necks of hippocampal dendritic spines might reduce diffusion of activated molecules to neighboring synapses, thereby attributing specificity to activated or potentiated synapses.},
  language = {en},
  number = {8},
  journal = {Journal of Neuroscience},
  author = {Harris, K. M. and Stevens, J. K.},
  month = aug,
  year = {1989},
  pages = {2982-2997},
  file = {articles/Harris1989.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/RN7UAMPK/2982.html},
  pmid = {2769375}
}

@article{Knott2006,
  title = {Spine Growth Precedes Synapse Formation in the Adult Neocortex in Vivo},
  volume = {9},
  copyright = {2006 Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/nn1747},
  abstract = {Spine growth precedes synapse formation in the adult neocortex \emph{in vivo}},
  language = {En},
  number = {9},
  journal = {Nature Neuroscience},
  author = {Knott, Graham W. and Holtmaat, Anthony and Wilbrecht, Linda and Welker, Egbert and Svoboda, Karel},
  month = sep,
  year = {2006},
  pages = {1117},
  file = {articles/Knott2006.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/2KU5A59B/nn1747.html}
}

@article{Dayan2008,
  series = {Cognitive neuroscience},
  title = {Reinforcement Learning: {{The Good}}, {{The Bad}} and {{The Ugly}}},
  volume = {18},
  issn = {0959-4388},
  shorttitle = {Reinforcement Learning},
  doi = {10.1016/j.conb.2008.08.003},
  abstract = {Reinforcement learning provides both qualitative and quantitative frameworks for understanding and modeling adaptive decision-making in the face of rewards and punishments. Here we review the latest dispatches from the forefront of this field, and map out some of the territories where lie monsters.},
  number = {2},
  journal = {Current Opinion in Neurobiology},
  author = {Dayan, Peter and Niv, Yael},
  month = apr,
  year = {2008},
  pages = {185-196},
  file = {articles/Dayan2008.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/Y2ZAMNKF/S0959438808000767.html}
}

@article{Chalk2018,
  title = {Toward a Unified Theory of Efficient, Predictive, and Sparse Coding},
  volume = {115},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1711114115},
  language = {en},
  number = {1},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Chalk, Matthew and Marre, Olivier and Tka{\v c}ik, Ga{\v s}per},
  month = jan,
  year = {2018},
  pages = {186-191},
  file = {articles/Chalk2018.pdf}
}

@article{Dayan2014,
  title = {Model-Based and Model-Free {{Pavlovian}} Reward Learning: {{Revaluation}}, Revision, and Revelation},
  volume = {14},
  issn = {1530-7026, 1531-135X},
  shorttitle = {Model-Based and Model-Free {{Pavlovian}} Reward Learning},
  doi = {10.3758/s13415-014-0277-8},
  abstract = {Evidence supports at least two methods for learning about reward and punishment and making predictions for guiding actions. One method, called model-free, progressively acquires cached estimates of the long-run values of circumstances and actions from retrospective experience. The other method, called model-based, uses representations of the environment, expectations, and prospective calculations to make cognitive predictions of future value. Extensive attention has been paid to both methods in computational analyses of instrumental learning. By contrast, although a full computational analysis has been lacking, Pavlovian learning and prediction has typically been presumed to be solely model-free. Here, we revise that presumption and review compelling evidence from Pavlovian revaluation experiments showing that Pavlovian predictions can involve their own form of model-based evaluation. In model-based Pavlovian evaluation, prevailing states of the body and brain influence value computations, and thereby produce powerful incentive motivations that can sometimes be quite new. We consider the consequences of this revised Pavlovian view for the computational landscape of prediction, response, and choice. We also revisit differences between Pavlovian and instrumental learning in the control of incentive motivation.},
  language = {en},
  number = {2},
  journal = {Cognitive, Affective, \& Behavioral Neuroscience},
  author = {Dayan, Peter and Berridge, Kent C.},
  month = jun,
  year = {2014},
  pages = {473-492},
  file = {articles/Dayan2014.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/XZCVUBW7/10.html}
}

@article{Daw2011,
  title = {Model-{{Based Influences}} on {{Humans}}' {{Choices}} and {{Striatal Prediction Errors}}},
  volume = {69},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2011.02.027},
  abstract = {Summary
The mesostriatal dopamine system is prominently implicated in model-free reinforcement learning, with fMRI BOLD signals in ventral striatum notably covarying with model-free prediction errors. However, latent learning and devaluation studies show that behavior also shows hallmarks of model-based planning, and the interaction between model-based and model-free values, prediction errors, and preferences is underexplored. We designed a multistep decision task in which model-based and model-free influences on human choice behavior could be distinguished. By showing that choices reflected both influences we could then test the purity of the ventral striatal BOLD signal as a model-free report. Contrary to expectations, the signal reflected both model-free and model-based predictions in proportions matching those that best explained choice behavior. These results challenge the notion of a separate model-free learner and suggest a more integrated computational architecture for high-level human decision-making.},
  number = {6},
  journal = {Neuron},
  author = {Daw, Nathaniel D. and Gershman, Samuel J. and Seymour, Ben and Dayan, Peter and Dolan, Raymond J.},
  month = mar,
  year = {2011},
  pages = {1204-1215},
  file = {articles/Daw2011.pdf;/home/fh/.mozilla/firefox/d0w5bt9s.default/zotero/storage/JVZCY25D/S0896627311001255.html}
}

@article{Block1962,
  title = {The {{Perceptron}}: {{A Model}} for {{Brain Functioning}}. {{I}}},
  volume = {34},
  shorttitle = {The {{Perceptron}}},
  doi = {10.1103/RevModPhys.34.123},
  abstract = {DOI:https://doi.org/10.1103/RevModPhys.34.123},
  number = {1},
  journal = {Reviews of Modern Physics},
  author = {Block, H. D.},
  month = jan,
  year = {1962},
  pages = {123-135},
  file = {articles/Block1962.pdf}
}

@techreport{Novikoff1963,
  title = {On Convergence Proofs for Perceptrons},
  institution = {{STANFORD RESEARCH INST MENLO PARK CALIF}},
  author = {Novikoff, Albert BJ},
  year = {1963},
  file = {undefined/Novikoff1963on_convergence_proofs_for_perceptrons.pdf}
}

@book{Minsky1969,
  title = {Perceptrons},
  publisher = {{MIT Press}},
  author = {Minsky, Marvin and Papert, Seymour},
  year = {1969},
  file = {books/Minsky1969_Perceptrons.djvu}
}

@article{Vicente-Saez2018,
  title = {Open {{Science}} Now: {{A}} Systematic Literature Review for an Integrated Definition},
  issn = {0148-2963},
  shorttitle = {Open {{Science}} Now},
  doi = {10.1016/j.jbusres.2017.12.043},
  abstract = {Open Science is a disruptive phenomenon that is emerging around the world and especially in Europe. Open Science brings about socio-cultural and technological change, based on openness and connectivity, on how research is designed, performed, captured, and assessed. Several studies show that there is a lack of awareness about what Open Science is, mainly due to the fact that there is no formal definition of Open Science. The purpose of this paper is to build a rigorous, integrated, and up-to-date definition of the Open Science phenomenon through a systematic literature review. The resulting definition ``Open Science is transparent and accessible knowledge that is shared and developed through collaborative networks'' helps the scientific community, the business world, political actors, and citizens to have a common and clear understanding about what Open Science is, and stimulates an open debate about the social, economic, and human added value of this phenomenon.},
  journal = {Journal of Business Research},
  author = {{Vicente-Saez}, Ruben and {Martinez-Fuentes}, Clara},
  year = {2018},
  keywords = {Definition,Open access,Open innovation,Open science,Research and innovation management,Responsible research and innovation},
  file = {articles/Vicente-Saez2018.pdf}
}

@article{Frank2018,
  title = {Hotspots of Dendritic Spine Turnover Facilitate Clustered Spine Addition and Learning and Memory},
  volume = {9},
  copyright = {2018 The Author(s)},
  issn = {2041-1723},
  doi = {10.1038/s41467-017-02751-2},
  abstract = {$<$p$>$Structural remodeling of dendritic spines is thought to be a mechanism of memory storage. Here, the authors look at how spine turnover and clustering predict future learning and memory performance, and see that a genetically modified mouse with enhanced spine turnover has enhanced learning.$<$/p$>$},
  language = {En},
  number = {1},
  journal = {Nature Communications},
  author = {Frank, Adam C. and Huang, Shan and Zhou, Miou and Gdalyahu, Amos and Kastellakis, George and Silva, Tawnie K. and Lu, Elaine and Wen, Ximiao and Poirazi, Panayiota and Trachtenberg, Joshua T. and Silva, Alcino J.},
  month = jan,
  year = {2018},
  pages = {422},
  file = {articles/Frank2018.pdf}
}

@article{Sammons2018,
  title = {Size-{{Dependent Axonal Bouton Dynamics}} Following {{Visual Deprivation In Vivo}}},
  volume = {22},
  issn = {2211-1247},
  doi = {10.1016/j.celrep.2017.12.065},
  language = {English},
  number = {3},
  journal = {Cell Reports},
  author = {Sammons, Rosanna P. and Clopath, Claudia and Barnes, Samuel J.},
  month = jan,
  year = {2018},
  keywords = {LTP,homeostasis,network,plasticity,visual cortex,axonal bouton,GCaMP,population coupling,presynaptic,sensory deprivation},
  pages = {576-584},
  file = {articles/Sammons2018.pdf},
  pmid = {29346758}
}

@article{Ziv2018,
  title = {Synaptic {{Tenacity}} or {{Lack Thereof}}: {{Spontaneous Remodeling}} of {{Synapses}}},
  volume = {41},
  issn = {0166-2236, 1878-108X},
  shorttitle = {Synaptic {{Tenacity}} or {{Lack Thereof}}},
  doi = {10.1016/j.tins.2017.12.003},
  language = {English},
  number = {2},
  journal = {Trends in Neurosciences},
  author = {Ziv, Noam E. and Brenner, Naama},
  month = feb,
  year = {2018},
  keywords = {synaptic plasticity,stochastic processes,synaptic remodeling,synaptic tenacity},
  pages = {89-99},
  file = {articles/Ziv2018.pdf},
  pmid = {29275902}
}

@article{Kappel2015,
  title = {Network {{Plasticity}} as {{Bayesian Inference}}},
  volume = {11},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004485},
  abstract = {Author Summary Synaptic connectivity between neurons in the brain and the efficacies (``weights'') of these synaptic connections are thought to encode the long-term memory of an organism. But a closer look at their molecular implementation, as well as imaging experiments over longer periods of time, have shown that synaptic connections are subject to numerous stochastic processes. We propose that this seeming unreliability of synaptic connections is not a bug, but an important feature. It endows networks of neurons with an important experimentally observed but theoretically not understood capability: Automatic compensation for internal and external changes. This perspective of network plasticity requires a new conceptual and mathematical framework, which is provided by this article. Stochasticity of synapses is seen here not as noise of an inherently deterministic system, but as an inherent property, similarly as Brownian motion of particles in a physical system cannot be abstracted away if one wants to understand certain properties of a physical system. In fact, we find that this underlying stochasticity of synaptic connections enables a network of neurons to continuously try out new network configurations while maintaining its functionality.},
  language = {en},
  number = {11},
  journal = {PLOS Computational Biology},
  author = {Kappel, David and Habenschuss, Stefan and Legenstein, Robert and Maass, Wolfgang},
  month = nov,
  year = {2015},
  keywords = {Action potentials,Network analysis,Neuronal plasticity,Neuronal tuning,Neurons,Synapses,Neural networks,Synaptic plasticity},
  pages = {e1004485},
  file = {articles/Kappel2015.pdf}
}

@article{Kaufman2012,
  title = {Long-Term {{Relationships}} between {{Cholinergic Tone}}, {{Synchronous Bursting}} and {{Synaptic Remodeling}}},
  volume = {7},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0040980},
  abstract = {Cholinergic neuromodulation plays key roles in the regulation of neuronal excitability, network activity, arousal, and behavior. On longer time scales, cholinergic systems play essential roles in cortical development, maturation, and plasticity. Presumably, these processes are associated with substantial synaptic remodeling, yet to date, long-term relationships between cholinergic tone and synaptic remodeling remain largely unknown. Here we used automated microscopy combined with multielectrode array recordings to study long-term relationships between cholinergic tone, excitatory synapse remodeling, and network activity characteristics in networks of cortical neurons grown on multielectrode array substrates. Experimental elevations of cholinergic tone led to the abrupt suppression of episodic synchronous bursting activity (but not of general activity), followed by a gradual growth of excitatory synapses over hours. Subsequent blockage of cholinergic receptors led to an immediate restoration of synchronous bursting and the gradual reversal of synaptic growth. Neither synaptic growth nor downsizing was governed by multiplicative scaling rules. Instead, these occurred in a subset of synapses, irrespective of initial synaptic size. Synaptic growth seemed to depend on intrinsic network activity, but not on the degree to which bursting was suppressed. Intriguingly, sustained elevations of cholinergic tone were associated with a gradual recovery of synchronous bursting but not with a reversal of synaptic growth. These findings show that cholinergic tone can strongly affect synaptic remodeling and synchronous bursting activity, but do not support a strict coupling between the two. Finally, the reemergence of synchronous bursting in the presence of elevated cholinergic tone indicates that the capacity of cholinergic neuromodulation to indefinitely suppress synchronous bursting might be inherently limited.},
  language = {en},
  number = {7},
  journal = {PLOS ONE},
  author = {Kaufman, Maya and Corner, Michael A. and Ziv, Noam E.},
  month = jul,
  year = {2012},
  keywords = {Action potentials,Network analysis,Neuronal dendrites,Neurons,Synapses,Neural networks,Cholinergics,Sleep},
  pages = {e40980},
  file = {articles/Kaufman2012.pdf}
}

@article{Ross-Hellauer2017,
  title = {Survey on Open Peer Review: {{Attitudes}} and Experience amongst Editors, Authors and Reviewers},
  volume = {12},
  issn = {1932-6203},
  shorttitle = {Survey on Open Peer Review},
  doi = {10.1371/journal.pone.0189311},
  abstract = {Open peer review (OPR) is a cornerstone of the emergent Open Science agenda. Yet to date no large-scale survey of attitudes towards OPR amongst academic editors, authors, reviewers and publishers has been undertaken. This paper presents the findings of an online survey, conducted for the OpenAIRE2020 project during September and October 2016, that sought to bridge this information gap in order to aid the development of appropriate OPR approaches by providing evidence about attitudes towards and levels of experience with OPR. The results of this cross-disciplinary survey, which received 3,062 full responses, show the majority (60.3\%) of respondents to be believe that OPR as a general concept should be mainstream scholarly practice (although attitudes to individual traits varied, and open identities peer review was not generally favoured). Respondents were also in favour of other areas of Open Science, like Open Access (88.2\%) and Open Data (80.3\%). Among respondents we observed high levels of experience with OPR, with three out of four (76.2\%) reporting having taken part in an OPR process as author, reviewer or editor. There were also high levels of support for most of the traits of OPR, particularly open interaction, open reports and final-version commenting. Respondents were against opening reviewer identities to authors, however, with more than half believing it would make peer review worse. Overall satisfaction with the peer review system used by scholarly journals seems to strongly vary across disciplines. Taken together, these findings are very encouraging for OPR's prospects for moving mainstream but indicate that due care must be taken to avoid a ``one-size fits all'' solution and to tailor such systems to differing (especially disciplinary) contexts. OPR is an evolving phenomenon and hence future studies are to be encouraged, especially to further explore differences between disciplines and monitor the evolution of attitudes.},
  language = {en},
  number = {12},
  journal = {PLOS ONE},
  author = {{Ross-Hellauer}, Tony and Deppe, Arvid and Schmidt, Birgit},
  month = dec,
  year = {2017},
  keywords = {Open science,Agriculture,Ecology and environmental sciences,Open access publishing,Peer review,Scientific publishing,Social sciences,Surveys},
  pages = {e0189311},
  file = {articles/Ross-Hellauer22.pdf}
}

@article{Hutson2018,
  title = {Artificial Intelligence Faces Reproducibility Crisis},
  volume = {359},
  copyright = {Copyright \textcopyright{} 2018 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.359.6377.725},
  abstract = {The booming field of artificial intelligence (AI) is grappling with a replication crisis, much like the ones that have afflicted psychology, medicine, and other fields over the past decade. Just because algorithms are based on code doesn't mean experiments are easily replicated. Far from it. Unpublished codes and a sensitivity to training conditions have made it difficult for AI researchers to reproduce many key results. That is leading to a new conscientiousness about research methods and publication protocols. Last week, at a meeting of the Association for the Advancement of Artificial Intelligence in New Orleans, Louisiana, reproducibility was on the agenda, with some teams diagnosing the problem\textemdash{}and one laying out tools to mitigate it.
Unpublished code and sensitivity to training conditions make many claims hard to verify.
Unpublished code and sensitivity to training conditions make many claims hard to verify.},
  language = {en},
  number = {6377},
  journal = {Science},
  author = {Hutson, Matthew},
  month = feb,
  year = {2018},
  pages = {725-726},
  file = {articles/Hutson2018.pdf}
}

@article{Bon2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.08008},
  primaryClass = {cs},
  title = {Novel Processes and Metrics for a Scientific Evaluation Rooted in the Principles of Science - {{Version}} 1},
  abstract = {Scientific evaluation is a determinant of how scientists, institutions and funders behave, and as such is a key element in the making of science. In this article, we propose an alternative to the current norm of evaluating research with journal rank. Following a well-defined notion of scientific value, we introduce qualitative processes that can also be quantified and give rise to meaningful and easy-to-use article-level metrics. In our approach, the goal of a scientist is transformed from convincing an editorial board through a vertical process to convincing peers through an horizontal one. We argue that such an evaluation system naturally provides the incentives and logic needed to constantly promote quality, reproducibility, openness and collaboration in science. The system is legally and technically feasible and can gradually lead to the self-organized reappropriation of the scientific process by the scholarly community and its institutions. We propose an implementation of our evaluation system with the platform "the Self-Journals of Science" (www.sjscience.org).},
  journal = {arXiv:1701.08008 [cs]},
  author = {Bon, Micha\"el and Taylor, Michael and McDowell, Gary S.},
  month = jan,
  year = {2017},
  keywords = {Computer Science - Digital Libraries,68-02,H.3.7; H.5.3},
  file = {articles/Bon2017.pdf}
}

@article{Rabinowitz2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.07740},
  primaryClass = {cs},
  title = {Machine {{Theory}} of {{Mind}}},
  abstract = {Theory of mind (ToM; Premack \& Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test (Wimmer \& Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.},
  journal = {arXiv:1802.07740 [cs]},
  author = {Rabinowitz, Neil C. and Perbet, Frank and Song, H. Francis and Zhang, Chiyuan and Eslami, S. M. Ali and Botvinick, Matthew},
  month = feb,
  year = {2018},
  keywords = {Computer Science - Artificial Intelligence},
  file = {articles/Rabinowitz2018.pdf}
}

@article{Braganza2018,
  title = {The {{Circuit Motif}} as a {{Conceptual Tool}} for {{Multilevel Neuroscience}}},
  volume = {41},
  issn = {0166-2236, 1878-108X},
  doi = {10.1016/j.tins.2018.01.002},
  language = {English},
  number = {3},
  journal = {Trends in Neurosciences},
  author = {Braganza, Oliver and Beck, Heinz},
  month = mar,
  year = {2018},
  keywords = {inhibition,behavior,circuit motif,high-dimensional research,multilevel neuroscience,optogenetics},
  pages = {128-136},
  file = {articles/Braganza2018.pdf},
  pmid = {29397990}
}

@article{Kaiser2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1105.4705},
  title = {A {{Tutorial}} in {{Connectome Analysis}}: {{Topological}} and {{Spatial Features}} of {{Brain Networks}}},
  volume = {57},
  issn = {10538119},
  shorttitle = {A {{Tutorial}} in {{Connectome Analysis}}},
  doi = {10.1016/j.neuroimage.2011.05.025},
  abstract = {High-throughput methods for yielding the set of connections in a neural system, the connectome, are now being developed. This tutorial describes ways to analyze the topological and spatial organization of the connectome at the macroscopic level of connectivity between brain regions as well as the microscopic level of connectivity between neurons. We will describe topological features at three different levels: the local scale of individual nodes, the regional scale of sets of nodes, and the global scale of the complete set of nodes in a network. Such features can be used to characterize components of a network and to compare different networks, e.g. the connectome of patients and control subjects for clinical studies. At the global scale, different types of networks can be distinguished and we will describe Erd$\backslash$"os-R$\backslash$'enyi random, scale-free, small-world, modular, and hierarchical archetypes of networks. Finally, the connectome also has a spatial organization and we describe methods for analyzing wiring lengths of neural systems. As an introduction for new researchers in the field of connectome analysis, we discuss the benefits and limitations of each analysis approach.},
  number = {3},
  journal = {NeuroImage},
  author = {Kaiser, Marcus},
  month = aug,
  year = {2011},
  keywords = {Quantitative Biology - Neurons and Cognition,Computer Science - Social and Information Networks,Physics - Physics and Society},
  pages = {892-907},
  file = {articles/Kaiser2011.pdf}
}

@article{Tannenbaum2017,
  title = {Theory of Nonstationary {{Hawkes}} Processes},
  volume = {96},
  doi = {10.1103/PhysRevE.96.062314},
  abstract = {We expand the theory of Hawkes processes to the nonstationary case, in which the mutually exciting point processes receive time-dependent inputs. We derive an analytical expression for the time-dependent correlations, which can be applied to networks with arbitrary connectivity, and inputs with arbitrary statistics. The expression shows how the network correlations are determined by the interplay between the network topology, the transfer functions relating units within the network, and the pattern and statistics of the external inputs. We illustrate the correlation structure using several examples in which neural network dynamics are modeled as a Hawkes process. In particular, we focus on the interplay between internally and externally generated oscillations and their signatures in the spike and rate correlation functions.},
  number = {6},
  journal = {Physical Review E},
  author = {Tannenbaum, Neta Ravid and Burak, Yoram},
  month = dec,
  year = {2017},
  pages = {062314},
  file = {articles/Tannenbaum2017.pdf}
}

@article{Marwick2017,
  title = {Computational {{Reproducibility}} in {{Archaeological Research}}: {{Basic Principles}} and a {{Case Study}} of {{Their Implementation}}},
  volume = {24},
  issn = {1072-5369, 1573-7764},
  shorttitle = {Computational {{Reproducibility}} in {{Archaeological Research}}},
  doi = {10.1007/s10816-015-9272-9},
  abstract = {The use of computers and complex software is pervasive in archaeology, yet their role in the analytical pipeline is rarely exposed for other researchers to inspect or reuse. This limits the progress of archaeology because researchers cannot easily reproduce each other's work to verify or extend it. Four general principles of reproducible research that have emerged in other fields are presented. An archaeological case study is described that shows how each principle can be implemented using freely available software. The costs and benefits of implementing reproducible research are assessed. The primary benefit, of sharing data in particular, is increased impact via an increased number of citations. The primary cost is the additional time required to enhance reproducibility, although the exact amount is difficult to quantify.},
  language = {en},
  number = {2},
  journal = {Journal of Archaeological Method and Theory},
  author = {Marwick, Ben},
  month = jun,
  year = {2017},
  pages = {424-450},
  file = {articles/Marwick2017.pdf}
}

@article{Berkowitz2018a,
  title = {Decoding Neural Responses with Minimal Information Loss},
  copyright = {\textcopyright{} 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  doi = {10.1101/273854},
  abstract = {Cortical tissue has a circuit motif termed the cortical column, which is thought to represent its basic computational unit but whose function remains unclear. Here we propose, and show quantitative evidence, that the cortical column performs computations necessary to decode neural activity with minimal information loss. The cortical decoder achieves higher accuracy compared to simpler decoders found in invertebrate and subcortical circuits by incorporating specific recurrent network dynamics. This recurrent dynamics also makes it possible to choose between alternative stimulus categories. The structure of cortical decoder predicts quadratic dependence of cortex size relative to subcortical parts of the brain. We quantitatively verify this relationship using anatomical data across mammals. The results offer a new perspective on the evolution and computational function of cortical columns.},
  language = {en},
  journal = {bioRxiv},
  author = {Berkowitz, John and Sharpee, Tatyana},
  month = feb,
  year = {2018},
  pages = {273854},
  file = {articles/Berkowitz2018.pdf}
}

@article{Benureau2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.08205},
  primaryClass = {cs},
  title = {Re-Run, {{Repeat}}, {{Reproduce}}, {{Reuse}}, {{Replicate}}: {{Transforming Code}} into {{Scientific Contributions}}},
  shorttitle = {Re-Run, {{Repeat}}, {{Reproduce}}, {{Reuse}}, {{Replicate}}},
  abstract = {Scientific code is not production software. Scientific code participates in the evaluation of a scientific hypothesis. This imposes specific constraints on the code that are often overlooked in practice. We articulate, with a small example, five characteristics that a scientific code in computational science should possess: re-runnable, repeatable, reproducible, reusable and replicable.},
  journal = {arXiv:1708.08205 [cs]},
  author = {Benureau, Fabien and Rougier, Nicolas},
  month = aug,
  year = {2017},
  keywords = {Computer Science - Computers and Society,Computer Science - General Literature},
  file = {articles/Benureau2017.pdf}
}

@article{Gao2017,
  title = {A Theory of Multineuronal Dimensionality, Dynamics and Measurement},
  copyright = {\textcopyright{} 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  doi = {10.1101/214262},
  abstract = {In many experiments, neuroscientists tightly control behavior, record many trials, and obtain trial-averaged firing rates from hundreds of neurons in circuits containing billions of behaviorally relevant neurons. Dimensionality reduction methods reveal a striking simplicity underlying such multi-neuronal data: they can be reduced to a low-dimensional space, and the resulting neural trajectories in this space yield a remarkably insightful dynamical portrait of circuit computation. This simplicity raises profound and timely conceptual questions. What are its origins and its implications for the complexity of neural dynamics? How would the situation change if we recorded more neurons? When, if at all, can we trust dynamical portraits obtained from measuring an infinitesimal fraction of task relevant neurons? We present a theory that answers these questions, and test it using physiological recordings from reaching monkeys. This theory reveals conceptual insights into how task complexity governs both neural dimensionality and accurate recovery of dynamic portraits, thereby providing quantitative guidelines for future large-scale experimental design.},
  language = {en},
  journal = {bioRxiv},
  author = {Gao, Peiran and Trautmann, Eric and Yu, Byron M. and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna and Ganguli, Surya},
  month = nov,
  year = {2017},
  pages = {214262},
  file = {articles/Gao2017.pdf}
}

@article{Bannach-Brown2018,
  title = {The Use of Text-Mining and Machine Learning Algorithms in Systematic Reviews: Reducing Workload in Preclinical Biomedical Sciences and Reducing Human Screening Error},
  copyright = {\textcopyright{} 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
  shorttitle = {The Use of Text-Mining and Machine Learning Algorithms in Systematic Reviews},
  doi = {10.1101/255760},
  abstract = {Background: In this paper we outline a method of applying machine learning (ML) algorithms to aid citation screening in an on-going broad and shallow systematic review, with the aim of achieving a high performing algorithm comparable to human screening. Methods: We tested a range of machine learning algorithms. We applied ML algorithms to incremental numbers of training records and recorded the performance on sensitivity and specificity on an unseen validation set of papers. The performance of these algorithms was assessed on measures of recall, specificity, and accuracy. The classification results of the best performing algorithm was taken forward and applied to the remaining unseen records in the dataset and will be taken forward to the next stage of systematic review. ML was used to identify potential human errors during screening by analysing the training and validation datasets against the machine-ranked score. Results: We found that ML algorithms perform at a desirable level. Classifiers reached 98.7\% sensitivity based on learning from a training set of 5749 records, with an inclusion prevalence of 13.2\%. The highest level of specificity reached was 86\%. Human errors in the training and validation set were successfully identified using ML scores to highlight discrepancies. Training the ML algorithm on the corrected dataset improved the specificity of the algorithm without compromising sensitivity. Error analysis sees a 3\% increase or change in sensitivity and specificity, which increases precision and accuracy of the ML algorithm. Conclusions: The technique of using ML to identify human error needs to be investigated in more depth, however this pilot shows a promising approach to integrating human decisions and automation in systematic review methodology.},
  language = {en},
  journal = {bioRxiv},
  author = {{Bannach-Brown}, Alexandra and Przyby\l{}a, Piotr and Thomas, James and Rice, Andrew S. C. and Ananiadou, Sophia and Liao, Jing and Macleod, Malcolm Robert},
  month = jan,
  year = {2018},
  pages = {255760},
  file = {articles/Bannach-Brown2018.pdf}
}

@article{Mikaitis2018,
  title = {Neuromodulated {{Synaptic Plasticity}} on the {{SpiNNaker Neuromorphic System}}},
  volume = {12},
  issn = {1662-453X},
  doi = {10.3389/fnins.2018.00105},
  abstract = {SpiNNaker is a digital neuromorphic architecture, designed specifically for the low power simulation of large-scale spiking neural networks at speeds close to biological real-time. Unlike other neuromorphic systems, SpiNNaker allows users to develop their own neuron and synapse models as well as specify arbitrary connectivity. As a result SpiNNaker has proved to be a powerful tool for studying different neuron models as well as synaptic plasticity -- believed to be one of the main mechanisms behind learning and memory in the brain. A number of Spike-Timing-Dependent-Plasticity(STDP) rules have already been implemented on SpiNNaker and have been shown to be capable of solving various learning tasks in real-time. However, while STDP is an important biological theory of learning, it is a form of Hebbian or unsupervised learning and therefore does not explain behaviours that depend on feedback from the environement. Instead, learning rules based on neuromodulated STDP (three-factor learning rules) have been shown to be capable of solving reinforcement learning tasks in a biologically plausible manner. In this paper we demonstrate for the first time how a model of three-factor STDP, with the third-factor representing spikes from dopaminergic neurons, can be implemented on the SpiNNaker neuromorphic system. Using this learning rule we first show how reward and punishment signals can be delivered to a single synapse before going on to demonstrate it in a larger network which solves the credit assignment problem in a Pavlovian conditioning experiment. Because of its extra complexity, we find that our three-factor learning rule requires approximately 2x as much processing time as the existing SpiNNaker STDP learning rules. However, we show that it is still possible to run our Pavlovian conditioning model with up to 10000 neurons in real-time, opening up new research opportunities for modelling behavioural learning on SpiNNaker.},
  language = {English},
  journal = {Frontiers in Neuroscience},
  author = {Mikaitis, Mantas and Pineda Garc\'ia, Garibaldi and Knight, James C. and Furber, Steve B.},
  year = {2018},
  keywords = {STDP,behavioural learning,Neuromodulation,reinforcement learning,SpiNNaker,three-factor learning rules},
  file = {articles/Mikaitis2018.pdf}
}

@article{Benureau2018,
  title = {Re-Run, {{Repeat}}, {{Reproduce}}, {{Reuse}}, {{Replicate}}: {{Transforming Code}} into {{Scientific Contributions}}},
  volume = {11},
  issn = {1662-5196},
  shorttitle = {Re-Run, {{Repeat}}, {{Reproduce}}, {{Reuse}}, {{Replicate}}},
  doi = {10.3389/fninf.2017.00069},
  abstract = {Scientific code is different from production software. Scientific code, by producing results that are then analyzed and interpreted, participates in the elaboration of scientific conclusions. This imposes specific constraints on the code that are often overlooked in practice. We articulate, with a small example, five characteristics that a scientific code in computational science should possess: re-runnable, repeatable, reproducible, reusable, and replicable. The code should be executable (re-runnable) and produce the same result more than once (repeatable); it should allow an investigator to reobtain the published results (reproducible) while being easy to use, understand and modify (reusable), and it should act as an available reference for any ambiguity in the algorithmic descriptions of the article (replicable).},
  journal = {Frontiers in Neuroinformatics},
  author = {Benureau, Fabien C. Y. and Rougier, Nicolas P.},
  month = jan,
  year = {2018},
  file = {articles/Benureau2018.pdf}
}

@article{Collberg2013,
  title = {Measuring {{Reproducibility}} in {{Computer Systems Research}}},
  abstract = {We describe a study into the willingness of Computer Systems researchers to share their code and data. We find that . . . . We also propose a novel sharing specification scheme that will require researchers to specify the level of reproducibility that reviewers and readers can assume from a paper either submitted for publication, or published.},
  author = {Collberg, Christian and Proebsting, Todd and Moraila, Gina and Shankaran, Akash and Zuoming, Shi and Warren, Alex M},
  month = dec,
  year = {2013},
  file = {articles/Collberg2013.pdf}
}

@article{Balleine2010,
  title = {Human and {{Rodent Homologies}} in {{Action Control}}: {{Corticostriatal Determinants}} of {{Goal}}-{{Directed}} and {{Habitual Action}}},
  volume = {35},
  copyright = {2009 Nature Publishing Group},
  issn = {1740-634X},
  shorttitle = {Human and {{Rodent Homologies}} in {{Action Control}}},
  doi = {10.1038/npp.2009.131},
  abstract = {Recent behavioral studies in both humans and rodents have found evidence that performance in decision-making tasks depends on two different learning processes; one encoding the relationship between actions and their consequences and a second involving the formation of stimulus\textendash{}response associations. These learning processes are thought to govern goal-directed and habitual actions, respectively, and have been found to depend on homologous corticostriatal networks in these species. Thus, recent research using comparable behavioral tasks in both humans and rats has implicated homologous regions of cortex (medial prefrontal cortex/medial orbital cortex in humans and prelimbic cortex in rats) and of dorsal striatum (anterior caudate in humans and dorsomedial striatum in rats) in goal-directed action and in the control of habitual actions (posterior lateral putamen in humans and dorsolateral striatum in rats). These learning processes have been argued to be antagonistic or competing because their control over performance appears to be all or none. Nevertheless, evidence has started to accumulate suggesting that they may at times compete and at others cooperate in the selection and subsequent evaluation of actions necessary for normal choice performance. It appears likely that cooperation or competition between these sources of action control depends not only on local interactions in dorsal striatum but also on the cortico-basal ganglia network within which the striatum is embedded and that mediates the integration of learning with basic motivational and emotional processes. The neural basis of the integration of learning and motivation in choice and decision-making is still controversial and we review some recent hypotheses relating to this issue.},
  language = {en},
  number = {1},
  journal = {Neuropsychopharmacology},
  author = {Balleine, Bernard W. and O'Doherty, John P.},
  month = jan,
  year = {2010},
  pages = {48-69},
  file = {articles/Balleine2010.pdf}
}

@article{Eglen2016,
  title = {Bivariate Spatial Point Patterns in the Retina: A Reproducible Review},
  copyright = {Attribution 2.0 UK: England \& Wales},
  shorttitle = {Bivariate Spatial Point Patterns in the Retina},
  abstract = {In this article I present a reproducible review of recent research to investigate the spatial positioning of neurons in the nervous system. In particular, I focus on the relative spatial positioning of pairs of cell types within the retina. I examine three different cases by which two types of neurons might be arranged relative to each other. (1) Cells of different type might be effectively independent of each other. (2) Cells of one type are randomly assigned one of two labels to create two related populations. (3) Interactions between cells of different type generate functional dependencies. I show briefly how spatial statistic techniques can be applied to investigate the nature of spatial interactions between two cell types. Finally, I have termed this article a `reproducible review' because all the data and computer code are integrated into the manuscript so that others can repeat the analysis presented here. I close the review with a discussion of this concept.},
  language = {en},
  author = {Eglen, Stephen J.},
  month = apr,
  year = {2016},
  file = {articles/Eglen2016.pdf}
}

@article{Mesnard2017,
  title = {Reproducible and {{Replicable Computational Fluid Dynamics}}: {{It}}'s {{Harder Than You Think}}},
  volume = {19},
  issn = {1521-9615},
  shorttitle = {Reproducible and {{Replicable Computational Fluid Dynamics}}},
  doi = {10.1109/MCSE.2017.3151254},
  number = {4},
  journal = {Computing in Science \& Engineering},
  author = {Mesnard, Olivier and Barba, Lorena A.},
  month = jul,
  year = {2017},
  pages = {44-55},
  file = {articles/Mesnard2017.pdf}
}

@article{Collberg2015,
  title = {Repeatability and {{Benefaction}} in {{Computer Systems Research}}},
  abstract = {We describe a study into the extent to which Computer Systems researchers share their code and data and the extent to which such code builds. Starting with 601 papers from ACM conferences and journals, we examine 402 papers whose results were backed by code. For 32.3\% of these papers we were able to obtain the code and build it within 30 minutes; for 48.3\% of the papers we managed to build the code, but it may have required extra effort; for 54.0\% of the papers either we managed to build the code or the authors stated the code would build with reasonable effort. We also propose a novel sharing specification scheme that requires researchers to specify the level of sharing that reviewers and readers can assume from a paper.},
  author = {Collberg, Christian and Proebsting, Todd and Warren, Alex M},
  month = feb,
  year = {2015},
  file = {articles/Collberg2015.pdf}
}

@article{Collberg2016,
  title = {Repeatability in {{Computer Systems Research}}},
  volume = {59},
  issn = {0001-0782},
  doi = {10.1145/2812803},
  abstract = {To encourage repeatable research, fund repeatability engineering and reward commitments to sharing research artifacts.},
  number = {3},
  journal = {Commun. ACM},
  author = {Collberg, Christian and Proebsting, Todd A.},
  month = feb,
  year = {2016},
  pages = {62--69},
  file = {articles/Collberg2016.pdf}
}

@article{Alstott2014,
  title = {Powerlaw: {{A Python Package}} for {{Analysis}} of {{Heavy}}-{{Tailed Distributions}}},
  volume = {9},
  issn = {1932-6203},
  shorttitle = {Powerlaw},
  doi = {10.1371/journal.pone.0085777},
  abstract = {Power laws are theoretically interesting probability distributions that are also frequently used to describe empirical data. In recent years, effective statistical methods for fitting power laws have been developed, but appropriate use of these techniques requires significant programming and statistical insight. In order to greatly decrease the barriers to using good statistical methods for fitting power law distributions, we developed the powerlaw Python package. This software package provides easy commands for basic fitting and statistical analysis of distributions. Notably, it also seeks to support a variety of user needs by being exhaustive in the options available to the user. The source code is publicly available and easily extensible.},
  language = {en},
  number = {1},
  journal = {PLOS ONE},
  author = {Alstott, Jeff and Bullmore, Ed and Plenz, Dietmar},
  month = jan,
  year = {2014},
  keywords = {Neurons,Probability distribution,Statistical distributions,Data visualization,Source code,Software tools,Semantics,Simulation and modeling},
  pages = {e85777},
  file = {articles/Alstott22.pdf}
}

@article{Klaus2011,
  title = {Statistical {{Analyses Support Power Law Distributions Found}} in {{Neuronal Avalanches}}},
  volume = {6},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0019779},
  abstract = {The size distribution of neuronal avalanches in cortical networks has been reported to follow a power law distribution with exponent close to -1.5, which is a reflection of long-range spatial correlations in spontaneous neuronal activity. However, identifying power law scaling in empirical data can be difficult and sometimes controversial. In the present study, we tested the power law hypothesis for neuronal avalanches by using more stringent statistical analyses. In particular, we performed the following steps: (i) analysis of finite-size scaling to identify scale-free dynamics in neuronal avalanches, (ii) model parameter estimation to determine the specific exponent of the power law, and (iii) comparison of the power law to alternative model distributions. Consistent with critical state dynamics, avalanche size distributions exhibited robust scaling behavior in which the maximum avalanche size was limited only by the spatial extent of sampling (``finite size'' effect). This scale-free dynamics suggests the power law as a model for the distribution of avalanche sizes. Using both the Kolmogorov-Smirnov statistic and a maximum likelihood approach, we found the slope to be close to -1.5, which is in line with previous reports. Finally, the power law model for neuronal avalanches was compared to the exponential and to various heavy-tail distributions based on the Kolmogorov-Smirnov distance and by using a log-likelihood ratio test. Both the power law distribution without and with exponential cut-off provided significantly better fits to the cluster size distributions in neuronal avalanches than the exponential, the lognormal and the gamma distribution. In summary, our findings strongly support the power law scaling in neuronal avalanches, providing further evidence for critical state dynamics in superficial layers of cortex.},
  language = {en},
  number = {5},
  journal = {PLOS ONE},
  author = {Klaus, Andreas and Yu, Shan and Plenz, Dietmar},
  month = may,
  year = {2011},
  keywords = {Probability distribution,Statistical distributions,Statistical models,Anesthesia,Electrode recording,Microelectrodes,Monkeys,Test statistics},
  pages = {e19779},
  file = {articles/Klaus22.pdf}
}

@article{Clauset2009,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0706.1062},
  title = {Power-Law Distributions in Empirical Data},
  volume = {51},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/070710111},
  abstract = {Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution -- the part of the distribution representing large but rare events -- and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.},
  number = {4},
  journal = {SIAM Review},
  author = {Clauset, Aaron and Shalizi, Cosma Rohilla and Newman, M. E. J.},
  month = nov,
  year = {2009},
  keywords = {Physics - Data Analysis; Statistics and Probability,Condensed Matter - Disordered Systems and Neural Networks,Statistics - Applications,Statistics - Methodology},
  pages = {661-703},
  file = {articles/Clauset22.pdf}
}

@book{Press2007,
  address = {Cambridge, UK; New York},
  title = {Numerical Recipes: The Art of Scientific Computing},
  isbn = {978-0-511-33555-6},
  shorttitle = {Numerical Recipes},
  language = {English},
  publisher = {{Cambridge University Press}},
  author = {Press, William H},
  year = {2007},
  file = {books/Press2007_Numerical-recipes-the-art-of-scientific-computing.pdf},
  note = {OCLC: 212427139}
}

@article{Brette2017,
  title = {Is Coding a Relevant Metaphor for the Brain?},
  copyright = {\textcopyright{} 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  doi = {10.1101/168237},
  abstract = {"Neural coding" is a popular metaphor in neuroscience, where objective properties of the world are communicated to the brain in the form of spikes. Here I argue that this metaphor is often inappropriate and misleading. First, when neurons are said to encode experimental parameters, the implied communication channel consists of both the experimental and biological system. Thus, the terms "neural code" are used inappropriately when "neuroexperimental code" would be more accurate, although less insightful. Second, the brain cannot be presumed to decode neural messages into objective properties of the world, since it never gets to observe those properties. To avoid dualism, codes must relate not to external properties but to internal sensorimotor models. Because this requires structured representations, neural assemblies cannot be the basis of such codes. Third, a message is informative to the extent that the reader understands its language. But the neural code is private to the encoder since only the message is communicated: each neuron speaks its own language. It follows that in the neural coding metaphor, the brain is a Tower of Babel. Finally, the relation between input signals and actions is circular; that inputs do not preexist to outputs makes the coding paradigm problematic. I conclude that the view that spikes are messages is generally not tenable. An alternative proposition is that action potentials are actions on other neurons and the environment, and neurons interact with each other rather than exchange messages.},
  language = {en},
  journal = {bioRxiv},
  author = {Brette, Romain},
  month = jul,
  year = {2017},
  pages = {168237},
  file = {articles/Brette2017.pdf}
}

@article{Zhu2017,
  title = {Joint {{Learning}} of {{Binocularly Driven Saccades}} and {{Vergence}} by {{Active Efficient Coding}}},
  volume = {11},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2017.00058},
  abstract = {This paper investigates two types of eye movements: vergence and saccades. Vergence eye movements are responsible for bringing the images of the two eyes into correspondence whereas saccades drive gaze to interesting regions in the scene. Control of both vergence and saccades develop during early infancy. To date, these two types of eye movements have been studied separately. Here we propose a computational model of an active vision system that integrates these two types of eye movements. We hypothesize that incorporating a saccade strategy driven by bottom-up attention will benefit the development of vergence control. The integrated system is based on the Active Efficient Coding framework, which describes the joint development of sensory processing and eye movement control to jointly optimize the coding efficiency of the sensory system. In the integrated system, we propose a binocular saliency model to drive saccades based on learned binocular feature extractors, which simultaneously encode both depth and texture information. Saliency in our model also depends on the current fixation point. This extends prior work, which focused on monocular images and saliency measures that are independent of the current fixation. Our results show that the proposed saliency driven saccades lead to better vergence performance and faster learning in the overall system than random saccades. Faster learning is significant because it indicates that the system actively selects inputs for the most effective learning. This work suggests that saliency driven saccades provide a scaffold for the development of vergence control during infancy.},
  language = {English},
  journal = {Frontiers in Neurorobotics},
  author = {Zhu, Qingpeng and Triesch, Jochen and Shi, Bertram E.},
  year = {2017},
  keywords = {reinforcement learning,Active Efficient Coding,binocular saliency map,GASSOM,Saccades,vergence},
  file = {articles/Zhu2017.pdf}
}

@article{Nessler2013,
  title = {Bayesian {{Computation Emerges}} in {{Generic Cortical Microcircuits}} through {{Spike}}-{{Timing}}-{{Dependent Plasticity}}},
  volume = {9},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003037},
  abstract = {The principles by which networks of neurons compute, and how spike-timing dependent plasticity (STDP) of synaptic weights generates and maintains their computational function, are unknown. Preceding work has shown that soft winner-take-all (WTA) circuits, where pyramidal neurons inhibit each other via interneurons, are a common motif of cortical microcircuits. We show through theoretical analysis and computer simulations that Bayesian computation is induced in these network motifs through STDP in combination with activity-dependent changes in the excitability of neurons. The fundamental components of this emergent Bayesian computation are priors that result from adaptation of neuronal excitability and implicit generative models for hidden causes that are created in the synaptic weights through STDP. In fact, a surprising result is that STDP is able to approximate a powerful principle for fitting such implicit generative models to high-dimensional spike inputs: Expectation Maximization. Our results suggest that the experimentally observed spontaneous activity and trial-to-trial variability of cortical neurons are essential features of their information processing capability, since their functional role is to represent probability distributions rather than static neural codes. Furthermore it suggests networks of Bayesian computation modules as a new model for distributed information processing in the cortex.},
  language = {en},
  number = {4},
  journal = {PLOS Computational Biology},
  author = {Nessler, Bernhard and Pfeiffer, Michael and Buesing, Lars and Maass, Wolfgang},
  month = apr,
  year = {2013},
  keywords = {Action potentials,Machine learning,Neuronal plasticity,Neurons,Probability distribution,Synapses,Neural networks,Learning},
  pages = {e1003037},
  file = {articles/Nessler2013.pdf}
}

@article{Savin2010,
  title = {Independent {{Component Analysis}} in {{Spiking Neurons}}},
  volume = {6},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000757},
  abstract = {Although models based on independent component analysis (ICA) have been successful in explaining various properties of sensory coding in the cortex, it remains unclear how networks of spiking neurons using realistic plasticity rules can realize such computation. Here, we propose a biologically plausible mechanism for ICA-like learning with spiking neurons. Our model combines spike-timing dependent plasticity and synaptic scaling with an intrinsic plasticity rule that regulates neuronal excitability to maximize information transmission. We show that a stochastically spiking neuron learns one independent component for inputs encoded either as rates or using spike-spike correlations. Furthermore, different independent components can be recovered, when the activity of different neurons is decorrelated by adaptive lateral inhibition.},
  language = {en},
  number = {4},
  journal = {PLOS Computational Biology},
  author = {Savin, Cristina and Joshi, Prashant and Triesch, Jochen},
  month = apr,
  year = {2010},
  keywords = {Action potentials,Neuronal plasticity,Neurons,Synapses,Transfer functions,Neural networks,Synaptic plasticity,Coding mechanisms},
  pages = {e1000757},
  file = {articles/Savin2010.pdf}
}

@article{Stodden2018,
  title = {An Empirical Analysis of Journal Policy Effectiveness for Computational Reproducibility},
  volume = {115},
  copyright = {\textcopyright{} 2018 . Published under the PNAS license.},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1708290115},
  abstract = {A key component of scientific communication is sufficient information for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that generated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. This work evaluates the effectiveness of journal policy that requires the data and code necessary for reproducibility be made available postpublication by the authors upon request. We assess the effectiveness of such a policy by (i) requesting data and code from authors and (ii) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the journal Science after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44\% of our sample and were able to reproduce the findings for 26\%. We find this policy\textemdash{}author remission of data and code postpublication upon request\textemdash{}an improvement over no policy, but currently insufficient for reproducibility.},
  language = {en},
  number = {11},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun},
  month = mar,
  year = {2018},
  keywords = {code access,data access,open science,reproducibility policy,reproducible research},
  pages = {2584-2589},
  file = {articles/Stodden2018.pdf},
  pmid = {29531050}
}

@article{Lukosevicius2009,
  title = {Reservoir Computing Approaches to Recurrent Neural Network Training},
  volume = {3},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2009.03.005},
  abstract = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current ``brand-names'' of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed ``map'' of it.},
  number = {3},
  journal = {Computer Science Review},
  author = {Luko{\v s}evi{\v c}ius, Mantas and Jaeger, Herbert},
  month = aug,
  year = {2009},
  pages = {127-149},
  file = {articles/Lukoševičius2009.pdf}
}

@article{Maass2002,
  title = {Real-{{Time Computing Without Stable States}}: {{A New Framework}} for {{Neural Computation Based}} on {{Perturbations}}},
  volume = {14},
  issn = {0899-7667},
  shorttitle = {Real-{{Time Computing Without Stable States}}},
  doi = {10.1162/089976602760407955},
  abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
  number = {11},
  journal = {Neural Computation},
  author = {Maass, Wolfgang and Natschl\"ager, Thomas and Markram, Henry},
  month = nov,
  year = {2002},
  pages = {2531-2560},
  file = {articles/Maass2002.pdf}
}

@article{Kietzmann2017,
  title = {Deep {{Neural Networks In Computational Neuroscience}}},
  doi = {10.1101/133504},
  abstract = {The goal of computational neuroscience is to find mechanistic explanations of how the nervous system processes information to support cognitive function and behaviour. At the heart of the field are its models, i.e. mathematical and computational descriptions of the system being studied. These models typically map sensory stimuli to neural responses and/or neural to behavioural responses and range for simple to complex. Recently, deep neural networks (DNNs), using either feedforward and recurrent architectures, have come to dominate several domains of artificial intelligence (AI). As the term ``neural network'' suggests, these models are inspired by biological brains. However, current DNN models abstract from many details of biological neural networks. Their abstractions contribute to their computational efficiency, enabling to perform complex feats of intelligence, ranging from perceptual tasks (e.g. visual object and auditory speech recognition) to cognitive tasks (e.g. machine translation), and on to motor control tasks (e.g. playing computer games or controlling a robot arm). In addition to their ability to model complex intelligent behaviours, DNNs have been shown to predict neural responses to novel sensory stimuli that cannot be predicted with any other currently available type of model. DNNs can have millions of parameters (connection strengths), which are required to capture the domain knowledge needed for task performance. These parameters are often set by task training using stochastic gradient descent. The computational properties of the units are the result of four directly manipulable elements: input statistics, network structure, functional objective, and learning algorithm. The advances with neural nets in engineering provide the technological basis for building task-performing models of varying degrees of biological realism that promise substantial insights for computational neuroscience.},
  author = {Kietzmann, Tim Christian and McClure, Patrick and Kriegeskorte, Nikolaus},
  month = may,
  year = {2017},
  pages = {-},
  file = {articles/Kietzmann2017.pdf}
}

@article{Vu2018,
  title = {A {{Shared Vision}} for {{Machine Learning}} in {{Neuroscience}}},
  volume = {38},
  copyright = {Copyright \textcopyright{} 2018 the authors 0270-6474/18/381601-07\$15.00/0},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0508-17.2018},
  abstract = {With ever-increasing advancements in technology, neuroscientists are able to collect data in greater volumes and with finer resolution. The bottleneck in understanding how the brain works is consequently shifting away from the amount and type of data we can collect and toward what we actually do with the data. There has been a growing interest in leveraging this vast volume of data across levels of analysis, measurement techniques, and experimental paradigms to gain more insight into brain function. Such efforts are visible at an international scale, with the emergence of big data neuroscience initiatives, such as the BRAIN initiative (Bargmann et al., 2014), the Human Brain Project, the Human Connectome Project, and the National Institute of Mental Health's Research Domain Criteria initiative. With these large-scale projects, much thought has been given to data-sharing across groups (Poldrack and Gorgolewski, 2014; Sejnowski et al., 2014); however, even with such data-sharing initiatives, funding mechanisms, and infrastructure, there still exists the challenge of how to cohesively integrate all the data. At multiple stages and levels of neuroscience investigation, machine learning holds great promise as an addition to the arsenal of analysis tools for discovering how the brain works.},
  language = {en},
  number = {7},
  journal = {Journal of Neuroscience},
  author = {Vu, Mai-Anh T. and Adal\i, T\"ulay and Ba, Demba and Buzs\'aki, Gy\"orgy and Carlson, David and Heller, Katherine and Liston, Conor and Rudin, Cynthia and Sohal, Vikaas S. and Widge, Alik S. and Mayberg, Helen S. and Sapiro, Guillermo and Dzirasa, Kafui},
  month = feb,
  year = {2018},
  keywords = {reinforcement learning,explainable artificial intelligence,machine learning},
  pages = {1601-1607},
  file = {articles/Vu2018.pdf},
  pmid = {29374138}
}

@article{Mallat1993,
  title = {Matching Pursuits with Time-Frequency Dictionaries},
  volume = {41},
  issn = {1053-587X},
  doi = {10.1109/78.258082},
  abstract = {The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992)},
  number = {12},
  journal = {IEEE Transactions on Signal Processing},
  author = {Mallat, S. G. and Zhang, Zhifeng},
  month = dec,
  year = {1993},
  keywords = {adaptive signal representations,adaptive time-frequency transform,Dictionaries,Fourier transforms,Gabor functions,Interference,linear waveform expansion,matching pursuit algorithm,Matching pursuit algorithms,matching pursuit decomposition,Natural languages,noisy signals,optimized wavepacket orthonormal basis,pattern extraction,Pursuit algorithms,signal energy distribution,signal expansion,signal processing,Signal processing algorithms,Signal representations,signal structures,Time frequency analysis,time-frequency analysis,time-frequency dictionaries,time-frequency plane,Vocabulary,wavelet transforms},
  pages = {3397-3415},
  file = {articles/Mallat1993.pdf}
}

@article{Heeger2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.06288},
  primaryClass = {cs, q-bio},
  title = {{{ORGaNICs}}: {{A Theory}} of {{Working Memory}} in {{Brains}} and {{Machines}}},
  shorttitle = {{{ORGaNICs}}},
  abstract = {Working memory is a cognitive process that is responsible for temporarily holding and manipulating information. Most of the empirical neuroscience research on working memory has focused on measuring sustained activity in prefrontal cortex (PFC) and/or parietal cortex during simple delayed-response tasks, and most of the models of working memory have been based on neural integrators. But working memory means much more than just holding a piece of information online. We describe a new theory of working memory, based on a recurrent neural circuit that we call ORGaNICs (Oscillatory Recurrent GAted Neural Integrator Circuits). ORGaNICs are a variety of Long Short Term Memory units (LSTMs), imported from machine learning and artificial intelligence. ORGaNICs can be used to explain the complex dynamics of delay-period activity in prefrontal cortex (PFC) during a working memory task. The theory is analytically tractable so that we can characterize the dynamics, and the theory provides a means for reading out information from the dynamically varying responses at any point in time, in spite of the complex dynamics. ORGaNICs can be implemented with a biophysical (electrical circuit) model of pyramidal cells, combined with shunting inhibition via a thalamocortical loop. Although introduced as a computational theory of working memory, ORGaNICs are also applicable to models of sensory processing, motor preparation and motor control. ORGaNICs offer computational advantages compared to other varieties of LSTMs that are commonly used in AI applications. Consequently, ORGaNICs are a framework for canonical computation in brains and machines.},
  journal = {arXiv:1803.06288 [cs, q-bio]},
  author = {Heeger, David J. and Mackey, Wayne E.},
  month = mar,
  year = {2018},
  keywords = {Quantitative Biology - Neurons and Cognition,Computer Science - Artificial Intelligence},
  file = {articles/Heeger2018.pdf}
}

@article{Himmelstein2018,
  title = {Research: {{Sci}}-{{Hub}} Provides Access to Nearly All Scholarly Literature},
  volume = {7},
  copyright = {\textcopyright{} 2018 Himmelstein et al.. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
  issn = {2050-084X},
  shorttitle = {Research},
  doi = {10.7554/eLife.32822},
  abstract = {The availability of almost all articles from toll access journals in the Sci-Hub repository will disrupt scholarly publishing towards more open models.},
  language = {en},
  journal = {eLife},
  author = {Himmelstein, Daniel S. and Romero, Ariel Rodriguez and Levernier, Jacob G. and Munro, Thomas Anthony and McLaughlin, Stephen Reid and Tzovaras, Bastian Greshake and Greene, Casey S.},
  month = feb,
  year = {2018},
  pages = {e32822},
  file = {articles/Himmelstein2018.pdf}
}

@article{Davies2018,
  title = {Loihi: {{A Neuromorphic Manycore Processor}} with {{On}}-{{Chip Learning}}},
  volume = {38},
  issn = {0272-1732},
  shorttitle = {Loihi},
  doi = {10.1109/MM.2018.112130359},
  abstract = {Loihi is a 60-mm2 chip fabricated in Intels 14-nm process that advances the state-of-the-art modeling of spiking neural networks in silicon. It integrates a wide range of novel features for the field, such as hierarchical connectivity, dendritic compartments, synaptic delays, and, most importantly, programmable synaptic learning rules. Running a spiking convolutional form of the Locally Competitive Algorithm, Loihi can solve LASSO optimization problems with over three orders of magnitude superior energy-delay-product compared to conventional solvers running on a CPU iso-process/voltage/area. This provides an unambiguous example of spike-based computation, outperforming all known conventional solutions.},
  number = {1},
  journal = {IEEE Micro},
  author = {Davies, M. and Srinivasa, N. and Lin, T. H. and Chinya, G. and Cao, Y. and Choday, S. H. and Dimou, G. and Joshi, P. and Imam, N. and Jain, S. and Liao, Y. and Lin, C. K. and Lines, A. and Liu, R. and Mathaikutty, D. and McCoy, S. and Paul, A. and Tse, J. and Venkataramanan, G. and Weng, Y. H. and Wild, A. and Yang, Y. and Wang, H.},
  month = jan,
  year = {2018},
  keywords = {Neurons,_tablet,Biological neural networks,artificial intelligence,Computer architecture,machine learning,Algorithm design and analysis,Computational modeling,neuromorphic computing,Neuromorphics},
  pages = {82-99},
  file = {articles/Davies2018.pdf}
}

@article{Diehl2015,
  title = {Unsupervised Learning of Digit Recognition Using Spike-Timing-Dependent Plasticity},
  volume = {9},
  issn = {1662-5188},
  doi = {10.3389/fncom.2015.00099},
  abstract = {In order to understand how the mammalian neocortex is performing computations, two things are necessary; we need to have a good understanding of the available neuronal processing units and mechanisms, and we need to gain a better understanding of how those mechanisms are combined to build functioning systems. Therefore, in recent years there is an increasing interest in how spiking neural networks (SNN) can be used to perform complex computations or solve pattern recognition tasks. However, it remains a challenging task to design SNNs which use biologically plausible mechanisms (especially for learning new patterns), since most of such SNN architectures rely on training in a rate-based network and subsequent conversion to a SNN. We present a SNN for digit recognition which is based on mechanisms with increased biological plausibility, i.e. conductance-based instead of current-based synapses, spike-timing-dependent plasticity with time-dependent weight change, lateral inhibition, and an adaptive spiking threshold. Unlike most other systems, we do not use a teaching signal and do not present any class labels to the network. Using this unsupervised learning scheme, our architecture achieves 95\% accuracy on the MNIST benchmark, which is better than previous SNN implementations without supervision. The fact that we used no domain-specific knowledge points toward the general applicability of our network design. Also, the performance of our network scales well with the number of neurons used and shows similar performance for four different learning rules, indicating robustness of the full combination of mechanisms, which suggests applicability in heterogeneous biological neural networks.},
  language = {English},
  journal = {Frontiers in Computational Neuroscience},
  author = {Diehl, Peter U. and Cook, Matthew},
  year = {2015},
  keywords = {STDP,unsupervised learning,Classification,Digit recognition,Spiking Neural network},
  file = {articles/Diehl2015.pdf}
}

@article{Forster2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.08448},
  primaryClass = {cs, stat},
  title = {Neural {{Simpletrons}} - {{Minimalistic Directed Generative Networks}} for {{Learning}} with {{Few Labels}}},
  abstract = {Classifiers for the semi-supervised setting often combine strong supervised models with additional learning objectives to make use of unlabeled data. This results in powerful though very complex models that are hard to train and that demand additional labels for optimal parameter tuning, which are often not given when labeled data is very sparse. We here study a minimalistic multi-layer generative neural network for semi-supervised learning in a form and setting as similar to standard discriminative networks as possible. Based on normalized Poisson mixtures, we derive compact and local learning and neural activation rules. Learning and inference in the network can be scaled using standard deep learning tools for parallelized GPU implementation. With the single objective of likelihood optimization, both labeled and unlabeled data are naturally incorporated into learning. Empirical evaluations on standard benchmarks show, that for datasets with few labels the derived minimalistic network improves on all classical deep learning approaches and is competitive with their recent variants without the need of additional labels for parameter tuning. Furthermore, we find that the studied network is the best performing monolithic (‘non-hybrid') system for few labels, and that it can be applied in the limit of very few labels, where no other system has been reported to operate so far.},
  journal = {arXiv:1506.08448 [cs, stat]},
  author = {Forster, Dennis and Sheikh, Abdul-Saboor and L\"ucke, J\"org},
  month = jun,
  year = {2015},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {articles/Forster2015.pdf}
}

@article{Bloss2018,
  title = {Single Excitatory Axons Form Clustered Synapses onto {{CA1}} Pyramidal Cell Dendrites},
  volume = {21},
  copyright = {2018 The Author(s)},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0084-6},
  abstract = {Bloss et al. show single axons form clustered inputs onto the dendrites of hippocampal pyramidal cells in a projection-specific manner. The spatial and temporal features inherent in these connections efficiently drive dendritic depolarization.},
  language = {en},
  number = {3},
  journal = {Nature Neuroscience},
  author = {Bloss, Erik B. and Cembrowski, Mark S. and Karsh, Bill and Colonell, Jennifer and Fetter, Richard D. and Spruston, Nelson},
  month = mar,
  year = {2018},
  pages = {353-363},
  file = {articles/Bloss2018.pdf}
}

@article{Tran2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.03585},
  primaryClass = {cs},
  title = {The {{Importance}} of {{Being Recurrent}} for {{Modeling Hierarchical Structure}}},
  abstract = {Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks such as language modeling (Linzen et al., 2016) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures---recurrent versus non-recurrent---with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose.},
  journal = {arXiv:1803.03585 [cs]},
  author = {Tran, Ke and Bisazza, Arianna and Monz, Christof},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Computation and Language},
  file = {articles/Tran2018.pdf}
}

@article{Cutts2014,
  title = {Detecting {{Pairwise Correlations}} in {{Spike Trains}}: {{An Objective Comparison}} of {{Methods}} and {{Application}} to the {{Study}} of {{Retinal Waves}}},
  volume = {34},
  copyright = {Copyright \textcopyright{} 2014 Cutts and Eglen. This article is freely available online through the J Neurosci Author Open Choice option.},
  issn = {0270-6474, 1529-2401},
  shorttitle = {Detecting {{Pairwise Correlations}} in {{Spike Trains}}},
  doi = {10.1523/JNEUROSCI.2767-14.2014},
  abstract = {Correlations in neuronal spike times are thought to be key to processing in many neural systems. Many measures have been proposed to summarize these correlations and of these the correlation index is widely used and is the standard in studies of spontaneous retinal activity. We show that this measure has two undesirable properties: it is unbounded above and confounded by firing rate. We list properties needed for a measure to fairly quantify and compare correlations and we propose a novel measure of correlation\textemdash{}the spike time tiling coefficient. This coefficient, the correlation index, and 33 other measures of correlation of spike times are blindly tested for the required properties on synthetic and experimental data. Based on this, we propose a measure (the spike time tiling coefficient) to replace the correlation index. To demonstrate the benefits of this measure, we reanalyze data from seven key studies, which previously used the correlation index to investigate the nature of spontaneous activity. We reanalyze data from $\beta$2(KO) and $\beta$2(TG) mutants, mutants lacking connexin isoforms, and also the age-dependent changes in wild-type and $\beta$2(KO) correlations. Reanalysis of the data using the proposed measure can significantly change the conclusions. It leads to better quantification of correlations and therefore better inference from the data. We hope that the proposed measure will have wide applications, and will help clarify the role of activity in retinotopic map formation.},
  language = {en},
  number = {43},
  journal = {Journal of Neuroscience},
  author = {Cutts, Catherine S. and Eglen, Stephen J.},
  month = oct,
  year = {2014},
  keywords = {correlations,activity,development,retina,retinotopic map,spike times},
  pages = {14288-14303},
  file = {articles/Cutts2014.pdf},
  pmid = {25339742}
}

@article{Hosoda2011,
  title = {Origin of Lognormal-like Distributions with a Common Width in a Growth and Division Process},
  volume = {83},
  doi = {10.1103/PhysRevE.83.031118},
  abstract = {Lognormal statistical distributions are observed in a variety of scientific fields. The widths of these distributions in the log scale are often similar, but the underlying mechanism that maintains these widths within a small range has not been well explained. We show that a stochastic process of halving followed by addition can yield a stationary distribution that resembles the universal lognormal distribution with a certain width. The mechanism that we propose here would provide insight into the essence of why lognormal-like distributions in many systems have a common width.},
  number = {3},
  journal = {Physical Review E},
  author = {Hosoda, Kazufumi and Matsuura, Tomoaki and Suzuki, Hiroaki and Yomo, Tetsuya},
  month = mar,
  year = {2011},
  pages = {031118},
  file = {articles/Hosoda2011.pdf}
}

@article{Scheler2017,
  title = {Logarithmic Distributions Prove That Intrinsic Learning Is {{Hebbian}}},
  volume = {6},
  issn = {2046-1402},
  doi = {10.12688/f1000research.12130.2},
  language = {en},
  journal = {F1000Research},
  author = {Scheler, Gabriele},
  month = oct,
  year = {2017},
  pages = {1222},
  file = {articles/Scheler2017.pdf}
}

@article{Ha2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.10122},
  primaryClass = {cs, stat},
  title = {World {{Models}}},
  doi = {10.5281/zenodo.1207631},
  abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at \$$\backslash$href\{https://worldmodels.github.io/\}\{$\backslash$mathtt\{https://worldmodels.github.io\}\}\$.},
  journal = {arXiv:1803.10122 [cs, stat]},
  author = {Ha, David and Schmidhuber, J\"urgen},
  month = mar,
  year = {2018},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {articles/Ha2018.pdf}
}

@article{Stern2018,
  title = {A Transformation from Temporal to Ensemble Coding in a Model of Piriform Cortex},
  volume = {7},
  copyright = {\textcopyright{} 2018 Stern et al.. This article is distributed under the terms of the Creative Commons Attribution License permitting unrestricted use and redistribution provided that the original author and source are credited.},
  issn = {2050-084X},
  doi = {10.7554/eLife.34831},
  language = {en},
  journal = {eLife},
  author = {Stern, Merav and Bolding, Kevin A. and Abbott, Larry F. and Franks, Kevin M.},
  month = mar,
  year = {2018},
  pages = {e34831},
  file = {articles/Stern2018.pdf}
}

@book{Ben-Israel2003,
  address = {New York},
  edition = {2nd ed},
  series = {CMS books in mathematics},
  title = {Generalized Inverses: Theory and Applications},
  isbn = {978-0-387-00293-4},
  lccn = {QA188 .B46 2003},
  shorttitle = {Generalized Inverses},
  number = {15},
  publisher = {{Springer}},
  author = {{Ben-Israel}, Adi and Greville, T. N. E.},
  year = {2003},
  keywords = {Matrix inversion},
  file = {books/Ben-Israel2003_Generalized-inverses-theory-and-applications.pdf}
}

@article{Penrose1955,
  title = {A Generalized Inverse for Matrices},
  volume = {51},
  issn = {1469-8064, 0305-0041},
  doi = {10.1017/S0305004100030401},
  abstract = {This paper describes a generalization of the inverse of a non-singular matrix, as the unique solution of a certain set of equations. This generalized inverse exists for any (possibly rectangular) matrix whatsoever with complex elements. It is used here for solving linear matrix equations, and among other applications for finding an expression for the principal idempotent elements of a matrix. Also a new type of spectral decomposition is given.},
  language = {en},
  number = {3},
  journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
  author = {Penrose, R.},
  month = jul,
  year = {1955},
  pages = {406-413},
  file = {articles/Penrose1955.pdf}
}

@inproceedings{Nallapu2016,
  title = {Dynamics of {{Reward Based Decision Making}}: {{A Computational Study}}},
  booktitle = {International {{Conference}} on {{Artificial Neural Networks}}},
  publisher = {{Springer}},
  author = {Nallapu, Bhargav Teja and Rougier, Nicolas P},
  year = {2016},
  pages = {322--329},
  file = {conferences/Nallapu2016_Dynamics-of-Reward-Based-Decision-Making-A-Computational-Study.pdf}
}

@article{Baker2016,
  title = {1,500 Scientists Lift the Lid on Reproducibility},
  volume = {533},
  doi = {10.1038/533452a},
  abstract = {Survey sheds light on the `crisis' rocking research.},
  language = {en},
  number = {7604},
  journal = {Nature News},
  author = {Baker, Monya},
  month = may,
  year = {2016},
  pages = {452},
  file = {articles/Baker2016.pdf}
}

@article{Gerlai2018,
  title = {Reproducibility and Replicability in Zebrafish Behavioral Neuroscience Research},
  issn = {0091-3057},
  doi = {10.1016/j.pbb.2018.02.005},
  abstract = {Reproducibility and replicability are fundamentally important aspects of the scientific method. From time to time the discussion about whether scientific findings are replicable enough flares up. In fact, some recent publications claim we are witnessing a replication crisis. This is a particularly important problem in laboratory organisms that are relatively new, i.e., for which only limited amount of information is available, and for which only a limited number of methods have been developed. The zebrafish is a relative newcomer in behavioral neuroscience. This review considers four distinct reasons as possibly underlying reproducibility issues in behavioral neuroscience studies using the zebrafish. One, publication bias for positive results. Two, statistical issues that surround the question of how to address type 1 and type 2 errors, and how to make statistical inference. Three, inappropriate control of factors that are known to potentially influence results. And four, methodological issues stemming from insufficient understanding of factors that may influence experimental results. The review will mainly focus on experimental issues and solutions, i.e. the latter two reasons listed above. It is not intended to be comprehensive, and its examples are drawn mainly from the author's own studies and experience with zebrafish. Nevertheless, most issues discussed are not unique to his laboratory, to the zebrafish, or even to behavioral neuroscience.},
  journal = {Pharmacology Biochemistry and Behavior},
  author = {Gerlai, Robert},
  month = feb,
  year = {2018},
  keywords = {Behavior,Brain function,Husbandry,Replicability,Reproducibility,Standardization,Zebrafish},
  file = {articles/Gerlai2018.pdf}
}

@article{Mulugeta2018,
  title = {Credibility, {{Replicability}}, and {{Reproducibility}} in {{Simulation}} for {{Biomedicine}} and {{Clinical Applications}} in {{Neuroscience}}},
  volume = {12},
  issn = {1662-5196},
  doi = {10.3389/fninf.2018.00018},
  abstract = {Modeling and simulation in computational neuroscience is currently a research enterprise to better understand neural systems. It is not yet directly applicable to the problems of patients with brain disease. To be used for clinical applications, there must not only be considerable progress in the field but also a concerted effort to use best practices in order to demonstrate model credibility to regulatory bodies, to clinics and hospitals, to doctors, and to patients. In doing this for neuroscience, we can learn lessons from long-standing practices in other areas of simulation (aircraft, computer chips), from software engineering, and from other biomedical disciplines. In this manuscript, we introduce some basic concepts that will be important in the development of credible clinical neuroscience models: reproducibility and replicability; verification and validation; model configuration; and procedures and processes for credible mechanistic multiscale modeling. We also discuss how garnering strong community involvement can promote model credibility. Finally, in addition to direct usage with patients, we note the potential for simulation usage in the area of Simulation-Based Medical Education, an area which to date has been primarily reliant on physical models (mannequins) and scenario-based simulations rather than on numerical simulations.},
  language = {English},
  journal = {Frontiers in Neuroinformatics},
  author = {Mulugeta, Lealem and Drach, Andrew and Erdemir, Ahmet and Hunt, C. Anthony and Horner, Marc and Ku, Joy P. and Myers, Jerry G. Jr and Vadigepalli, Rajanikanth and Lytton, William W.},
  year = {2018},
  keywords = {Neurology,computational model,computational neuroscience,Version control,clinical translation,computational modeling,context of interests,credibility,good practice,Intended use,mathematical modeling,mechanistic explanation,Mechanistic explanations,mechanistic modelling,mechanistic models,model sharing,model testing,model validation,Model verification,Modeling and simulations,multiscale modeling,Neurosurgery,Psychiatry,Reproducibility of Results,simulation models,Simulation-based medical education,uncertainty quantification,verification and validation}
}

@article{Gal2017,
  title = {Rich Cell-Type-Specific Network Topology in Neocortical Microcircuitry},
  volume = {20},
  copyright = {2017 Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/nn.4576},
  abstract = {Uncovering structural regularities and architectural topologies of cortical circuitry is vital for understanding neural computations. Recently, an experimentally constrained algorithm generated a dense network reconstruction of a $\sim$0.3-mm3 volume from juvenile rat somatosensory neocortex, comprising $\sim$31,000 cells and $\sim$36 million synapses. Using this reconstruction, we found a small-world topology with an average of 2.5 synapses separating any two cells and multiple cell-type-specific wiring features. Amounts of excitatory and inhibitory innervations varied across cells, yet pyramidal neurons maintained relatively constant excitation/inhibition ratios. The circuit contained highly connected hub neurons belonging to a small subset of cell types and forming an interconnected cell-type-specific rich club. Certain three-neuron motifs were overrepresented, matching recent experimental results. Cell-type-specific network properties were even more striking when synaptic strength and sign were considered in generating a functional topology. Our systematic approach enables interpretation of microconnectomics 'big data' and provides several experimentally testable predictions.},
  language = {en},
  number = {7},
  journal = {Nature Neuroscience},
  author = {Gal, Eyal and London, Michael and Globerson, Amir and Ramaswamy, Srikanth and Reimann, Michael W. and Muller, Eilif and Markram, Henry and Segev, Idan},
  month = jul,
  year = {2017},
  pages = {1004-1013},
  file = {articles/Gal22.pdf}
}

@article{Gilmore2017,
  title = {Progress toward Openness, Transparency, and Reproducibility in Cognitive Neuroscience},
  volume = {1396},
  issn = {0077-8923},
  doi = {10.1111/nyas.13325},
  abstract = {Accumulating evidence suggests that many findings in psychological science and cognitive neuroscience may prove difficult to reproduce; statistical power in brain imaging studies is low and has not improved recently; software errors in analysis tools are common and can go undetected for many years; and, a few large?scale studies notwithstanding, open sharing of data, code, and materials remain the rare exception. At the same time, there is a renewed focus on reproducibility, transparency, and openness as essential core values in cognitive neuroscience. The emergence and rapid growth of data archives, meta?analytic tools, software pipelines, and research groups devoted to improved methodology reflect this new sensibility. We review evidence that the field has begun to embrace new open research practices and illustrate how these can begin to address problems of reproducibility, statistical power, and transparency in ways that will ultimately accelerate discovery.},
  number = {1},
  journal = {Annals of the New York Academy of Sciences},
  author = {Gilmore, Rick O. and Diaz, Michele T. and Wyble, Brad A. and {Yarkoni Tal}},
  month = may,
  year = {2017},
  keywords = {open science,data sharing,reproducibility},
  pages = {5-18},
  file = {articles/Gilmore2017.pdf}
}

@article{Berkowitz2018,
  title = {Decoding Neural Responses with Minimal Information Loss},
  copyright = {\textcopyright{} 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  doi = {10.1101/273854},
  abstract = {Cortical tissue has a circuit motif termed the cortical column, which is thought to represent its basic computational unit but whose function remains unclear. Here we propose, and show quantitative evidence, that the cortical column performs computations necessary to decode neural activity with minimal information loss. The cortical decoder achieves higher accuracy compared to simpler decoders found in invertebrate and subcortical circuits by incorporating specific recurrent network dynamics. This recurrent dynamics also makes it possible to choose between alternative stimulus categories. The structure of cortical decoder predicts quadratic dependence of cortex size relative to subcortical parts of the brain. We quantitatively verify this relationship using anatomical data across mammals. The results offer a new perspective on the evolution and computational function of cortical columns.},
  language = {en},
  journal = {bioRxiv},
  author = {Berkowitz, John and Sharpee, Tatyana},
  month = feb,
  year = {2018},
  pages = {273854},
  file = {articles/Berkowitz22.pdf}
}

@article{Schwartz2006,
  title = {Spike-Triggered Neural Characterization},
  volume = {6},
  issn = {1534-7362},
  doi = {10.1167/6.4.13},
  language = {en},
  number = {4},
  journal = {Journal of Vision},
  author = {Schwartz, Odelia and Pillow, Jonathan W. and Rust, Nicole C. and Simoncelli, Eero P.},
  month = feb,
  year = {2006},
  pages = {13-13},
  file = {articles/Schwartz2006.pdf}
}

@article{Bittner2017,
  title = {Behavioral Time Scale Synaptic Plasticity Underlies {{CA1}} Place Fields},
  volume = {357},
  copyright = {Copyright \textcopyright{} 2017 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aan3846},
  abstract = {A different form of synaptic plasticity
How do synaptic or other neuronal changes support learning? This subject has been dominated by Hebb's postulate of synaptic change. Although there is strong experimental support for Hebbian plasticity in a number of preparations, alternative ideas have also been developed over the years. Bittner et al. provide in vivo, in vitro, and modeling data to support the view that non-Hebbian plasticity may underlie the formation of hippocampal place fields (see the Perspective by Krupic). Instead of multiple pairings, a single strong Ca2+ plateau potential in neuronal dendrites paired with spatial inputs may be sufficient to produce place cells.
Science, this issue p. 1033; see also p. 974
Learning is primarily mediated by activity-dependent modifications of synaptic strength within neuronal circuits. We discovered that place fields in hippocampal area CA1 are produced by a synaptic potentiation notably different from Hebbian plasticity. Place fields could be produced in vivo in a single trial by potentiation of input that arrived seconds before and after complex spiking. The potentiated synaptic input was not initially coincident with action potentials or depolarization. This rule, named behavioral time scale synaptic plasticity, abruptly modifies inputs that were neither causal nor close in time to postsynaptic activation. In slices, five pairings of subthreshold presynaptic activity and calcium (Ca2+) plateau potentials produced a large potentiation with an asymmetric seconds-long time course. This plasticity efficiently stores entire behavioral sequences within synaptic weights to produce predictive place cell activity.
A particular type of long\textendash{}time scale plasticity shapes the formation of stable place fields in the hippocampus.
A particular type of long\textendash{}time scale plasticity shapes the formation of stable place fields in the hippocampus.},
  language = {en},
  number = {6355},
  journal = {Science},
  author = {Bittner, Katie C. and Milstein, Aaron D. and Grienberger, Christine and Romani, Sandro and Magee, Jeffrey C.},
  month = sep,
  year = {2017},
  pages = {1033-1036},
  file = {articles/Bittner2017.pdf},
  pmid = {28883072}
}

@article{Garasto2018,
  title = {Visual Reconstruction from 2-Photon Calcium Imaging Suggests Linear Readout Properties of Neurons in Mouse Primary Visual Cortex},
  copyright = {\textcopyright{} 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  doi = {10.1101/300392},
  abstract = {Deciphering the neural code, that is interpreting the responses of sensory neurons from the perspective of a downstream population, is an important step towards understanding how the brain processes sensory stimulation. While previous work has focused on classification algorithms to identify the most likely stimulus label in a predefined set of categories, fewer studies have approached a full stimulus reconstruction task. Outstanding questions revolve around the type of algorithm that is most suited to decoding (i.e. full reconstruction, in the context of this study), especially in the presence of strong encoding non-linearities, and the possible role of pairwise correlations. We present, here, the first pixel-by-pixel reconstruction of a complex natural stimulus from 2-photon calcium imaging responses of mouse primary visual cortex (V1). We decoded the activity of approximately 100 neurons from layer 2/3 using an optimal linear estimator and an artificial neural network. We also investigated how much accuracy is lost in this decoding operation when ignoring pairwise neural correlations. We found that a simple linear estimator is sufficient to extract relevant stimulus features from the neural responses, and that it was not significantly outperformed by a non-linear decoding algorithm. The importance of pairwise correlations for reconstruction accuracy was also limited. The results of this study suggest that, conditional on the spatial and temporal limits of the recording technique, V1 neurons display linear readout properties, with low information content in the joint distribution of their activity.},
  language = {en},
  journal = {bioRxiv},
  author = {Garasto, Stef and Bharath, Anil A. and Schultz, Simon R.},
  month = apr,
  year = {2018},
  pages = {300392},
  file = {articles/Garasto2018.pdf}
}

@article{Ioffe2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.03167},
  primaryClass = {cs},
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  journal = {arXiv:1502.03167 [cs]},
  author = {Ioffe, Sergey and Szegedy, Christian},
  month = feb,
  year = {2015},
  keywords = {Computer Science - Learning},
  file = {articles/Ioffe2015.pdf}
}

@article{Szegedy2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1409.4842},
  primaryClass = {cs},
  title = {Going {{Deeper}} with {{Convolutions}}},
  abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  journal = {arXiv:1409.4842 [cs]},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  month = sep,
  year = {2014},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {articles/Szegedy2014.pdf}
}

@article{Zenke2018,
  title = {{{SuperSpike}}: {{Supervised Learning}} in {{Multilayer Spiking Neural Networks}}},
  issn = {0899-7667},
  shorttitle = {{{SuperSpike}}},
  doi = {10.1162/neco_a_01086},
  abstract = {A vast majority of computation in the brain is performed by spiking neural networks. Despite the ubiquity of such spiking, we currently lack an understanding of how biological spiking neural circuits learn and compute in vivo, as well as how we can instantiate such capabilities in artificial spiking circuits in silico. Here we revisit the problem of supervised learning in temporally coding multilayer spiking neural networks. First, by using a surrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based three-factor learning rule capable of training multilayer networks of deterministic integrate-and-fire neurons to perform nonlinear computations on spatiotemporal spike patterns. Second, inspired by recent results on feedback alignment, we compare the performance of our learning rule under different credit assignment strategies for propagating output errors to hidden units. Specifically, we test uniform, symmetric, and random feedback, finding that simpler tasks can be solved with any type of feedback, while more complex tasks require symmetric feedback. In summary, our results open the door to obtaining a better scientific understanding of learning and computation in spiking neural networks by advancing our ability to train them to solve nonlinear problems involving transformations between different spatiotemporal spike time patterns.},
  journal = {Neural Computation},
  author = {Zenke, Friedemann and Ganguli, Surya},
  month = apr,
  year = {2018},
  pages = {1-28},
  file = {articles/Zenke2018.pdf}
}

@article{Williams2018,
  title = {Higher-Order Thalamocortical Inputs Gate Synaptic Long-Term Potentiation via Disinhibiton},
  copyright = {\textcopyright{} 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  doi = {10.1101/281477},
  abstract = {Sensory experience and perceptual learning changes the receptive field properties of cortical pyramidal neurons, largely mediated by long-term potentiation (LTP) of synapses. The circuit mechanisms underlying cortical LTP remain unclear. In the mouse somatosensory cortex (S1), LTP can be elicited in layer (L) 2/3 pyramidal neurons by rhythmic whisker stimulation. We combined electrophysiology, optogenetics, and chemogenetics in thalamocortical slices to dissect the synaptic circuitry underlying this LTP. We found that projections from higher-order, posteriormedial thalamic complex (POm) to S1 are key to eliciting NMDAR-dependent LTP of intracortical synapses. Paired activation of intracortical and higher-order thalamocortical pathways increased vasoactive intestinal peptide (VIP) interneuron and decreased somatostatin (SST) interneuron activity, which was critical for inducing LTP. Our results reveal a novel circuit motif in which higher-order thalamic feedback gates plasticity of intracortical synapses in S1 via disinhibition. This motif may allow contextual feedback to shape synaptic circuits that process first-order sensory information.},
  language = {en},
  journal = {bioRxiv},
  author = {Williams, Leena E. and Holtmaat, Anthony},
  month = mar,
  year = {2018},
  pages = {281477},
  file = {articles/Williams2018.pdf}
}

@article{Ioannidis2005,
  title = {Why {{Most Published Research Findings Are False}}},
  volume = {2},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
  language = {en},
  number = {8},
  journal = {PLOS Medicine},
  author = {Ioannidis, John P. A.},
  month = aug,
  year = {2005},
  keywords = {Clinical research design,Finance,Genetic epidemiology,Genetics of disease,Meta-analysis,Randomized controlled trials,Research design,Schizophrenia},
  pages = {e124},
  file = {articles/Ioannidis2005.pdf}
}

@article{Hawkins2017,
  title = {A {{Theory}} of {{How Columns}} in the {{Neocortex Enable Learning}} the {{Structure}} of the {{World}}},
  volume = {11},
  issn = {1662-5110},
  doi = {10.3389/fncir.2017.00081},
  abstract = {Neocortical regions are organized into columns and layers. Connections between layers run mostly perpendicular to the surface suggesting a columnar functional organization. Some layers have long-range excitatory lateral connections suggesting interactions between columns. Similar patterns of connectivity exist in all regions but their exact role remain a mystery. In this paper, we propose a network model composed of columns and layers that performs robust object learning and recognition. Each column integrates its changing input over time to learn complete predictive models of observed objects. Excitatory lateral connections across columns allow the network to more rapidly infer objects based on the partial knowledge of adjacent columns. Because columns integrate input over time and space, the network learns models of complex objects that extend well beyond the receptive field of individual cells. Our network model introduces a new feature to cortical columns. We propose that a representation of location relative to the object being sensed is calculated within the sub-granular layers of each column. The location signal is provided as an input to the network, where it is combined with sensory data. Our model contains two layers and one or more columns. Simulations show that using Hebbian-like learning rules small single-column networks can learn to recognize hundreds of objects, with each object containing tens of features. Multi-column networks recognize objects with significantly fewer movements of the sensory receptors. Given the ubiquity of columnar and laminar connectivity patterns throughout the neocortex, we propose that columns and regions have more powerful recognition and modeling capabilities than previously assumed.},
  language = {English},
  journal = {Frontiers in Neural Circuits},
  author = {Hawkins, Jeff and Ahmad, Subutai and Cui, Yuwei},
  year = {2017},
  keywords = {cortical columns,cortical layers,Hierarchical temporal memory,Neocortex,sensorimotor learning},
  file = {articles/Hawkins2017.pdf}
}

@article{Curto2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.01905},
  primaryClass = {q-bio},
  title = {What Can Topology Tell Us about the Neural Code?},
  abstract = {Neuroscience is undergoing a period of rapid experimental progress and expansion. New mathematical tools, previously unknown in the neuroscience community, are now being used to tackle fundamental questions and analyze emerging data sets. Consistent with this trend, the last decade has seen an uptick in the use of topological ideas and methods in neuroscience. In this talk I will survey recent applications of topology in neuroscience, and explain why topology is an especially natural tool for understanding neural codes. Note: This is a write-up of my talk for the Current Events Bulletin, held at the 2016 Joint Math Meetings in Seattle, WA.},
  journal = {arXiv:1605.01905 [q-bio]},
  author = {Curto, Carina},
  month = may,
  year = {2016},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {articles/Curto2016.pdf}
}

@article{Hobbiss2018,
  title = {Homeostatic Plasticity Scales Dendritic Spine Volumes and Changes the Threshold and Specificity of {{Hebbian}} Plasticity},
  copyright = {\textcopyright{} 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  doi = {10.1101/308965},
  abstract = {Information is encoded within neural networks through synaptic weight changes. Synaptic learning rules involve a combination of rapid Hebbian plasticity with slower homeostatic synaptic plasticity (HSP) that regulates neuronal activity through global synaptic scaling. While Hebbian plasticity has been extensively investigated, much less is known about HSP. Here we investigate the structural and functional consequences of HSP at dendritic spines of mouse hippocampal neurons. We find that prolonged activity blockade induces spine growth, paralleling synaptic strength increases. Following activity blockade, glutamate uncaging-mediated long-term potentiation at single spines leads to size-dependent structural plasticity: smaller spines undergo robust growth, while larger spines remain unchanged. Moreover, we find that neighboring spines in the vicinity of the stimulated spine exhibit volume changes following HSP, indicating that plasticity has spread across a group of synapses. Overall, these findings demonstrate that Hebbian and homeostatic plasticity shape neural connectivity through coordinated structural plasticity of clustered inputs.},
  language = {en},
  journal = {bioRxiv},
  author = {Hobbiss, Anna F. and Cortes, Yazmin Ramiro and Israely, Inbal},
  month = apr,
  year = {2018},
  pages = {308965},
  file = {articles/Hobbiss2018.pdf}
}

@article{Kim2004,
  title = {{{PDZ}} Domain Proteins of Synapses},
  volume = {5},
  copyright = {2004 Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/nrn1517},
  abstract = {PDZ domains are modular protein-interaction domains that are specialized for binding to C-terminal peptide motifs of other proteins. PDZ-domain-based scaffolds typically assemble large molecular complexes at specific subcellular sites, such as synapses. The postsynaptic density (PSD) of brain excitatory synapses contains many PDZ proteins, of which PSD-95 is the best characterized. PSD-95 contains multiple domains that mediate its association with various membrane proteins (receptors, ion channels and cell-adhesion molecules) and cytoplasmic signalling molecules. PSD-95 influences the activity of interacting membrane proteins by controlling their surface delivery, stability, subcellular location, subunit composition and even intrinsic functional properties. By linking them with cytoplasmic signal transduction proteins, PSD-95 facilitates the functional coupling of postsynaptic receptors and ion channels to downstream signalling pathways involving nitric oxide, Ras, Rap and Rac. Molecular genetic manipulations of PSD-95 in culture and in vivo have strong effects on synaptic transmission and plasticity. The abundance and activity of PSD-95 at synapses is dynamically controlled by activity, palmitoylation (lipid modification), degradation and phosphorylation. The four members of the PSD-95 family of proteins show distinct patterns of palmitoylation, subcellular localization, temporal expression and binding partners, which presumably underlie their differential functions in synapses and neurons. Genetic studies have revealed the in vivo functions of PSD-95 family scaffolds. In addition to learning and memory, PSD-95 family proteins are now implicated in the organization of cholinergic synapses, visual processing, pain perception and behavioural responses to drugs of abuse. GRIP/ABP and PICK1 are PDZ proteins that bind directly to AMPA receptors and regulate their trafficking. With seven PDZ domains, GRIP interacts with numerous proteins, some of which (such as Eph receptors) might also regulate synaptic function. PICK1 also binds to a variety of membrane and cytoplasmic proteins, including kainate receptors. PDZ scaffolds function in motor trafficking of their associated protein complexes. Several PDZ proteins bind directly to motor proteins of the kinesin and myosin families. In this way, PDZ scaffolds on the surface of vesicles can act as 'receptors', linking motors with their specific cargoes. PDZ-based interactions are post-translationally regulated to allow controlled assembly or disassembly of synaptic protein complexes. For instance, AMPA-receptor interactions with GRIP are disrupted by phosphorylation of the C-terminal peptide of GluR2, which is correlated with internalization of AMPA receptors and synaptic depression. With increasing knowledge of the structure and in vivo functions of synaptic PDZ proteins, PDZ interactions could become plausible targets for pharmaceutical intervention, thereby opening up a wealth of possibilities for the treatment of brain diseases.},
  language = {en},
  number = {10},
  journal = {Nature Reviews Neuroscience},
  author = {Kim, Eunjoon and Sheng, Morgan},
  month = oct,
  year = {2004},
  pages = {771-781},
  file = {articles/Kim2004.pdf}
}

@article{Bellec2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.09574},
  primaryClass = {cs, q-bio},
  title = {Long Short-Term Memory and {{Learning}}-to-Learn in Networks of Spiking Neurons},
  abstract = {Networks of spiking neurons (SNNs) are frequently studied as models for networks of neurons in the brain, but also as paradigm for novel energy efficient computing hardware. In principle they are especially suitable for computations in the temporal domain, such as speech processing, because their computations are carried out via events in time and space. But so far they have been lacking the capability to preserve information for longer time spans during a computation, until it is updated or needed - like a register of a digital computer. This function is provided to artificial neural networks through Long Short-Term Memory (LSTM) units. We show here that SNNs attain similar capabilities if one includes adapting neurons in the network. Adaptation denotes an increase of the firing threshold of a neuron after preceding firing. A substantial fraction of neurons in the neocortex of rodents and humans has been found to be adapting. It turns out that if adapting neurons are integrated in a suitable manner into the architecture of SNNs, the performance of these enhanced SNNs, which we call LSNNs, for computation in the temporal domain approaches that of artificial neural networks with LSTM-units. In addition, the computing and learning capabilities of LSNNs can be substantially enhanced through learning-to-learn (L2L) methods from machine learning, that have so far been applied primarily to LSTM networks and apparently never to SSNs. This preliminary report on arXiv will be replaced by a more detailed version in about a month.},
  journal = {arXiv:1803.09574 [cs, q-bio]},
  author = {Bellec, Guillaume and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
  month = mar,
  year = {2018},
  keywords = {Quantitative Biology - Neurons and Cognition,Computer Science - Neural and Evolutionary Computing},
  file = {articles/Bellec2018.pdf}
}

@article{Gamanut2018,
  title = {The {{Mouse Cortical Connectome}}, {{Characterized}} by an {{Ultra}}-{{Dense Cortical Graph}}, {{Maintains Specificity}} by {{Distinct Connectivity Profiles}}},
  volume = {97},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2017.12.037},
  abstract = {Summary
The inter-areal wiring pattern of the mouse cerebral cortex was analyzed in relation to a refined parcellation of cortical areas. Twenty-seven retrograde tracer injections were made in 19 areas of a 47-area parcellation of the mouse neocortex. Flat mounts of the cortex and multiple histological markers enabled detailed counts of labeled neurons in individual areas. The observed log-normal distribution of connection weights to each cortical area spans 5 orders of magnitude and reveals a distinct connectivity profile for each area, analogous to that observed in macaques. The cortical network has a density of 97\%, considerably higher than the 66\% density reported in macaques. A weighted graph analysis reveals a similar global efficiency but weaker spatial clustering compared with that reported in macaques. The consistency, precision of the connectivity profile, density, and weighted graph analysis of the present data differ significantly from those obtained in earlier studies in the mouse.},
  number = {3},
  journal = {Neuron},
  author = {G{\u a}m{\u a}nu{\c t}, R{\u a}zvan and Kennedy, Henry and Toroczkai, Zolt\'an and {Ercsey-Ravasz}, M\'aria and Van Essen, David C. and Knoblauch, Kenneth and Burkhalter, Andreas},
  month = feb,
  year = {2018},
  keywords = {connectivity,neocortex,anatomy,log-normal,retrograde,rodent,tract-tracing},
  pages = {698-715.e10},
  file = {articles/Gămănuţ2018.pdf}
}

@article{Mejias2016,
  title = {Feedforward and Feedback Frequency-Dependent Interactions in a Large-Scale Laminar Network of the Primate Cortex},
  volume = {2},
  copyright = {Copyright \textcopyright{} 2016, The Authors. This is an open-access article distributed under the terms of the Creative Commons Attribution-NonCommercial license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
  issn = {2375-2548},
  doi = {10.1126/sciadv.1601335},
  abstract = {Interactions between top-down and bottom-up processes in the cerebral cortex hold the key to understanding attentional processes, predictive coding, executive control, and a gamut of other brain functions. However, the underlying circuit mechanism remains poorly understood and represents a major challenge in neuroscience. We approached this problem using a large-scale computational model of the primate cortex constrained by new directed and weighted connectivity data. In our model, the interplay between feedforward and feedback signaling depends on the cortical laminar structure and involves complex dynamics across multiple (intralaminar, interlaminar, interareal, and whole cortex) scales. The model was tested by reproducing, as well as providing insights into, a wide range of neurophysiological findings about frequency-dependent interactions between visual cortical areas, including the observation that feedforward pathways are associated with enhanced gamma (30 to 70 Hz) oscillations, whereas feedback projections selectively modulate alpha/low-beta (8 to 15 Hz) oscillations. Furthermore, the model reproduces a functional hierarchy based on frequency-dependent Granger causality analysis of interareal signaling, as reported in recent monkey and human experiments, and suggests a mechanism for the observed context-dependent hierarchy dynamics. Together, this work highlights the necessity of multiscale approaches and provides a modeling platform for studies of large-scale brain circuit dynamics and functions.
A large-scale laminar network model sheds light on frequency-dependent interactions in the primate cortex.
A large-scale laminar network model sheds light on frequency-dependent interactions in the primate cortex.},
  language = {en},
  number = {11},
  journal = {Science Advances},
  author = {Mejias, Jorge F. and Murray, John D. and Kennedy, Henry and Wang, Xiao-Jing},
  month = nov,
  year = {2016},
  pages = {e1601335},
  file = {articles/Mejias2016.pdf}
}

@article{Phillips2010,
  title = {Categorial {{Compositionality}}: {{A Category Theory Explanation}} for the {{Systematicity}} of {{Human Cognition}}},
  volume = {6},
  issn = {1553-7358},
  shorttitle = {Categorial {{Compositionality}}},
  doi = {10.1371/journal.pcbi.1000858},
  abstract = {Classical and Connectionist theories of cognitive architecture seek to explain systematicity (i.e., the property of human cognition whereby cognitive capacity comes in groups of related behaviours) as a consequence of syntactically and functionally compositional representations, respectively. However, both theories depend on ad hoc assumptions to exclude specific instances of these forms of compositionality (e.g. grammars, networks) that do not account for systematicity. By analogy with the Ptolemaic (i.e. geocentric) theory of planetary motion, although either theory can be made to be consistent with the data, both nonetheless fail to fully explain it. Category theory, a branch of mathematics, provides an alternative explanation based on the formal concept of adjunction, which relates a pair of structure-preserving maps, called functors. A functor generalizes the notion of a map between representational states to include a map between state transformations (or processes). In a formal sense, systematicity is a necessary consequence of a higher-order theory of cognitive architecture, in contrast to the first-order theories derived from Classicism or Connectionism. Category theory offers a re-conceptualization for cognitive science, analogous to the one that Copernicus provided for astronomy, where representational states are no longer the center of the cognitive universe\textemdash{}replaced by the relationships between the maps that transform them.},
  language = {en},
  number = {7},
  journal = {PLOS Computational Biology},
  author = {Phillips, Steven and Wilson, William H.},
  month = jul,
  year = {2010},
  keywords = {Learning,Behavior,Cognition,Cognitive science,Grammar,Human mobility,Syntax,Vector spaces},
  pages = {e1000858},
  file = {articles/Phillips2010.pdf}
}

@article{Kording2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.00262},
  primaryClass = {q-bio},
  title = {The Geometry of {{Tempotronlike}} Problems},
  abstract = {In the discrete Tempotron learning problem a neuron receives time varying inputs and for a set of such input sequences (\$$\backslash$mathcal S\_-\$ set) the neuron must be sub-threshold for all times while for some other sequences (\$$\backslash$mathcal S\_+\$ set) the neuron must be super threshold for at least one time. Here we present a graphical treatment of a slight reformulation of the tempotron problem. We show that the problem's general form is equivalent to the question if a polytope, specified by a set of inequalities, is contained in the union of a set of equally defined polytopes. Using recent results from computational geometry, we show that the problem is W[1]-hard. This phrasing gives some new insights into the nature of gradient based learning algorithms. A sampling based approach can, under certain circumstances provide an approximation in polynomial time. Other problems, related to hierarchical neural networks may share some topological structure.},
  journal = {arXiv:1511.00262 [q-bio]},
  author = {Kording, Konrad Paul},
  month = nov,
  year = {2015},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {articles/Kording2015.pdf}
}

@article{Ebner2015,
  title = {Open and Closed Cortico-Subcortical Loops: {{A}} Neuro-Computational Account of Access to Consciousness in the Distractor-Induced Blindness Paradigm},
  volume = {35},
  issn = {1053-8100},
  shorttitle = {Open and Closed Cortico-Subcortical Loops},
  doi = {10.1016/j.concog.2015.02.007},
  abstract = {How the brain decides which information to process `consciously' has been debated over for decades without a simple explanation at hand. While most experiments manipulate the perceptual energy of presented stimuli, the distractor-induced blindness task is a prototypical paradigm to investigate gating of information into consciousness without or with only minor visual manipulation. In this paradigm, subjects are asked to report intervals of coherent dot motion in a rapid serial visual presentation (RSVP) stream, whenever these are preceded by a particular color stimulus in a different RSVP stream. If distractors (i.e., intervals of coherent dot motion prior to the color stimulus) are shown, subjects' abilities to perceive and report intervals of target dot motion decrease, particularly with short delays between intervals of target color and target motion. We propose a biologically plausible neuro-computational model of how the brain controls access to consciousness to explain how distractor-induced blindness originates from information processing in the cortex and basal ganglia. The model suggests that conscious perception requires reverberation of activity in cortico-subcortical loops and that basal-ganglia pathways can either allow or inhibit this reverberation. In the distractor-induced blindness paradigm, inadequate distractor-induced response tendencies are suppressed by the inhibitory `hyperdirect' pathway of the basal ganglia. If a target follows such a distractor closely, temporal aftereffects of distractor suppression prevent target identification. The model reproduces experimental data on how delays between target color and target motion affect the probability of target detection.},
  journal = {Consciousness and Cognition},
  author = {Ebner, Christian and Schroll, Henning and Winther, Gesche and Niedeggen, Michael and Hamker, Fred H.},
  month = sep,
  year = {2015},
  keywords = {Basal ganglia,Cognitive control,Computational model,Conscious access,Global workspace theory},
  pages = {295-307},
  file = {articles/Ebner2015.pdf}
}

@article{Buckley2017,
  title = {The Free Energy Principle for Action and Perception: {{A}} Mathematical Review},
  volume = {81},
  issn = {0022-2496},
  shorttitle = {The Free Energy Principle for Action and Perception},
  doi = {10.1016/j.jmp.2017.09.004},
  abstract = {The `free energy principle' (FEP) has been suggested to provide a unified theory of the brain, integrating data and theory relating to action, perception, and learning. The theory and implementation of the FEP combines insights from Helmholtzian `perception as inference', machine learning theory, and statistical thermodynamics. Here, we provide a detailed mathematical evaluation of a suggested biologically plausible implementation of the FEP that has been widely used to develop the theory. Our objectives are (i) to describe within a single article the mathematical structure of this implementation of the FEP; (ii) provide a simple but complete agent-based model utilising the FEP and (iii) to disclose the assumption structure of this implementation of the FEP to help elucidate its significance for the brain sciences.},
  journal = {Journal of Mathematical Psychology},
  author = {Buckley, Christopher L. and Kim, Chang Sub and McGregor, Simon and Seth, Anil K.},
  month = dec,
  year = {2017},
  keywords = {Inference,Perception,Action,Agent-based model,Bayesian brain,Free energy principle},
  pages = {55-79},
  file = {articles/Buckley2017.pdf}
}

@misc{Bzdok2018,
  type = {News},
  title = {Points of {{Significance}}: {{Statistics}} versus Machine Learning},
  copyright = {2018 Nature Publishing Group},
  shorttitle = {Points of {{Significance}}},
  abstract = {Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns.},
  language = {en},
  howpublished = {https://www.nature.com/articles/nmeth.4642},
  journal = {Nature Methods},
  author = {Bzdok, Danilo and Altman, Naomi and Krzywinski, Martin},
  month = apr,
  year = {2018},
  doi = {10.1038/nmeth.4642}
}

@inproceedings{Jimenez2015,
  title = {The {{Role}} of {{Container Technology}} in {{Reproducible Computer Systems Research}}},
  doi = {10.1109/IC2E.2015.75},
  abstract = {Evaluating experimental results in the field of computer systems is a challenging task, mainly due to the many changes in software and hardware that computational environments go through. In this position paper, we analyze salient features of container technology that, if leveraged correctly, can help reduce the complexity of reproducing experiments in systems research. We present a use case in the area of distributed storage systems to illustrate the extensions that we envision, mainly in terms of container management infrastructure. We also discuss the benefits and limitations of using containers as a way of reproducing research in other areas of experimental systems research.},
  booktitle = {2015 {{IEEE International Conference}} on {{Cloud Engineering}}},
  author = {Jimenez, I. and Maltzahn, C. and Moody, A. and Mohror, K. and Lofstead, J. and {Arpaci-Dusseau}, R. and {Arpaci-Dusseau}, A.},
  month = mar,
  year = {2015},
  keywords = {Virtualization,container management infrastructure,container technology,Containers,distributed storage systems,experimental system research,Hardware,Kernel,Linux,Measurement,reproducible computer system research,Scalability,storage allocation},
  pages = {379-385},
  file = {conferences/Jimenez2015_The-Role-of-Container-Technology-in-Reproducible-Computer-Systems-Research.pdf}
}

@article{Holtmaat2009,
  title = {Experience-Dependent Structural Synaptic Plasticity in the Mammalian Brain},
  volume = {10},
  copyright = {2009 Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/nrn2699},
  abstract = {Synaptic plasticity in adult neural circuits may involve the strengthening or weakening of existing synapses as well as structural plasticity, including synapse formation and elimination. Indeed, long-term in vivo imaging studies are beginning to reveal the structural dynamics of neocortical neurons in the normal and injured adult brain. Although the overall cell-specific morphology of axons and dendrites, as well as of a subpopulation of small synaptic structures, are remarkably stable, there is increasing evidence that experience-dependent plasticity of specific circuits in the somatosensory and visual cortex involves cell type-specific structural plasticity: some boutons and dendritic spines appear and disappear, accompanied by synapse formation and elimination, respectively. This Review focuses on recent evidence for such structural forms of synaptic plasticity in the mammalian cortex and outlines open questions.},
  language = {en},
  number = {9},
  journal = {Nature Reviews Neuroscience},
  author = {Holtmaat, Anthony and Svoboda, Karel},
  month = sep,
  year = {2009},
  pages = {647-658},
  file = {articles/Holtmaat2009.pdf}
}

@article{Watts1998,
  title = {Collective Dynamics of `Small-World' Networks},
  volume = {393},
  copyright = {1998 Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/30918},
  abstract = {Networks of coupled dynamical systems have been used to model biological oscillators1,2,3,4, Josephson junction arrays5,6, excitable media7, neural networks8,9,10, spatial games11, genetic control networks12 and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks `rewired' to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them `small-world' networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices.},
  language = {en},
  number = {6684},
  journal = {Nature},
  author = {Watts, Duncan J. and Strogatz, Steven H.},
  month = jun,
  year = {1998},
  pages = {440-442},
  file = {articles/Watts1998.pdf}
}

@inproceedings{Jimenez2017,
  title = {The {{Popper Convention}}: {{Making Reproducible Systems Evaluation Practical}}},
  shorttitle = {The {{Popper Convention}}},
  doi = {10.1109/IPDPSW.2017.157},
  abstract = {Independent validation of experimental results in the field of systems research is a challenging task, mainly due to differences in software and hardware in computational environments. Recreating an environment that resembles the original is difficult and time-consuming. In this paper we introduce \_Popper\_, a convention based on a set of modern open source software (OSS) development principles for generating reproducible scientific publications. Concretely, we make the case for treating an article as an OSS project following a DevOps approach and applying software engineering best-practices to manage its associated artifacts and maintain the reproducibility of its findings. Popper leverages existing cloud-computing infrastructure and DevOps tools to produce academic articles that are easy to validate and extend. We present a use case that illustrates the usefulness of this approach. We show how, by following the \_Popper\_ convention, reviewers and researchers can quickly get to the point of getting results without relying on the original author's intervention.},
  booktitle = {2017 {{IEEE International Parallel}} and {{Distributed Processing Symposium Workshops}} ({{IPDPSW}})},
  author = {Jimenez, I. and Sevilla, M. and Watkins, N. and Maltzahn, C. and Lofstead, J. and Mohror, K. and {Arpaci-Dusseau}, A. and {Arpaci-Dusseau}, R.},
  month = may,
  year = {2017},
  keywords = {Virtualization,reproducibility,Automation,cloud computing,cloud-computing infrastructure,computational environments,DevOps approach,DevOps tools,Guidelines,natural sciences computing,open source software development principles,OSS project,Packaging,Popper convention,public domain software,repeatability,replicability,reproducible scientific publications,reproducible systems evaluation,Software,software engineering,state of the practice,systems research,Tools,Virtual machining},
  pages = {1561-1570},
  file = {conferences/Jimenez2017_The-Popper-Convention-Making-Reproducible-Systems-Evaluation-Practical.pdf}
}

@article{Oh2014,
  title = {A Mesoscale Connectome of the Mouse Brain},
  volume = {508},
  copyright = {2014 Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature13186},
  abstract = {Comprehensive knowledge of the brain's wiring diagram is fundamental for understanding how the nervous system processes information at both local and global scales. However, with the singular exception of the C. elegans microscale connectome, there are no complete connectivity data sets in other species. Here we report a brain-wide, cellular-level, mesoscale connectome for the mouse. The Allen Mouse Brain Connectivity Atlas uses enhanced green fluorescent protein (EGFP)-expressing adeno-associated viral vectors to trace axonal projections from defined regions and cell types, and high-throughput serial two-photon tomography to image the EGFP-labelled axons throughout the brain. This systematic and standardized approach allows spatial registration of individual experiments into a common three dimensional (3D) reference space, resulting in a whole-brain connectivity matrix. A computational model yields insights into connectional strength distribution, symmetry and other network properties. Virtual tractography illustrates 3D topography among interconnected regions. Cortico-thalamic pathway analysis demonstrates segregation and integration of parallel pathways. The Allen Mouse Brain Connectivity Atlas is a freely available, foundational resource for structural and functional investigations into the neural circuits that support behavioural and cognitive processes in health and disease.},
  language = {en},
  number = {7495},
  journal = {Nature},
  author = {Oh, Seung Wook and Harris, Julie A. and Ng, Lydia and Winslow, Brent and Cain, Nicholas and Mihalas, Stefan and Wang, Quanxin and Lau, Chris and Kuan, Leonard and Henry, Alex M. and Mortrud, Marty T. and Ouellette, Benjamin and Nguyen, Thuc Nghi and Sorensen, Staci A. and Slaughterbeck, Clifford R. and Wakeman, Wayne and Li, Yang and Feng, David and Ho, Anh and Nicholas, Eric and Hirokawa, Karla E. and Bohn, Phillip and Joines, Kevin M. and Peng, Hanchuan and Hawrylycz, Michael J. and Phillips, John W. and Hohmann, John G. and Wohnoutka, Paul and Gerfen, Charles R. and Koch, Christof and Bernard, Amy and Dang, Chinh and Jones, Allan R. and Zeng, Hongkui},
  month = apr,
  year = {2014},
  pages = {207-214},
  file = {articles/Oh2014.pdf}
}

@article{Bullmore2009,
  title = {Complex Brain Networks: Graph Theoretical Analysis of Structural and Functional Systems},
  volume = {10},
  copyright = {2009 Nature Publishing Group},
  issn = {1471-0048},
  shorttitle = {Complex Brain Networks},
  doi = {10.1038/nrn2575},
  abstract = {Recent developments in the quantitative analysis of complex networks, based largely on graph theory, have been rapidly translated to studies of brain network organization. The brain's structural and functional systems have features of complex networks \textemdash{} such as small-world topology, highly connected hubs and modularity \textemdash{} both at the whole-brain scale of human neuroimaging and at a cellular scale in non-human animals. In this article, we review studies investigating complex brain networks in diverse experimental modalities (including structural and functional MRI, diffusion tensor imaging, magnetoencephalography and electroencephalography in humans) and provide an accessible introduction to the basic principles of graph theory. We also highlight some of the technical challenges and key questions to be addressed by future developments in this rapidly moving field.},
  language = {en},
  number = {3},
  journal = {Nature Reviews Neuroscience},
  author = {Bullmore, Ed and Sporns, Olaf},
  month = mar,
  year = {2009},
  pages = {186-198},
  file = {articles/Bullmore2009.pdf}
}

@article{Ypma2016,
  title = {Statistical {{Analysis}} of {{Tract}}-{{Tracing Experiments Demonstrates}} a {{Dense}}, {{Complex Cortical Network}} in the {{Mouse}}},
  volume = {12},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005104},
  abstract = {Anatomical tract tracing methods are the gold standard for estimating the weight of axonal connectivity between a pair of pre-defined brain regions. Large studies, comprising hundreds of experiments, have become feasible by automated methods. However, this comes at the cost of positive-mean noise making it difficult to detect weak connections, which are of particular interest as recent high resolution tract-tracing studies of the macaque have identified many more weak connections, adding up to greater connection density of cortical networks, than previously recognized. We propose a statistical framework that estimates connectivity weights and credibility intervals from multiple tract-tracing experiments. We model the observed signal as a log-normal distribution generated by a combination of tracer fluorescence and positive-mean noise, also accounting for injections into multiple regions. Using anterograde viral tract-tracing data provided by the Allen Institute for Brain Sciences, we estimate the connection density of the mouse intra-hemispheric cortical network to be 73\% (95\% credibility interval (CI): 71\%, 75\%); higher than previous estimates (40\%). Inter-hemispheric density was estimated to be 59\% (95\% CI: 54\%, 62\%). The weakest estimable connections (about 6 orders of magnitude weaker than the strongest connections) are likely to represent only one or a few axons. These extremely weak connections are topologically more random and longer distance than the strongest connections, which are topologically more clustered and shorter distance (spatially clustered). Weak links do not substantially contribute to the global topology of a weighted brain graph, but incrementally increased topological integration of a binary graph. The topology of weak anatomical connections in the mouse brain, rigorously estimable down to the biological limit of a single axon between cortical areas in these data, suggests that they might confer functional advantages for integrative information processing and/or they might represent a stochastic factor in the development of the mouse connectome.},
  language = {en},
  number = {9},
  journal = {PLOS Computational Biology},
  author = {Ypma, Rolf J. F. and Bullmore, Edward T.},
  year = {12-Sep-2016},
  keywords = {Algorithms,Network analysis,Neurons,Neural networks,Axons,Connectomics,Macaque,Mice},
  pages = {e1005104},
  file = {articles/Ypma2016.pdf}
}

@article{Markov2014,
  title = {A {{Weighted}} and {{Directed Interareal Connectivity Matrix}} for {{Macaque Cerebral Cortex}}},
  volume = {24},
  issn = {1047-3211},
  doi = {10.1093/cercor/bhs270},
  abstract = {Abstract.  Retrograde tracer injections in 29 of the 91 areas of the macaque cerebral cortex revealed 1,615 interareal pathways, a third of which have not previ},
  language = {en},
  number = {1},
  journal = {Cerebral Cortex},
  author = {Markov, N. T. and {Ercsey-Ravasz}, M. M. and Gomes, Ribeiro and R, A. and Lamy, C. and Magrou, L. and Vezoli, J. and Misery, P. and Falchier, A. and Quilodran, R. and Gariel, M. A. and Sallet, J. and Gamanut, R. and Huissoud, C. and Clavagnier, S. and Giroud, P. and {Sappey-Marinier}, D. and Barone, P. and Dehay, C. and Toroczkai, Z. and Knoblauch, K. and Essen, Van and C, D. and Kennedy, H.},
  month = jan,
  year = {2014},
  pages = {17-36},
  file = {articles/Markov2014.pdf}
}

@article{Martens2017,
  title = {Anti-Correlations in the Degree Distribution Increase Stimulus Detection Performance in Noisy Spiking Neural Networks},
  volume = {42},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-016-0629-1},
  abstract = {Neuronal circuits in the rodent barrel cortex are characterized by stable low firing rates. However, recent experiments show that short spike trains elicited by electrical stimulation in single neurons can induce behavioral responses. Hence, the underlying neural networks provide stability against internal fluctuations in the firing rate, while simultaneously making the circuits sensitive to small external perturbations. Here we studied whether stability and sensitivity are affected by the connectivity structure in recurrently connected spiking networks. We found that anti-correlation between the number of afferent (in-degree) and efferent (out-degree) synaptic connections of neurons increases stability against pathological bursting, relative to networks where the degrees were either positively correlated or uncorrelated. In the stable network state, stimulation of a few cells could lead to a detectable change in the firing rate. To quantify the ability of networks to detect the stimulation, we used a receiver operating characteristic (ROC) analysis. For a given level of background noise, networks with anti-correlated degrees displayed the lowest false positive rates, and consequently had the highest stimulus detection performance. We propose that anti-correlation in the degree distribution may be a computational strategy employed by sensory cortices to increase the detectability of external stimuli. We show that networks with anti-correlated degrees can in principle be formed by applying learning rules comprised of a combination of spike-timing dependent plasticity, homeostatic plasticity and pruning to networks with uncorrelated degrees. To test our prediction we suggest a novel experimental method to estimate correlations in the degree distribution.},
  language = {en},
  number = {1},
  journal = {Journal of Computational Neuroscience},
  author = {Martens, Marijn B. and Houweling, Arthur R. and Tiesinga, Paul H. E.},
  month = feb,
  year = {2017},
  pages = {87-106},
  file = {articles/Martens2017.pdf}
}

@article{Dehaene2017,
  title = {What Is Consciousness, and Could Machines Have It?},
  volume = {358},
  copyright = {Copyright \textcopyright{} 2017, American Association for the Advancement of Science. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aan8871},
  abstract = {The controversial question of whether machines may ever be conscious must be based on a careful consideration of how consciousness arises in the only physical system that undoubtedly possesses it: the human brain. We suggest that the word ``consciousness'' conflates two different types of information-processing computations in the brain: the selection of information for global broadcasting, thus making it flexibly available for computation and report (C1, consciousness in the first sense), and the self-monitoring of those computations, leading to a subjective sense of certainty or error (C2, consciousness in the second sense). We argue that despite their recent successes, current machines are still mostly implementing computations that reflect unconscious processing (C0) in the human brain. We review the psychological and neural science of unconscious (C0) and conscious computations (C1 and C2) and outline how they may inspire novel machine architectures.},
  language = {en},
  number = {6362},
  journal = {Science},
  author = {Dehaene, Stanislas and Lau, Hakwan and Kouider, Sid},
  month = oct,
  year = {2017},
  pages = {486-492},
  file = {articles/Dehaene2017.pdf},
  pmid = {29074769}
}

@article{Mijalkov2017,
  title = {{{BRAPH}}: {{A}} Graph Theory Software for the Analysis of Brain Connectivity},
  volume = {12},
  issn = {1932-6203},
  shorttitle = {{{BRAPH}}},
  doi = {10.1371/journal.pone.0178798},
  abstract = {The brain is a large-scale complex network whose workings rely on the interaction between its various regions. In the past few years, the organization of the human brain network has been studied extensively using concepts from graph theory, where the brain is represented as a set of nodes connected by edges. This representation of the brain as a connectome can be used to assess important measures that reflect its topological architecture. We have developed a freeware MatLab-based software (BRAPH\textendash{}BRain Analysis using graPH theory) for connectivity analysis of brain networks derived from structural magnetic resonance imaging (MRI), functional MRI (fMRI), positron emission tomography (PET) and electroencephalogram (EEG) data. BRAPH allows building connectivity matrices, calculating global and local network measures, performing non-parametric permutations for group comparisons, assessing the modules in the network, and comparing the results to random networks. By contrast to other toolboxes, it allows performing longitudinal comparisons of the same patients across different points in time. Furthermore, even though a user-friendly interface is provided, the architecture of the program is modular (object-oriented) so that it can be easily expanded and customized. To demonstrate the abilities of BRAPH, we performed structural and functional graph theory analyses in two separate studies. In the first study, using MRI data, we assessed the differences in global and nodal network topology in healthy controls, patients with amnestic mild cognitive impairment, and patients with Alzheimer's disease. In the second study, using resting-state fMRI data, we compared healthy controls and Parkinson's patients with mild cognitive impairment.},
  language = {en},
  number = {8},
  journal = {PLOS ONE},
  author = {Mijalkov, Mite and Kakaei, Ehsan and Pereira, Joana B. and Westman, Eric and Volpe, Giovanni and Initiative, for the Alzheimer's Disease Neuroimaging},
  year = {01-Aug-2017},
  keywords = {Network analysis,Neural networks,Alzheimer's disease,Cognitive impairment,Functional magnetic resonance imaging,Graph theory,Graphical user interface,Neuroimaging},
  pages = {e0178798},
  file = {articles/Mijalkov2017.pdf}
}

@article{Krakauer2017,
  title = {Neuroscience {{Needs Behavior}}: {{Correcting}} a {{Reductionist Bias}}},
  volume = {93},
  issn = {1097-4199},
  shorttitle = {Neuroscience {{Needs Behavior}}},
  doi = {10.1016/j.neuron.2016.12.041},
  abstract = {There are ever more compelling tools available for neuroscience research, ranging from selective genetic targeting to optogenetic circuit control to mapping whole connectomes. These approaches are coupled with a deep-seated, often tacit, belief in the reductionist program for understanding the link between the brain and behavior. The aim of this program is causal explanation through neural manipulations that allow testing of necessity and sufficiency claims. We argue, however, that another equally important approach seeks an alternative form of understanding through careful theoretical and experimental decomposition of behavior. Specifically, the detailed analysis of tasks and of the behavior they elicit is best suited for discovering component processes and their underlying algorithms. In most cases, we argue that study of the neural implementation of behavior is best investigated after such behavioral work. Thus, we advocate a more pluralistic notion of neuroscience when it comes to the brain-behavior relationship: behavioral work provides understanding, whereas neural interventions test causality.},
  language = {eng},
  number = {3},
  journal = {Neuron},
  author = {Krakauer, John W. and Ghazanfar, Asif A. and {Gomez-Marin}, Alex and MacIver, Malcolm A. and Poeppel, David},
  month = feb,
  year = {2017},
  keywords = {Algorithms,Animals,Brain,Neurosciences,Behavior,Bias,Humans,Task Performance and Analysis},
  pages = {480-490},
  file = {articles/Krakauer2017.pdf},
  pmid = {28182904}
}

@article{Manninen2018,
  title = {Challenges in {{Reproducibility}}, {{Replicability}}, and {{Comparability}} of {{Computational Models}} and {{Tools}} for {{Neuronal}} and {{Glial Networks}}, {{Cells}}, and {{Subcellular Structures}}},
  volume = {12},
  issn = {1662-5196},
  doi = {10.3389/fninf.2018.00020},
  abstract = {The possibility to replicate and reproduce published research results is one of the biggest challenges in all areas of science. In computational neuroscience, there are thousands of models available. However, it is rarely possible to reimplement the models based on the information in the original publication, let alone rerun the models just because the model implementations have not been made publicly available. We evaluate and discuss the comparability of a versatile choice of simulation tools: tools for biochemical reactions and spiking neuronal networks, and relatively new tools for growth in cell cultures. The replicability and reproducibility issues are considered for computational models that are equally diverse, including the models for intracellular signal transduction of neurons and glial cells, in addition to single glial cells, neuron-glia interactions, and selected examples of spiking neuronal networks. We also address the comparability of the simulation results with one another to comprehend if the studied models can be used to answer similar research questions. In addition to presenting the challenges in reproducibility and replicability of published results in computational neuroscience, we highlight the need for developing recommendations and good practices for publishing simulation tools and computational models. Model validation and flexible model description must be an integral part of the tool used to simulate and develop computational models. Constant improvement on experimental techniques and recording protocols leads to increasing knowledge about the biophysical mechanisms in neural systems. This poses new challenges for computational neuroscience: extended or completely new computational methods and models may be required. Careful evaluation and categorization of the existing models and tools provide a foundation for these future needs, for constructing multiscale models or extending the models to incorporate additional or more detailed biophysical mechanisms. Improving the quality of publications in computational neuroscience, enabling progressive building of advanced computational models and tools, can be achieved only through adopting publishing standards which underline replicability and reproducibility of research results.},
  language = {English},
  journal = {Frontiers in Neuroinformatics},
  author = {Manninen, Tiina and A\'cimovi\'c, Jugoslava and Havela, Riikka and Teppola, Heidi and Linne, Marja-Leena},
  year = {2018},
  keywords = {computational model,reproducibility,replicability,astrocyte,Glial cell,Neuron,neuronal network,Subcellular structure},
  file = {articles/Manninen2018.pdf}
}

@article{Bridi2018,
  title = {Two Distinct Mechanisms for Experience-Dependent Homeostasis},
  copyright = {2018 The Author(s)},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0150-0},
  abstract = {The authors revise the classical view that homeostasis of neuronal activity is achieved by negative firing rate feedback: during sensory deprivation, homeostasis occurs via the sliding threshold, which acts via firing patterns rather than rates.},
  language = {en},
  journal = {Nature Neuroscience},
  author = {Bridi, Michelle C. D. and de Pasquale, Roberto and Lantz, Crystal L. and Gu, Yu and Borrell, Andrew and Choi, Se-Young and He, Kaiwen and Tran, Trinh and Hong, Su Z. and Dykman, Andrew and Lee, Hey-Kyoung and Quinlan, Elizabeth M. and Kirkwood, Alfredo},
  month = may,
  year = {2018},
  pages = {1},
  file = {articles/Bridi2018.pdf}
}

@article{Vegue2017,
  title = {On the {{Structure}} of {{Cortical Microcircuits Inferred}} from {{Small Sample Sizes}}},
  volume = {37},
  copyright = {Copyright \textcopyright{} 2017 the authors 0270-6474/17/378498-13\$15.00/0},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0984-17.2017},
  abstract = {The structure in cortical microcircuits deviates from what would be expected in a purely random network, which has been seen as evidence of clustering. To address this issue, we sought to reproduce the nonrandom features of cortical circuits by considering several distinct classes of network topology, including clustered networks, networks with distance-dependent connectivity, and those with broad degree distributions. To our surprise, we found that all of these qualitatively distinct topologies could account equally well for all reported nonrandom features despite being easily distinguishable from one another at the network level. This apparent paradox was a consequence of estimating network properties given only small sample sizes. In other words, networks that differ markedly in their global structure can look quite similar locally. This makes inferring network structure from small sample sizes, a necessity given the technical difficulty inherent in simultaneous intracellular recordings, problematic. We found that a network statistic called the sample degree correlation (SDC) overcomes this difficulty. The SDC depends only on parameters that can be estimated reliably given small sample sizes and is an accurate fingerprint of every topological family. We applied the SDC criterion to data from rat visual and somatosensory cortex and discovered that the connectivity was not consistent with any of these main topological classes. However, we were able to fit the experimental data with a more general network class, of which all previous topologies were special cases. The resulting network topology could be interpreted as a combination of physical spatial dependence and nonspatial, hierarchical clustering.
SIGNIFICANCE STATEMENT The connectivity of cortical microcircuits exhibits features that are inconsistent with a simple random network. Here, we show that several classes of network models can account for this nonrandom structure despite qualitative differences in their global properties. This apparent paradox is a consequence of the small numbers of simultaneously recorded neurons in experiment: when inferred via small sample sizes, many networks may be indistinguishable despite being globally distinct. We develop a connectivity measure that successfully classifies networks even when estimated locally with a few neurons at a time. We show that data from rat cortex is consistent with a network in which the likelihood of a connection between neurons depends on spatial distance and on nonspatial, asymmetric clustering.},
  language = {en},
  number = {35},
  journal = {Journal of Neuroscience},
  author = {Vegu\'e, Marina and Perin, Rodrigo and Roxin, Alex},
  month = aug,
  year = {2017},
  keywords = {microcircuits,clustering,cortical connectivity,multiple patch-clamp,networks,statistics},
  pages = {8498-8510},
  file = {articles/Vegué2017.pdf},
  pmid = {28760860}
}

@article{Khan2018,
  title = {Distinct Learning-Induced Changes in Stimulus Selectivity and Interactions of {{GABAergic}} Interneuron Classes in Visual Cortex},
  volume = {21},
  copyright = {2018 The Author(s)},
  issn = {1546-1726},
  doi = {10.1038/s41593-018-0143-z},
  abstract = {Khan et al. simultaneously measured activity from excitatory cells and three classes of inhibitory interneurons in visual cortex and show that learning differentially shapes the stimulus selectivity and interactions of multiple cell classes.},
  language = {en},
  number = {6},
  journal = {Nature Neuroscience},
  author = {Khan, Adil G. and Poort, Jasper and Chadwick, Angus and Blot, Antonin and Sahani, Maneesh and {Mrsic-Flogel}, Thomas D. and Hofer, Sonja B.},
  month = jun,
  year = {2018},
  pages = {851-859},
  file = {articles/Khan2018.pdf}
}

@article{Pfister2006,
  title = {Triplets of {{Spikes}} in a {{Model}} of {{Spike Timing}}-{{Dependent Plasticity}}},
  volume = {26},
  copyright = {Copyright \textcopyright{} 2006 Society for Neuroscience 0270-6474/06/269673-10\$15.00/0},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1425-06.2006},
  abstract = {Classical experiments on spike timing-dependent plasticity (STDP) use a protocol based on pairs of presynaptic and postsynaptic spikes repeated at a given frequency to induce synaptic potentiation or depression. Therefore, standard STDP models have expressed the weight change as a function of pairs of presynaptic and postsynaptic spike. Unfortunately, those paired-based STDP models cannot account for the dependence on the repetition frequency of the pairs of spike. Moreover, those STDP models cannot reproduce recent triplet and quadruplet experiments. Here, we examine a triplet rule (i.e., a rule which considers sets of three spikes, i.e., two pre and one post or one pre and two post) and compare it to classical pair-based STDP learning rules. With such a triplet rule, it is possible to fit experimental data from visual cortical slices as well as from hippocampal cultures. Moreover, when assuming stochastic spike trains, the triplet learning rule can be mapped to a Bienenstock\textendash{}Cooper\textendash{}Munro learning rule.},
  language = {en},
  number = {38},
  journal = {Journal of Neuroscience},
  author = {Pfister, Jean-Pascal and Gerstner, Wulfram},
  month = sep,
  year = {2006},
  keywords = {Hebbian learning,STDP,computational neuroscience,long-term potentiation,modeling,spike triplet},
  pages = {9673-9682},
  file = {articles/Pfister2006.pdf},
  pmid = {16988038}
}

@article{Gallinaro2018,
  title = {Associative Properties of Structural Plasticity Based on Firing Rate Homeostasis in Recurrent Neuronal Networks},
  volume = {8},
  copyright = {2018 The Author(s)},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-22077-3},
  abstract = {Correlation-based Hebbian plasticity is thought to shape neuronal connectivity during development and learning, whereas homeostatic plasticity would stabilize network activity. Here we investigate another, new aspect of this dichotomy: Can Hebbian associative properties also emerge as a network effect from a plasticity rule based on homeostatic principles on the neuronal level? To address this question, we simulated a recurrent network of leaky integrate-and-fire neurons, in which excitatory connections are subject to a structural plasticity rule based on firing rate homeostasis. We show that a subgroup of neurons develop stronger within-group connectivity as a consequence of receiving stronger external stimulation. In an experimentally well-documented scenario we show that feature specific connectivity, similar to what has been observed in rodent visual cortex, can emerge from such a plasticity rule. The experience-dependent structural changes triggered by stimulation are long-lasting and decay only slowly when the neurons are exposed again to unspecific external inputs.},
  language = {en},
  number = {1},
  journal = {Scientific Reports},
  author = {Gallinaro, J\'ulia V. and Rotter, Stefan},
  month = feb,
  year = {2018},
  pages = {3754},
  file = {articles/Gallinaro2018.pdf}
}

@article{Guzman2016,
  title = {Synaptic Mechanisms of Pattern Completion in the Hippocampal {{CA3}} Network},
  volume = {353},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaf1836},
  language = {en},
  number = {6304},
  journal = {Science},
  author = {Guzman, S. J. and Schlogl, A. and Frotscher, M. and Jonas, P.},
  month = sep,
  year = {2016},
  pages = {1117-1123},
  file = {articles/Guzman2016.pdf}
}

@article{Liu2011,
  title = {Controllability of Complex Networks},
  volume = {473},
  copyright = {2011 Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature10011},
  abstract = {The ultimate proof of our understanding of natural or technological systems is reflected in our ability to control them. Although control theory offers mathematical tools for steering engineered and natural systems towards a desired state, a framework to control complex self-organized systems is lacking. Here we develop analytical tools to study the controllability of an arbitrary complex directed network, identifying the set of driver nodes with time-dependent control that can guide the system's entire dynamics. We apply these tools to several real networks, finding that the number of driver nodes is determined mainly by the network's degree distribution. We show that sparse inhomogeneous networks, which emerge in many real complex systems, are the most difficult to control, but that dense and homogeneous networks can be controlled using a few driver nodes. Counterintuitively, we find that in both model and real systems the driver nodes tend to avoid the high-degree nodes.},
  language = {en},
  number = {7346},
  journal = {Nature},
  author = {Liu, Yang-Yu and Slotine, Jean-Jacques and Barab\'asi, Albert-L\'aszl\'o},
  month = may,
  year = {2011},
  pages = {167-173},
  file = {articles/Liu2011.pdf}
}

@article{Yoshihara2018,
  title = {`{{Necessary}} and Sufficient' in Biology Is Not Necessarily Necessary \textendash{} Confusions and Erroneous Conclusions Resulting from Misapplied Logic in the Field of Biology, Especially Neuroscience},
  volume = {0},
  issn = {0167-7063},
  doi = {10.1080/01677063.2018.1468443},
  abstract = {In this article, we describe an incorrect use of logic which involves the careless application of the `necessary and sufficient' condition originally used in formal logic. This logical fallacy is causing frequent confusion in current biology, especially in neuroscience. In order to clarify this problem, we first dissect the structure of this incorrect logic (which we refer to as `misapplied-N\&S') to show how necessity and sufficiency in misapplied-N\&S are not matching each other. Potential pitfalls of utilizing misapplied-N\&S are exemplified by cases such as the discrediting of command neurons and other potentially key neurons, the distorting of truth in optogenetic studies, and the wrongful justification of studies with little meaning. In particular, the use of the word `sufficient' in optogenetics tends to generate misunderstandings by opening up multiple interpretations. To avoid the confusion caused by the misleading logic, we now recommend using `indispensable and inducing' instead of using `necessary and sufficient.' However, we ultimately recommend fully articulating the limits of what our experiments suggest, not relying on such simple phrases. Only after this problem is fully understood and more rigorous language is demanded, can we finally interpret experimental results in an accurate way.},
  number = {0},
  journal = {Journal of Neurogenetics},
  author = {Yoshihara, Motojiro and Yoshihara, Motoyuki},
  month = may,
  year = {2018},
  keywords = {optogenetics,command neurons,Drosophila,Feeding neuron,indispensable and inducing,Logic,memory,necessary and sufficient},
  pages = {1-12},
  file = {articles/Yoshihara2018.pdf},
  pmid = {29757057}
}

@article{Teramae2014,
  title = {Computational {{Implications}} of {{Lognormally Distributed Synaptic Weights}}},
  volume = {102},
  issn = {0018-9219},
  doi = {10.1109/JPROC.2014.2306254},
  abstract = {The connectivity structure of neural networks has significant implications for neural information processing, and much experimental effort has been made to clarify the structure of neural networks in the brain, i.e., both graph structure and weight structure of synaptic connections. A traditional view of neural information processing suggests that neurons compute in a highly parallel and distributed manner, in which the cooperation of many weak synaptic inputs is necessary to activate a single neuron. Recent experiments, however, have shown that not all synapses are weak in cortical circuits, but some synapses are extremely strong (several tens of times larger than the average weight). In fact, the weights of excitatory synapses between cortical excitatory neurons often obey a lognormal distribution with a long tail of strong synapses. Here, we review some of our important and recent works on computation with sparsely distributed synaptic weights and discuss the possible implications of this synaptic principle for neural computation by spiking neurons. We demonstrate that internal noise emerges from long-tailed distributions of synaptic weights to produce stochastic resonance effect in the reverberating synaptic pathways constituted by strong synapses. We show a spike-timing-dependent plasticity rule and other mechanisms that produce such weight distributions. A possible hardware realization of lognormally connected networks is also shown.},
  number = {4},
  journal = {Proceedings of the IEEE},
  author = {n Teramae, J. and Fukai, T.},
  month = apr,
  year = {2014},
  keywords = {Memory,Neurons,Biological neural networks,Principal component analysis,neural nets,stochastic processes,recurrent networks,Associative memory (AM),bioelectric potentials,brain,computational implications,connectivity structure,cortical circuits,cortical excitatory neurons,distributed computation,excitatory synapse weights,feedforward networks,graph structure,hardware realization,Integrated circuit modeling,internal noise,log normal distribution,lognormal distribution,long-tailed distributions,medical computing,network connectivity,neural computation,neural dynamics,neural information processing,neural network structure,neuromorphic engineering,neurophysiology,Noise measurement,parallel computation,principal component analysis (PCA),reverberating synaptic pathways,single neuron,sparse coding,sparsely distributed synaptic weights,spike sequence,spike-timing-dependent plasticity (STDP),spike-timing-dependent plasticity rule,spiking neurons,Stability analysis,stochastic resonance,Stochastic resonance,stochastic resonance effect,Stochastic systems,strong synapses,synaptic connection,synaptic principle,weak synaptic inputs,weight distribution,weight structure},
  pages = {500-512},
  file = {articles/Teramae2014.pdf}
}

@article{Sporns2016,
  title = {Modular {{Brain Networks}}},
  volume = {67},
  issn = {0066-4308},
  doi = {10.1146/annurev-psych-122414-033634},
  abstract = {The development of new technologies for mapping structural and functional brain connectivity has led to the creation of comprehensive network maps of neuronal circuits and systems. The architecture of these brain networks can be examined and analyzed with a large variety of graph theory tools. Methods for detecting modules, or network communities, are of particular interest because they uncover major building blocks or subnetworks that are particularly densely connected, often corresponding to specialized functional components. A large number of methods for community detection have become available and are now widely applied in network neuroscience. This article first surveys a number of these methods, with an emphasis on their advantages and shortcomings; then it summarizes major findings on the existence of modules in both structural and functional brain networks and briefly considers their potential functional roles in brain evolution, wiring minimization, and the emergence of functional specialization and complex dynamics.},
  number = {1},
  journal = {Annual Review of Psychology},
  author = {Sporns, Olaf and Betzel, Richard F.},
  month = jan,
  year = {2016},
  pages = {613-640},
  file = {articles/Sporns2016.pdf}
}

@article{Newman2004,
  title = {Finding and Evaluating Community Structure in Networks},
  volume = {69},
  doi = {10.1103/PhysRevE.69.026113},
  abstract = {We propose and study a set of algorithms for discovering community structure in networks\textemdash{}natural divisions of network nodes into densely connected subgroups. Our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using any one of a number of possible ``betweenness'' measures, and second, these measures are, crucially, recalculated after each removal. We also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. We demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems.},
  number = {2},
  journal = {Physical Review E},
  author = {Newman, M. E. J. and Girvan, M.},
  month = feb,
  year = {2004},
  pages = {026113},
  file = {articles/Newman2004.pdf}
}

@article{Newman2006,
  title = {Modularity and Community Structure in Networks},
  volume = {103},
  copyright = {\textcopyright{} 2006 by The National Academy of Sciences of the USA},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0601602103},
  abstract = {Many networks of interest in the sciences, including social networks, computer networks, and metabolic and regulatory networks, are found to divide naturally into communities or modules. The problem of detecting and characterizing this community structure is one of the outstanding issues in the study of networked systems. One highly effective approach is the optimization of the quality function known as ``modularity'' over the possible divisions of a network. Here I show that the modularity can be expressed in terms of the eigenvectors of a characteristic matrix for the network, which I call the modularity matrix, and that this expression leads to a spectral algorithm for community detection that returns results of demonstrably higher quality than competing methods in shorter running times. I illustrate the method with applications to several published network data sets.},
  language = {en},
  number = {23},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Newman, M. E. J.},
  month = jun,
  year = {2006},
  keywords = {clustering,metabolic network,modules,partitioning,social network},
  pages = {8577-8582},
  file = {articles/Newman2006.pdf},
  pmid = {16723398}
}

@article{du2008normalized,
  title = {A Normalized and a Hybrid Modularity},
  journal = {Draft paper, eclectic. ss. uci. edu/\~ drwhite/links2pdf. htm},
  author = {Du, Haifeng and White, Douglas R and Ren, Yike and Li, Shuzhuo},
  year = {2008},
  file = {articles/Du2008.pdf}
}

@article{Jarrell2012,
  title = {The {{Connectome}} of a {{Decision}}-{{Making Neural Network}}},
  volume = {337},
  copyright = {Copyright \textcopyright{} 2012, American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1221762},
  abstract = {In order to understand the nervous system, it is necessary to know the synaptic connections between the neurons, yet to date, only the wiring diagram of the adult hermaphrodite of the nematode Caenorhabditis elegans has been determined. Here, we present the wiring diagram of the posterior nervous system of the C. elegans adult male, reconstructed from serial electron micrograph sections. This region of the male nervous system contains the sexually dimorphic circuits for mating. The synaptic connections, both chemical and gap junctional, form a neural network with four striking features: multiple, parallel, short synaptic pathways directly connecting sensory neurons to end organs; recurrent and reciprocal connectivity among sensory neurons; modular substructure; and interneurons acting in feedforward loops. These features help to explain how the network robustly and rapidly selects and executes the steps of a behavioral program on the basis of the inputs from multiple sensory neurons.
The complete wiring structure of the synaptic network governing mating behavior of male nematodes is revealed.
The complete wiring structure of the synaptic network governing mating behavior of male nematodes is revealed.},
  language = {en},
  number = {6093},
  journal = {Science},
  author = {Jarrell, Travis A. and Wang, Yi and Bloniarz, Adam E. and Brittin, Christopher A. and Xu, Meng and Thomson, J. Nichol and Albertson, Donna G. and Hall, David H. and Emmons, Scott W.},
  month = jul,
  year = {2012},
  pages = {437-444},
  file = {articles/Jarrell2012.pdf},
  pmid = {22837521}
}

@article{Harriger2012,
  title = {Rich {{Club Organization}} of {{Macaque Cerebral Cortex}} and {{Its Role}} in {{Network Communication}}},
  volume = {7},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0046497},
  abstract = {Graph-theoretical analysis of brain connectivity data has revealed significant features of brain network organization across a range of species. Consistently, large-scale anatomical networks exhibit highly nonrandom attributes including an efficient small world modular architecture, with distinct network communities that are interlinked by hub regions. The functional importance of hubs motivates a closer examination of their mutual interconnections, specifically to examine the hypothesis that hub regions are more densely linked than expected based on their degree alone, i.e. forming a central rich club. Extending recent findings of rich club topology in the cat and human brain, this report presents evidence for the existence of rich club organization in the cerebral cortex of a non-human primate, the macaque monkey, based on a connectivity data set representing a collation of numerous tract tracing studies. Rich club regions comprise portions of prefrontal, parietal, temporal and insular cortex and are widely distributed across network communities. An analysis of network motifs reveals that rich club regions tend to form star-like configurations, indicative of their central embedding within sets of nodes. In addition, rich club nodes and edges participate in a large number of short paths across the network, and thus contribute disproportionately to global communication. As rich club regions tend to attract and disperse communication paths, many of the paths follow a characteristic pattern of first increasing and then decreasing node degree. Finally, the existence of non-reciprocal projections imposes a net directional flow of paths into and out of the rich club, with some regions preferentially attracting and others dispersing signals. Overall, the demonstration of rich club organization in a non-human primate contributes to our understanding of the network principles underlying neural connectivity in the mammalian brain, and further supports the hypothesis that rich club regions and connections have a central role in global brain communication.},
  language = {en},
  number = {9},
  journal = {PLOS ONE},
  author = {Harriger, Logan and van den Heuvel, Martijn P. and Sporns, Olaf},
  year = {28-Sep-2012},
  keywords = {Brain,Mathematical models,Network analysis,Network motifs,Neural networks,Macaque,Centrality,Sequence motif analysis},
  pages = {e46497},
  file = {articles/Harriger2012.pdf}
}

@article{Hilgetag2000,
  title = {Anatomical Connectivity Defines the Organization of Clusters of Cortical Areas in the Macaque and the Cat},
  volume = {355},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2000.0551},
  abstract = {The number of different cortical structures in mammalian brains and the number of extrinsic fibres linking these regions are both large. As with any complex system, systematic analysis is required to draw reliable conclusions about the organization of the complex neural networks comprising these numerous elements. One aspect of organization that has long been suspected is that cortical networks are organized into `streams' or `systems'. Here we report computational analyses capable of showing whether clusters of strongly interconnected areas are aspects of the global organization of cortical systems in macaque and cat. We used two different approaches to analyse compilations of corticocortical connection data from the macaque and the cat. The first approach, optimal set analysis, employed an explicit definition of a neural `system' or `stream', which was based on differential connectivity. We defined a two\textendash{}component cost function that described the cost of the global cluster arrangement of areas in terms of the areas` connectivity within and between candidate clusters. Optimal cluster arrangements of cortical areas were then selected computationally from the very many possible arrangements, using an evolutionary optimization algorithm. The second approach, non\textendash{}parametric cluster analysis (NPCA), grouped cortical areas on the basis of their proximity in multidimensional scaling representations. We used non\textendash{}metric multidimensional scaling to represent the cortical connectivity structures metrically in two and five dimensions. NPCA then analysed these representations to determine the nature of the clusters for a wide range of different cluster shape parameters. The results from both approaches largely agreed. They showed that macaque and cat cortices are organized into densely intra\textendash{}connected clusters of areas, and identified the constituent members of the clusters. These clusters reflected functionally specialized sets of cortical areas, suggesting that structure and function are closely linked at this gross, systems level.},
  language = {en},
  number = {1393},
  journal = {Philosophical Transactions of the Royal Society of London B: Biological Sciences},
  author = {Hilgetag, Claus-C. and Burns, Gully A. P. C. and O'Neill, Marc A. and Scannell, Jack W. and Young, Malcolm P.},
  month = jan,
  year = {2000},
  pages = {91-110},
  pmid = {10703046}
}

@article{Baltieri2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.02649},
  primaryClass = {q-bio},
  title = {The Modularity of Action and Perception Revisited Using Control Theory and Active Inference},
  abstract = {The assumption that action and perception can be investigated independently is entrenched in theories, models and experimental approaches across the brain and mind sciences. In cognitive science, this has been a central point of contention between computationalist and 4Es (enactive, embodied, extended and embedded) theories of cognition, with the former embracing the "classical sandwich", modular, architecture of the mind and the latter actively denying this separation can be made. In this work we suggest that the modular independence of action and perception strongly resonates with the separation principle of control theory and furthermore that this principle provides formal criteria within which to evaluate the implications of the modularity of action and perception. We will also see that real-time feedback with the environment, often considered necessary for the definition of 4Es ideas, is not however a sufficient condition to avoid the "classical sandwich". Finally, we argue that an emerging framework in the cognitive and brain sciences, active inference, extends ideas derived from control theory to the study of biological systems while disposing of the separation principle, describing non-modular models of behaviour strongly aligned with 4Es theories of cognition.},
  journal = {arXiv:1806.02649 [q-bio]},
  author = {Baltieri, Manuel and Buckley, Christopher L.},
  month = jun,
  year = {2018},
  keywords = {Quantitative Biology - Neurons and Cognition},
  file = {articles/Baltieri2018.pdf}
}

@article{Rieubland2014,
  title = {Structured {{Connectivity}} in {{Cerebellar Inhibitory Networks}}},
  volume = {81},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2013.12.029},
  abstract = {Summary
Defining the rules governing synaptic connectivity is key to formulating theories of neural circuit function. Interneurons can be connected by both electrical and chemical synapses, but the organization and interaction of these two complementary microcircuits is unknown. By recording from multiple molecular layer interneurons in the cerebellar cortex, we reveal specific, nonrandom connectivity patterns in both GABAergic chemical and electrical interneuron networks. Both networks contain clustered motifs and show specific overlap between them. Chemical connections exhibit a preference for transitive patterns, such as feedforward triplet motifs. This structured connectivity is supported by a characteristic spatial organization: transitivity of chemical connectivity is directed vertically in the sagittal plane, and electrical synapses appear strictly confined to the sagittal plane. The specific, highly structured connectivity rules suggest that these motifs are essential for the function of the cerebellar network.},
  number = {4},
  journal = {Neuron},
  author = {Rieubland, Sarah and Roth, Arnd and H\"ausser, Michael},
  month = feb,
  year = {2014},
  pages = {913-929},
  file = {articles/Rieubland2014.pdf}
}

@article{Brette2018,
  title = {The World Is Complex, Not Just Noisy},
  doi = {10.17605/OSF.IO/N48GS},
  abstract = {To deny that human perception is optimal is not to claim that it is suboptimal. Rahnev \& Denison point out that optimality is often ill-defined. The fundamental issue is framing perception as a statistical inference problem. Outside the lab, the real perceptual challenge is to determine the lawful structure of the world, not variables of a predetermined statistical model.},
  journal = {PsyArXiv},
  author = {Brette, Romain},
  month = apr,
  year = {2018},
  file = {articles/Brette2018.pdf}
}

@article{Sizemore2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.05167},
  primaryClass = {q-bio},
  title = {The Importance of the Whole: Topological Data Analysis for the Network Neuroscientist},
  shorttitle = {The Importance of the Whole},
  abstract = {The application of network techniques to the analysis of neural data has greatly improved our ability to quantify and describe these rich interacting systems. Among many important contributions, networks have proven useful in identifying sets of node pairs that are densely connected and that collectively support brain function. Yet the restriction to pairwise interactions prevents us from realizing intrinsic topological features such as cavities within the interconnection structure that may be just as crucial for proper function. To detect and quantify these topological features we must turn to methods from algebraic topology that encode data as a simplicial complex built of sets of interacting nodes called simplices. On this substrate, we can then use the relations between simplices and higher-order connectivity to expose cavities within the complex, thereby summarizing its topological nature. Here we provide an introduction to persistent homology, a fundamental method from applied topology that builds a global descriptor of system structure by chronicling the evolution of cavities as we move through a combinatorial object such as a weighted network. We detail the underlying mathematics and perform demonstrative calculations on the mouse structural connectome, electrical and chemical synapses in $\backslash$textit\{C. elegans\}, and genomic interaction data. Finally we suggest avenues for future work and highlight new advances in mathematics that appear ready for use in revealing the architecture and function of neural systems.},
  journal = {arXiv:1806.05167 [q-bio]},
  author = {Sizemore, Ann E. and {Phillips-Cremins}, Jennifer and Ghrist, Robert and Bassett, Danielle S.},
  month = jun,
  year = {2018},
  keywords = {Quantitative Biology - Neurons and Cognition,55-01,Quantitative Biology - Quantitative Methods},
  file = {articles/Sizemore2018.pdf}
}

@article{Nakai2018,
  title = {Common {{Defects}} of {{Spine Dynamics}} and {{Circuit Function}} in {{Neurodevelopmental Disorders}}: {{A Systematic Review}} of {{Findings From}} in {{Vivo Optical Imaging}} of {{Mouse Models}}},
  volume = {12},
  issn = {1662-453X},
  shorttitle = {Common {{Defects}} of {{Spine Dynamics}} and {{Circuit Function}} in {{Neurodevelopmental Disorders}}},
  doi = {10.3389/fnins.2018.00412},
  abstract = {In vivo optical imaging is a powerful tool for revealing brain structure and function at both the circuit and cellular levels. Here, we provide a systematic review of findings obtained from in vivo imaging studies of mouse models of neurodevelopmental disorders, including the monogenic disorders fragile X syndrome, Rett syndrome, and Angelman syndrome, which are caused by genetic abnormalities of FMR1, MECP2, and UBE3A, as well as disorders caused by copy number variations (15q11-13 duplication and 22q11.2 deletion) and BTBR mice as an inbred strain model of autism spectrum disorder (ASD). Most studies visualize the structural and functional responsiveness of cerebral cortical neurons to sensory stimuli and the developmental and experience-dependent changes in these responses as a model of brain functions affected by these disorders. The optical imaging techniques include two-photon microscopy of fluorescently labeled dendritic spines or neurons loaded with fluorescent calcium indicators and macroscopic imaging of cortical activity using calcium indicators, voltage-sensitive dyes or intrinsic optical signals. Studies have revealed alterations in the density, stability, and turnover of dendritic spines, aberrant cortical sensory responses, impaired inhibitory function, and concomitant failure of circuit maturation as common causes for neurological deficits. Mechanistic hypotheses derived from in vivo imaging also provide new directions for therapeutic interventions. For instance, it was recently demonstrated that early postnatal administration of a selective serotonin reuptake inhibitor (SSRI) restores impaired cortical inhibitory function and ameliorates the aberrant social behaviors in a mouse model of ASD. We discuss the potential use of SSRIs for treating ASDs in light of these findings.},
  language = {English},
  journal = {Frontiers in Neuroscience},
  author = {Nakai, Nobuhiro and Takumi, Toru and Nakai, Junichi and Sato, Masaaki},
  year = {2018},
  keywords = {Autism spectrum disorders (ASD),calcium imaging,Dendritic Spines,Excitatory-Inhibitory balance,Serotonin,two-photon imaging},
  file = {articles/Nakai2018.pdf}
}

@article{Pradier2018,
  title = {Persistent but {{Labile Synaptic Plasticity}} at {{Excitatory Synapses}}},
  volume = {38},
  copyright = {Copyright \textcopyright{} 2018 the authors 0270-6474/18/385750-10\$15.00/0},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2772-17.2018},
  abstract = {Short-term synaptic plasticity contributes to many computations in the brain and allows synapses to keep a finite record of recent activity. Here we have investigated the mechanisms underlying an intriguing form of short-term plasticity termed labile LTP, at hippocampal and PFC synapses in male rats and male and female mice. In the hippocampus, labile LTP is triggered by high-frequency activation of presynaptic axons and is rapidly discharged with further activation of those axons. However, if the synapses are quiescent, they remain potentiated until further presynaptic activation. To distinguish labile LTP from NMDAR-dependent forms of potentiation, we blocked NMDARs in all experiments. Labile LTP was synapse-specific and was accompanied by a decreased paired pulse ratio, consistent with an increased release probability. Presynaptic Ca2+ and protein kinase activation during the tetanus appeared to be required for its initiation. Labile LTP was not reversed by a PKC inhibitor and did not require either RIM1$\alpha$ or synaptotagmin-7, proteins implicated in other forms of presynaptic short-term plasticity. Similar NMDAR-independent potentiation could be elicited at synapses in mPFC. Labile LTP allows for rapid information storage that is erased under controlled circumstances and could have a role in a variety of hippocampal and prefrontal cortical computations related to short-term memory.
SIGNIFICANCE STATEMENT Changes in synaptic strength are thought to represent information storage relevant to particular nervous system tasks. A single synapse can exhibit multiple overlapping forms of plasticity that shape information transfer from presynaptic to postsynaptic neurons. Here we investigate the mechanisms underlying labile LTP, an NMDAR-independent form of plasticity induced at hippocampal synapses. The potentiation is maintained for long periods as long as the synapses are infrequently active, but with regular activation, the synapses are depotentiated. Similar NMDAR-independent potentiation can also be induced at L2/3-to-L5 synapses in mPFC. Labile LTP requires a rise in presynaptic Ca2+ and protein kinase activation but is unaffected in RIM1$\alpha$ or synaptotagmin-7 mutant mice. Labile LTP may contribute to short-term or working memory in hippocampus and mPFC.},
  language = {en},
  number = {25},
  journal = {Journal of Neuroscience},
  author = {Pradier, Bruno and Lanning, Katherine and Taljan, Katherine T. and Feuille, Colin J. and Nagy, M. Aurel and Kauer, Julie A.},
  month = jun,
  year = {2018},
  keywords = {LTP,synaptic plasticity,working memory,memory,hippocampus,PFC},
  pages = {5750-5758},
  pmid = {29802202}
}

@book{Newman2010,
  address = {Oxford ; New York},
  title = {Networks: An Introduction},
  isbn = {978-0-19-920665-0},
  lccn = {T57.85 .N523 2010},
  shorttitle = {Networks},
  abstract = {"The scientific study of networks, including computer networks, social networks, and biological networks, has received an enormous amount of interest in the last few years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyze network data on a large scale, and the development of a variety of new theoretical tools has allowed us to extract new knowledge from many different kinds of networks. The study of networks is broadly interdisciplinary and important developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social sciences. This book brings together for the first time the most important breakthroughs in each of these fields and presents them in a coherent fashion, highlighting the strong interconnections between work in different areas. Subjects covered include the measurement and structure of networks in many branches of science, methods for analyzing network data, including methods developed in physics, statistics, and sociology, the fundamentals of graph theory, computer algorithms, and spectral methods, mathematical models of networks, including random graph models and generative models, and theories of dynamical processes taking place on networks"--},
  publisher = {{Oxford University Press}},
  author = {Newman, M. E. J.},
  year = {2010},
  keywords = {Network analysis (Planning),System analysis,Engineering systems,Social systems,Systems biology},
  file = {books/Newman2010_Networks-an-introduction.pdf},
  note = {OCLC: ocn456837194}
}

@article{Fernando2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.07917},
  primaryClass = {cs},
  title = {Meta {{Learning}} by the {{Baldwin Effect}}},
  doi = {10.1145/3205651.3205763},
  abstract = {The scope of the Baldwin effect was recently called into question by two papers that closely examined the seminal work of Hinton and Nowlan. To this date there has been no demonstration of its necessity in empirically challenging tasks. Here we show that the Baldwin effect is capable of evolving few-shot supervised and reinforcement learning mechanisms, by shaping the hyperparameters and the initial parameters of deep learning algorithms. Furthermore it can genetically accommodate strong learning biases on the same set of problems as a recent machine learning algorithm called MAML "Model Agnostic Meta-Learning" which uses second-order gradients instead of evolution to learn a set of reference parameters (initial weights) that can allow rapid adaptation to tasks sampled from a distribution. Whilst in simple cases MAML is more data efficient than the Baldwin effect, the Baldwin effect is more general in that it does not require gradients to be backpropagated to the reference parameters or hyperparameters, and permits effectively any number of gradient updates in the inner loop. The Baldwin effect learns strong learning dependent biases, rather than purely genetically accommodating fixed behaviours in a learning independent manner.},
  journal = {arXiv:1806.07917 [cs]},
  author = {Fernando, Chrisantha Thomas and Sygnowski, Jakub and Osindero, Simon and Wang, Jane and Schaul, Tom and Teplyashin, Denis and Sprechmann, Pablo and Pritzel, Alexander and Rusu, Andrei A.},
  month = jun,
  year = {2018},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Artificial Intelligence},
  file = {articles/Fernando2018.pdf}
}

@article{Newman2001,
  title = {Random Graphs with Arbitrary Degree Distributions and Their Applications},
  volume = {64},
  doi = {10.1103/PhysRevE.64.026118},
  abstract = {Recent work on the structure of social networks and the internet has focused attention on graphs with distributions of vertex degree that are significantly different from the Poisson degree distributions that have been widely studied in the past. In this paper we develop in detail the theory of random graphs with arbitrary degree distributions. In addition to simple undirected, unipartite graphs, we examine the properties of directed and bipartite graphs. Among other results, we derive exact expressions for the position of the phase transition at which a giant component first forms, the mean component size, the size of the giant component if there is one, the mean number of vertices a certain distance away from a randomly chosen vertex, and the average vertex-vertex distance within a graph. We apply our theory to some real-world graphs, including the world-wide web and collaboration graphs of scientists and Fortune 1000 company directors. We demonstrate that in some cases random graphs with appropriate distributions of vertex degree predict with surprising accuracy the behavior of the real world, while in others there is a measurable discrepancy between theory and reality, perhaps indicating the presence of additional social structure in the network that is not captured by the random graph., This article appears in the following collections:},
  number = {2},
  journal = {Physical Review E},
  author = {Newman, M. E. J. and Strogatz, S. H. and Watts, D. J.},
  month = jul,
  year = {2001},
  pages = {026118},
  file = {articles/Newman2001.pdf}
}

@article{Novikov2009,
  title = {On {{Distributions}} of {{First Passage Times}} and {{Optimal Stopping}} of {{AR}}(1) {{Sequences}}},
  volume = {53},
  issn = {0040-585X},
  doi = {10.1137/S0040585X97983730},
  abstract = {Sufficient conditions for the exponential boundedness of first passage times of autoregressive (AR(1)) sequences are derived in this paper. An identity involving the mean of the first passage time is obtained. Further, this identity is used for finding a logarithmic asymptotic of the mean of the first passage time of Gaussian AR(1)-sequences from a strip. Accuracy of the asymptotic approximation is illustrated by Monte Carlo simulations. A corrected approximation is suggested to improve accuracy of the approximation. An explicit formula is derived for the generating function of the first passage time for the case of AR(1)-sequences generated by an innovation with the exponential distribution. The latter formula is used to study an optimal stopping problem.},
  number = {3},
  journal = {Theory of Probability \& Its Applications},
  author = {Novikov, A.},
  month = jan,
  year = {2009},
  pages = {419-429},
  file = {articles/Novikov2009.pdf}
}

@article{Novikov2007,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0712.3468},
  primaryClass = {math},
  title = {Martingales and First Passage Times of {{AR}}(1) Sequences},
  abstract = {Using the martingale approach we find sufficient conditions for exponential boundedness of first passage times over a level for ergodic first order autoregressive sequences (AR(1)). Further, we prove a martingale identity to be used in obtaining explicit bounds for the expectation of first passage times.},
  journal = {arXiv:0712.3468 [math]},
  author = {Novikov, Alexander and Kordzakhia, Nino},
  month = dec,
  year = {2007},
  keywords = {34A30; 60J55; 49J15,Mathematics - Probability},
  file = {articles/Novikov2007.pdf}
}

@article{Omura2015,
  title = {A {{Lognormal Recurrent Network Model}} for {{Burst Generation}} during {{Hippocampal Sharp Waves}}},
  volume = {35},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4944-14.2015},
  language = {en},
  number = {43},
  journal = {Journal of Neuroscience},
  author = {Omura, Y. and Carvalho, M. M. and Inokuchi, K. and Fukai, T.},
  month = oct,
  year = {2015},
  pages = {14585-14601},
  file = {articles/Omura2015.pdf}
}

@article{Stepp2015,
  title = {Synaptic {{Plasticity Enables Adaptive Self}}-{{Tuning Critical Networks}}},
  volume = {11},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004043},
  abstract = {During rest, the mammalian cortex displays spontaneous neural activity. Spiking of single neurons during rest has been described as irregular and asynchronous. In contrast, recent in vivo and in vitro population measures of spontaneous activity, using the LFP, EEG, MEG or fMRI suggest that the default state of the cortex is critical, manifested by spontaneous, scale-invariant, cascades of activity known as neuronal avalanches. Criticality keeps a network poised for optimal information processing, but this view seems to be difficult to reconcile with apparently irregular single neuron spiking. Here, we simulate a 10,000 neuron, deterministic, plastic network of spiking neurons. We show that a combination of short- and long-term synaptic plasticity enables these networks to exhibit criticality in the face of intrinsic, i.e. self-sustained, asynchronous spiking. Brief external perturbations lead to adaptive, long-term modification of intrinsic network connectivity through long-term excitatory plasticity, whereas long-term inhibitory plasticity enables rapid self-tuning of the network back to a critical state. The critical state is characterized by a branching parameter oscillating around unity, a critical exponent close to -3/2 and a long tail distribution of a self-similarity parameter between 0.5 and 1.},
  language = {en},
  number = {1},
  journal = {PLOS Computational Biology},
  author = {Stepp, Nigel and Plenz, Dietmar and Srinivasa, Narayan},
  year = {15-Jan-2015},
  keywords = {Action potentials,Depression,Network analysis,Neuronal plasticity,Neurons,Synapses,Neural networks,Synaptic plasticity},
  pages = {e1004043},
  file = {articles/Stepp2015.pdf}
}

@article{Mollgaard2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.00715},
  title = {Measure of {{Node Similarity}} in {{Multilayer Networks}}},
  volume = {11},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0157436},
  abstract = {The weight of links in a network is often related to the similarity of the nodes. Here, we introduce a simple tunable measure for analysing the similarity of nodes across different link weights. In particular, we use the measure to analyze homophily in a group of 659 freshman students at a large university. Our analysis is based on data obtained using smartphones equipped with custom data collection software, complemented by questionnaire-based data. The network of social contacts is represented as a weighted multilayer network constructed from different channels of telecommunication as well as data on face-to-face contacts. We find that even strongly connected individuals are not more similar with respect to basic personality traits than randomly chosen pairs of individuals. In contrast, several socio-demographics variables have a significant degree of similarity. We further observe that similarity might be present in one layer of the multilayer network and simultaneously be absent in the other layers. For a variable such as gender, our measure reveals a transition from similarity between nodes connected with links of relatively low weight to dis-similarity for the nodes connected by the strongest links. We finally analyze the overlap between layers in the network for different levels of acquaintanceships.},
  number = {6},
  journal = {PLOS ONE},
  author = {Mollgaard, Anders and Zettler, Ingo and Dammeyer, Jesper and Jensen, Mogens H. and Lehmann, Sune and Mathiesen, Joachim},
  month = jun,
  year = {2016},
  keywords = {Computer Science - Social and Information Networks,Physics - Physics and Society},
  pages = {e0157436},
  file = {articles/Mollgaard2016.pdf}
}

@article{Scholz2010,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1010.0803},
  primaryClass = {cond-mat, physics:physics},
  title = {Node Similarity as a Basic Principle behind Connectivity in Complex Networks},
  abstract = {How are people linked in a highly connected society? Since in many networks a power-law (scale-free) node-degree distribution can be observed, power-law might be seen as a universal characteristics of networks. But this study of communication in the Flickr social online network reveals that power-law node-degree distributions are restricted to only sparsely connected networks. More densely connected networks, by contrast, show an increasing divergence from power-law. This work shows that this observation is consistent with the classic idea from social sciences that similarity is the driving factor behind communication in social networks. The strong relation between communication strength and node similarity could be confirmed by analyzing the Flickr network. It also is shown that node similarity as a network formation model can reproduce the characteristics of different network densities and hence can be used as a model for describing the topological transition from weakly to strongly connected societies.},
  journal = {arXiv:1010.0803 [cond-mat, physics:physics]},
  author = {Scholz, Matthias},
  month = oct,
  year = {2010},
  keywords = {Computer Science - Social and Information Networks,Physics - Physics and Society,Condensed Matter - Statistical Mechanics},
  file = {articles/Scholz2010.pdf}
}

@article{Evans2018,
  title = {A Synaptic Threshold Mechanism for Computing Escape Decisions},
  volume = {558},
  copyright = {2018 Macmillan Publishers Ltd., part of Springer Nature},
  issn = {1476-4687},
  doi = {10.1038/s41586-018-0244-6},
  abstract = {\&nbsp;In the midbrain defensive circuit, the decision to escape is computed by an unreliable synaptic connection that thresholds threat information integrated in the medial superior colliculus, and controls activation of dorsal periaqueductal grey neurons.},
  language = {en},
  number = {7711},
  journal = {Nature},
  author = {Evans, Dominic A. and Stempel, A. Vanessa and Vale, Ruben and Ruehle, Sabine and Lefler, Yaara and Branco, Tiago},
  month = jun,
  year = {2018},
  pages = {590-594}
}

@article{Barabasi1999,
  title = {Emergence of {{Scaling}} in {{Random Networks}}},
  volume = {286},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.286.5439.509},
  abstract = {Systems as diverse as genetic networks or the World Wide Web are best described as networks with complex topology. A common property of many large networks is that the vertex connectivities follow a scale-free power-law distribution. This feature was found to be a consequence of two generic mechanisms: (i) networks expand continuously by the addition of new vertices, and (ii) new vertices attach preferentially to sites that are already well connected. A model based on these two ingredients reproduces the observed stationary scale-free distributions, which indicates that the development of large networks is governed by robust self-organizing phenomena that go beyond the particulars of the individual systems.},
  language = {en},
  number = {5439},
  journal = {Science},
  author = {Barab\'asi, Albert-L\'aszl\'o and Albert, R\'eka},
  month = oct,
  year = {1999},
  pages = {509-512},
  file = {articles/Barabási1999.pdf},
  pmid = {10521342}
}

@article{Newman2000,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {cond-mat/0001118},
  title = {Models of the {{Small World}}: {{A Review}}},
  shorttitle = {Models of the {{Small World}}},
  abstract = {It is believed that almost any pair of people in the world can be connected to one another by a short chain of intermediate acquaintances, of typical length about six. This phenomenon, colloquially referred to as the ‘‘six degrees of separation,'' has been the subject of considerable recent interest within the physics community. This paper provides a short review of the topic.},
  journal = {arXiv:cond-mat/0001118},
  author = {Newman, M. E. J.},
  month = jan,
  year = {2000},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Nonlinear Sciences - Adaptation and Self-Organizing Systems},
  file = {articles/Newman2000.pdf}
}

@inproceedings{Orlandi2013,
  title = {The Emergence of Spontaneous Activity in Neuronal Cultures},
  doi = {10.1063/1.4776497},
  author = {Orlandi, J. G. and {Alvarez-Lacalle}, E. and Teller, S. and Soriano, J. and Casademunt, J.},
  year = {2013},
  pages = {25-27},
  file = {conferences/Orlandi2013_The-emergence-of-spontaneous-activity-in-neuronal-cultures.pdf}
}

@article{Korogod2015,
  title = {Ultrastructural Analysis of Adult Mouse Neocortex Comparing Aldehyde Perfusion with Cryo Fixation},
  volume = {4},
  issn = {2050-084X},
  doi = {10.7554/eLife.05793},
  language = {en},
  journal = {eLife},
  author = {Korogod, Natalya and Petersen, Carl CH and Knott, Graham W},
  month = aug,
  year = {2015},
  file = {articles/Korogod2015.pdf}
}

@article{Reimann2017,
  title = {Cliques of {{Neurons Bound}} into {{Cavities Provide}} a {{Missing Link}} between {{Structure}} and {{Function}}},
  volume = {11},
  issn = {1662-5188},
  doi = {10.3389/fncom.2017.00048},
  abstract = {The lack of a formal link between neural network structure and its emergent function has hampered our understanding of how the brain processes information. We have now come closer to describing such a link by taking the direction of synaptic transmission into account, constructing graphs of a network that reflect the direction of information flow, and analyzing these directed graphs using algebraic topology. Applying this approach to a local network of neurons in the neocortex revealed a remarkably intricate and previously unseen topology of synaptic connectivity. The synaptic network contains an abundance of cliques of neurons bound into cavities that guide the emergence of correlated activity. In response to stimuli, correlated activity binds synaptically connected neurons into functional cliques and cavities that evolve in a stereotypical sequence towards peak complexity. We propose that the brain processes stimuli by forming increasingly complex functional cliques and cavities.},
  language = {English},
  journal = {Frontiers in Computational Neuroscience},
  author = {Reimann, Michael W. and Nolte, Max and Scolamiero, Martina and Turner, Katharine and Perin, Rodrigo and Chindemi, Giuseppe and D\l{}otko, Pawe\l{} and Levi, Ran and Hess, Kathryn and Markram, Henry},
  year = {2017},
  keywords = {connectomics,correlations,Betti numbers,directed networks,Structure-function,topology},
  file = {articles/Reimann2017.pdf}
}

@article{Cote2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.11532},
  primaryClass = {cs, stat},
  title = {{{TextWorld}}: {{A Learning Environment}} for {{Text}}-Based {{Games}}},
  shorttitle = {{{TextWorld}}},
  abstract = {We introduce TextWorld, a sandbox learning environment for the training and evaluation of RL agents on text-based games. TextWorld is a Python library that handles interactive play-through of text games, as well as backend functions like state tracking and reward assignment. It comes with a curated list of games whose features and challenges we have analyzed. More significantly, it enables users to handcraft or automatically generate new games. Its generative mechanisms give precise control over the difficulty, scope, and language of constructed games, and can be used to relax challenges inherent to commercial text games like partial observability and sparse rewards. By generating sets of varied but similar games, TextWorld can also be used to study generalization and transfer learning. We cast text-based games in the Reinforcement Learning formalism, use our framework to develop a set of benchmark games, and evaluate several baseline agents on this set and the curated list.},
  journal = {arXiv:1806.11532 [cs, stat]},
  author = {C\^ot\'e, Marc-Alexandre and K\'ad\'ar, \'Akos and Yuan, Xingdi and Kybartas, Ben and Barnes, Tavian and Fine, Emery and Moore, James and Hausknecht, Matthew and Asri, Layla El and Adada, Mahmoud and Tay, Wendy and Trischler, Adam},
  month = jun,
  year = {2018},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning,Computer Science - Machine Learning},
  file = {articles/Côté2018.pdf}
}

@article{Soares2018,
  title = {Forget in a {{Flash}}: {{A Further Investigation}} of the {{Photo}}-{{Taking}}-{{Impairment Effect}}},
  volume = {7},
  issn = {2211-3681},
  shorttitle = {Forget in a {{Flash}}},
  doi = {10.1016/j.jarmac.2017.10.004},
  abstract = {A photo-taking-impairment effect has been observed such that participants are less likely to remember objects they photograph than objects they only observe. According to the offloading hypothesis, taking photos allows people to offload organic memory onto the camera's prosthetic memory, which they can rely upon to ``remember'' for them. We tested this hypothesis by manipulating whether participants perceived photo-taking as capable of serving as a form of offloading. In Experiment 1, participants used the ephemeral photo application Snapchat. In Experiment 2, participants manually deleted photos after taking them. In both experiments, participants exhibited a significant photo-taking-impairment effect even though they did not expect to have access to the photos. In fact, the effect was just as large as when participants believed they would have access to the photos. These results suggest that offloading may not be the sole, or even primary, mechanism for the photo-taking-impairment effect.},
  number = {1},
  journal = {Journal of Applied Research in Memory and Cognition},
  author = {Soares, Julia S. and Storm, Benjamin C.},
  month = mar,
  year = {2018},
  keywords = {Offloading,Photo-taking impairment,Snapchat,Transactive memory},
  pages = {154-160},
  file = {articles/Soares2018.pdf;articles/Soares22.pdf}
}

@article{Redish2018,
  title = {Opinion: {{Reproducibility}} Failures Are Essential to Scientific Inquiry},
  volume = {115},
  copyright = {\textcopyright{} 2018 . Published under the PNAS license.},
  issn = {0027-8424, 1091-6490},
  shorttitle = {Opinion},
  doi = {10.1073/pnas.1806370115},
  abstract = {Current fears of a ``reproducibility crisis'' have led researchers, sources of scientific funding, and the public to question both the efficacy and trustworthiness of science (1, 2). Suggested policy changes have been focused on statistical problems, such as p-hacking, and issues of experimental design and execution (3, 4). However, ``reproducibility'' is a broad concept that includes a number of issues (5) (see also www.pnas.org/improving\_reproducibility). Furthermore, reproducibility failures occur even in fields such as mathematics or computer science that do not have statistical problems or issues with experimental design. Most importantly, these proposed policy changes ignore a core feature of the process of scientific inquiry that occurs after reproducibility failures: the integration of conflicting observations and ideas into a coherent theory.

Fig. 1. 
Discussions about a ``reproducibility crisis'' often ignore what takes place when reproducibility fails: the integration of conflicting observations and ideas into a coherent theory. Image courtesy of Dave Cutler (artist).

Here we argue, using examples from mathematics and computer science, that current discussions of the reproducibility crisis overlook the essential role that failures of reproducibility play in scientific inquiry. This viewpoint that reproducibility is a key part of inquiry suggests several new perspectives and policies to promote good science. First, science needs to be given the time necessary to reconcile conflicting results. It typically takes decades to probe the parameter space of a discovery to identify and characterize the fundamental variables. Second, reproducibility failures are a critical part of this journey, and attention must be paid to the process of reconciling conflicting results. Third, success or failure should not be based on the conclusions of one or a few studies; strategies such as those of theoretical and synthesis articles that integrate diverse perspectives should be encouraged. We argue that the decades-long process of metabolizing reproducibility failures through theoretical \ldots{} 

[$\carriagereturn$][1]1To whom correspondence should be addressed. Email: redish\{at\}umn.edu.

 [1]: \#xref-corresp-1-1},
  language = {en},
  number = {20},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Redish, A. David and Kummerfeld, Erich and Morris, Rebecca Lea and Love, Alan C.},
  month = may,
  year = {2018},
  pages = {5042-5046},
  file = {articles/Redish2018.pdf},
  pmid = {29765001}
}

@article{Cooper2014,
  title = {Implementations Are Not Specifications: {{Specification}}, Replication and Experimentation in Computational Cognitive Modeling},
  volume = {27},
  issn = {1389-0417},
  shorttitle = {Implementations Are Not Specifications},
  doi = {10.1016/j.cogsys.2013.05.001},
  abstract = {Contemporary methods of computational cognitive modeling have recently been criticized by Addyman and French (2012) on the grounds that they have not kept up with developments in computer technology and human\textendash{}computer interaction. They present a manifesto for change according to which, it is argued, modelers should devote more effort to making their models accessible, both to non-modelers (with an appropriate easy-to-use user interface) and modelers alike. We agree that models, like data, should be freely available according to the normal standards of science, but caution against confusing implementations with specifications. Models may embody theories, but they generally also include implementation assumptions. Cognitive modeling methodology needs to be sensitive to this. We argue that specification, replication and experimentation are methodological approaches that can address this issue.},
  journal = {Cognitive Systems Research},
  author = {Cooper, Richard P. and Guest, Olivia},
  month = mar,
  year = {2014},
  keywords = {Computational experimentation,Implementation detail,Replication,Sensitivity analysis,Theory specification},
  pages = {42-49}
}

@article{Richter2017,
  series = {Computational Neuroscience},
  title = {Understanding Neural Circuit Development through Theory and Models},
  volume = {46},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2017.07.004},
  abstract = {How are neural circuits organized and tuned to achieve stable function and produce robust behavior? The organization process begins early in development and involves a diversity of mechanisms unique to this period. We summarize recent progress in theoretical neuroscience that has substantially contributed to our understanding of development at the single neuron, synaptic and network level. We go beyond classical models of topographic map formation, and focus on the generation of complex spatiotemporal activity patterns, their role in refinements of particular circuit features, and the emergence of functional computations. Aided by the development of novel quantitative methods for data analysis, theory and computational models have enabled us to test the adequacy of specific assumptions, explain experimental data and propose testable hypotheses. With the accumulation of experimental data, theory and models will likely play an even more important role in understanding the development of neural circuits.},
  journal = {Current Opinion in Neurobiology},
  author = {Richter, Leonidas MA and Gjorgjieva, Julijana},
  month = oct,
  year = {2017},
  pages = {39-47},
  file = {articles/Richter2017.pdf}
}

@article{Kappel2018,
  title = {A {{Dynamic Connectome Supports}} the {{Emergence}} of {{Stable Computational Function}} of {{Neural Circuits}} through {{Reward}}-{{Based Learning}}},
  volume = {5},
  copyright = {Copyright \textcopyright{} 2018 Kappel et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International license, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0301-17.2018},
  abstract = {Visual Abstract
$<$img class="highwire-fragment fragment-image" alt="Figure1" src="http://www.eneuro.org/content/eneuro/5/2/ENEURO.0301-17.2018/F1.medium.gif" width="440" height="142"/$>$Download figureOpen in new tabDownload powerpoint

Synaptic connections between neurons in the brain are dynamic because of continuously ongoing spine dynamics, axonal sprouting, and other processes. In fact, it was recently shown that the spontaneous synapse-autonomous component of spine dynamics is at least as large as the component that depends on the history of pre- and postsynaptic neural activity. These data are inconsistent with common models for network plasticity and raise the following questions: how can neural circuits maintain a stable computational function in spite of these continuously ongoing processes, and what could be functional uses of these ongoing processes? Here, we present a rigorous theoretical framework for these seemingly stochastic spine dynamics and rewiring processes in the context of reward-based learning tasks. We show that spontaneous synapse-autonomous processes, in combination with reward signals such as dopamine, can explain the capability of networks of neurons in the brain to configure themselves for specific computational tasks, and to compensate automatically for later changes in the network or task. Furthermore, we show theoretically and through computer simulations that stable computational performance is compatible with continuously ongoing synapse-autonomous changes. After reaching good computational performance it causes primarily a slow drift of network architecture and dynamics in task-irrelevant dimensions, as observed for neural activity in motor cortex and other areas. On the more abstract level of reinforcement learning the resulting model gives rise to an understanding of reward-driven network plasticity as continuous sampling of network configurations.},
  language = {en},
  number = {2},
  journal = {eNeuro},
  author = {Kappel, David and Legenstein, Robert and Habenschuss, Stefan and Hsieh, Michael and Maass, Wolfgang},
  month = mar,
  year = {2018},
  keywords = {reward-modulated STDP,spine dynamics,stochastic synaptic plasticity,synapse-autonomous processes,synaptic rewiring,task-irrelevant dimensions in motor control},
  pages = {ENEURO.0301-17.2018},
  file = {articles/Kappel2018.pdf}
}

@book{Vapnik2010,
  address = {New York, NY},
  edition = {2., nd ed. Softcover version of original hardcover edition 2000},
  series = {Information Science and Statistics},
  title = {The {{Nature}} of {{Statistical Learning Theory}}},
  isbn = {978-1-4419-3160-3},
  language = {eng},
  publisher = {{Springer New York}},
  author = {Vapnik, Vladimir},
  year = {2010},
  file = {books/Vapnik2010_The-Nature-of-Statistical-Learning-Theory.pdf},
  note = {OCLC: 986507767}
}

@article{Bertens2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1806.04900},
  primaryClass = {cs, stat},
  title = {A {{Machine}}-{{Learning Item Recommendation System}} for {{Video Games}}},
  abstract = {Video-game players generate huge amounts of data, as everything they do within a game is recorded. In particular, among all the stored actions and behaviors, there is information on the in-game purchases of virtual products. Such information is of critical importance in modern free-to-play titles, where gamers can select or buy a profusion of items during the game in order to progress and fully enjoy their experience. To try to maximize these kind of purchases, one can use a recommendation system so as to present players with items that might be interesting for them. Such systems can better achieve their goal by employing machine learning algorithms that are able to predict the rating of an item or product by a particular user. In this paper we evaluate and compare two of these algorithms, an ensemble-based model (extremely randomized trees) and a deep neural network, both of which are promising candidates for operational video-game recommender engines. Item recommenders can help developers improve the game. But, more importantly, it should be possible to integrate them into the game, so that users automatically get personalized recommendations while playing. The presented models are not only able to meet this challenge, providing accurate predictions of the items that a particular player will find attractive, but also sufficiently fast and robust to be used in operational settings.},
  journal = {arXiv:1806.04900 [cs, stat]},
  author = {Bertens, Paul and Guitart, Anna and Chen, Pei Pei and Peri\'a\~nez, \'Africa},
  month = jun,
  year = {2018},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  file = {articles/Bertens2018.pdf}
}

@article{2018,
  title = {Parvalbumin-{{Interneuron Output Synapses Show Spike}}-{{Timing}}-{{Dependent Plasticity}} That {{Contributes}} to {{Auditory Map Remodeling}}},
  volume = {99},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.07.018},
  abstract = {Parvalbumin (PV)-expressing interneurons mediate fast inhibition of principal neurons in many brain areas; however, long-term plasticity at PV-interne\ldots},
  language = {en},
  number = {4},
  journal = {Neuron},
  month = aug,
  year = {2018},
  pages = {720-735.e6}
}

@article{Nagaoka2016,
  title = {Abnormal Intrinsic Dynamics of Dendritic Spines in a Fragile {{X}} Syndrome Mouse Model in Vivo},
  volume = {6},
  copyright = {2016 Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/srep26651},
  abstract = {Dendritic spine generation and elimination play an important role in learning and memory, the dynamics of which have been examined within the neocortex in vivo. Spine turnover has also been detected in the absence of specific learning tasks, and is frequently exaggerated in animal models of autistic spectrum disorder (ASD). The present study aimed to examine whether the baseline rate of spine turnover was activity-dependent. This was achieved using a microfluidic brain interface and open-dura surgery, with the goal of abolishing neuronal Ca2+ signaling in the visual cortex of wild-type mice and rodent models of fragile X syndrome (Fmr1 knockout [KO]). In wild-type and Fmr1 KO mice, the majority of baseline turnover was found to be activity-independent. Accordingly, the application of matrix metalloproteinase-9 inhibitors selectively restored the abnormal spine dynamics observed in Fmr1 KO mice, without affecting the intrinsic dynamics of spine turnover in wild-type mice. Such findings indicate that the baseline turnover of dendritic spines is mediated by activity-independent intrinsic dynamics. Furthermore, these results suggest that the targeting of abnormal intrinsic dynamics might pose a novel therapy for ASD.},
  language = {en},
  journal = {Scientific Reports},
  author = {Nagaoka, Akira and Takehara, Hiroaki and {Hayashi-Takagi}, Akiko and Noguchi, Jun and Ishii, Kazuhiko and Shirai, Fukutoshi and Yagishita, Sho and Akagi, Takanori and Ichiki, Takanori and Kasai, Haruo},
  month = may,
  year = {2016},
  pages = {26651},
  file = {articles/Nagaoka2016.pdf}
}

@article{Fremaux2010,
  title = {Functional {{Requirements}} for {{Reward}}-{{Modulated Spike}}-{{Timing}}-{{Dependent Plasticity}}},
  volume = {30},
  copyright = {Copyright \textcopyright{} 2010 the authors 0270-6474/10/3013326-12\$15.00/0.  This article is freely available online through the J Neurosci Open Choice option.},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.6249-09.2010},
  abstract = {Recent experiments have shown that spike-timing-dependent plasticity is influenced by neuromodulation. We derive theoretical conditions for successful learning of reward-related behavior for a large class of learning rules where Hebbian synaptic plasticity is conditioned on a global modulatory factor signaling reward. We show that all learning rules in this class can be separated into a term that captures the covariance of neuronal firing and reward and a second term that presents the influence of unsupervised learning. The unsupervised term, which is, in general, detrimental for reward-based learning, can be suppressed if the neuromodulatory signal encodes the difference between the reward and the expected reward\textemdash{}but only if the expected reward is calculated for each task and stimulus separately. If several tasks are to be learned simultaneously, the nervous system needs an internal critic that is able to predict the expected reward for arbitrary stimuli. We show that, with a critic, reward-modulated spike-timing-dependent plasticity is capable of learning motor trajectories with a temporal resolution of tens of milliseconds. The relation to temporal difference learning, the relevance of block-based learning paradigms, and the limitations of learning with a critic are discussed.},
  language = {en},
  number = {40},
  journal = {Journal of Neuroscience},
  author = {Fr\'emaux, Nicolas and Sprekeler, Henning and Gerstner, Wulfram},
  month = oct,
  year = {2010},
  pages = {13326-13337},
  file = {articles/Frémaux2010.pdf},
  pmid = {20926659}
}

@article{Fremaux2016,
  title = {Neuromodulated {{Spike}}-{{Timing}}-{{Dependent Plasticity}}, and {{Theory}} of {{Three}}-{{Factor Learning Rules}}},
  volume = {9},
  issn = {1662-5110},
  doi = {10.3389/fncir.2015.00085},
  abstract = {Classical Hebbian learning puts the emphasis on joint pre- and postsynaptic activity, but neglects the potential role of neuromodulators. Since neuromodulators convey information about novelty or reward, the influence of neuromodulatorson synaptic plasticity is useful not just for action learning in classical conditioning, but also to decide 'when' to create new memories in response to a flow of sensory stimuli. In this review, we focus on timing requirements for pre- and postsynaptic activity in conjunction with one or several phasic neuromodulatory signals. While the emphasis of the text is on conceptual models and mathematical theories, we also discuss some experimental evidence for neuromodulation of Spike-Timing-Dependent Plasticity. We highlight the importance of synaptic mechanisms in bridging the temporal gap between sensory stimulation and neuromodulatory signals, and develop a framework for a class of neo-Hebbian three-factor learning rules that depend on presynaptic activity, postsynaptic variables as well as the influence of neuromodulators.},
  language = {English},
  journal = {Frontiers in Neural Circuits},
  author = {Fr\'emaux, Nicolas and Gerstner, Wulfram},
  year = {2016},
  keywords = {STDP,plasticity,Neuromodulation,novelty,Reward Learning,spiking neuron networks,Success,surprise,Synaptic plasticity (LTP/LTD)},
  file = {articles/Frémaux2016.pdf}
}

@article{Kim2018,
  title = {Recognition {{Dynamics}} in the {{Brain}} under the {{Free}}-{{Energy Principle}}},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01115},
  abstract = {We formulate the computational processes of perception in the framework of the principle of least action by postulating the theoretical action as a time integral of the variational free energy in the neurosciences. The free-energy principle is accordingly rephrased, on autopoetic grounds, as follows: all viable organisms attempt to minimize their sensory uncertainty about an unpredictable environment over a temporal horizon. By taking the variation of informational action, we derive neural recognition dynamics (RD), which by construction reduces to the Bayesian filtering of external states from noisy sensory inputs. Consequently, we effectively cast the gradient-descent scheme of minimizing the free energy into Hamiltonian mechanics by addressing only the positions and momenta of the organisms' representations of the causal environment. To demonstrate the utility of our theory, we show how the RD may be implemented in a neuronally based biophysical model at a single-cell level and subsequently in a coarse-grained, hierarchical architecture of the brain. We also present numerical solutions to the RD for a model brain and analyze the perceptual trajectories around attractors in neural state space.},
  journal = {Neural Computation},
  author = {Kim, Chang Sub},
  month = jul,
  year = {2018},
  pages = {1-44}
}

@article{Sporns2011,
  title = {The {{Non}}-{{Random Brain}}: {{Efficiency}}, {{Economy}}, and {{Complex Dynamics}}},
  volume = {5},
  issn = {1662-5188},
  shorttitle = {The {{Non}}-{{Random Brain}}},
  doi = {10.3389/fncom.2011.00005},
  abstract = {Modern anatomical tracing and imaging techniques are beginning to reveal the structural anatomy of neural circuits at small and large scales in unprecedented detail. When examined with analytic tools from graph theory and network science, neural connectivity exhibits highly nonrandom features, including high clustering and short path length, as well as modules and highly central hub nodes. These characteristic topological features of neural connections shape nonrandom dynamic interactions that occur during spontaneous activity or in response to external stimulation. Disturbances of connectivity and thus of neural dynamics are thought to underlie a number of disease states of the brain, and some evidence suggests that degraded functional performance of brain networks may be the outcome of a process of randomization affecting their nodes and edges. This article provides a survey of the nonrandom structure of neural connectivity, primarily at the large-scale of regions and pathways in the mammalian cerebral cortex. In addition, we will discuss how nonrandom connections can give rise to differentiated and complex patterns of dynamics and information flow. Finally, we will explore the idea that at least some disorders of the nervous system are associated with increased randomness of neural connections.},
  language = {English},
  journal = {Frontiers in Computational Neuroscience},
  author = {Sporns, Olaf},
  year = {2011},
  keywords = {Neuroimaging,networks,neural dynamics,complex systems,connectome,Neuroanatomy},
  file = {articles/Sporns2011.pdf}
}

@article{Kleberg2018a,
  title = {Neural Oligarchy: How Synaptic Plasticity Breeds Neurons with Extreme Influence},
  copyright = {\textcopyright{} 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  shorttitle = {Neural Oligarchy},
  doi = {10.1101/361394},
  abstract = {Synapses between cortical neurons are subject to constant modifications through synaptic plasticity mechanisms, which are believed to underlie learning and memory formation. The strengths of excitatory and inhibitory synapses in the cortex follow a right-skewed long-tailed distribution. Similarly, the firing rates of excitatory and inhibitory neurons also follow a right-skewed long-tailed distribution. How these distributions come about and how they maintain their shape over time is currently not well understood. Here we propose a spiking neural network model that explains the origin of these distributions as a consequence of the interaction of spike-timing dependent plasticity (STDP) of excitatory and inhibitory synapses and a multiplicative form of synaptic normalisation. Specifically, we show that the combination of additive STDP and multiplicative normalisation leads to lognormal-like distributions of excitatory and inhibitory synaptic efficacies as observed experimentally. The shape of these distributions remains stable even if spontaneous fluctuations of synaptic efficacies are added. In the same network, lognormal-like distributions of the firing rates of excitatory and inhibitory neurons result from small variability in the spiking thresholds of individual neurons. Interestingly, we find that variation in firing rates is strongly coupled to variation in synaptic efficacies: neurons with the highest firing rates develop very strong connections onto other neurons. Finally, we define an impact measure for individual neurons and demonstrate the existence of a small group of neurons with an exceptionally strong impact on the network, that arise as a result of synaptic plasticity. In summary, synaptic plasticity and small variability in neuronal parameters underlie a neural oligarchy in recurrent neural networks.},
  language = {en},
  journal = {bioRxiv},
  author = {Kleberg, Florence Isabelle and Triesch, Jochen},
  month = jul,
  year = {2018},
  pages = {361394},
  file = {articles/Kleberg22.pdf}
}

@article{Zierenberg2018,
  title = {Homeostatic {{Plasticity}} and {{External Input Shape Neural Network Dynamics}}},
  volume = {8},
  doi = {10.1103/PhysRevX.8.031018},
  abstract = {In vitro and in vivo spiking activity clearly differ. Whereas networks in vitro develop strong bursts separated by periods of very little spiking activity, in vivo cortical networks show continuous activity. This is puzzling considering that both networks presumably share similar single-neuron dynamics and plasticity rules. We propose that the defining difference between in vitro and in vivo dynamics is the strength of external input. In vitro, networks are virtually isolated, whereas in vivo every brain area receives continuous input. We analyze a model of spiking neurons in which the input strength, mediated by spike rate homeostasis, determines the characteristics of the dynamical state. In more detail, our analytical and numerical results on various network topologies show consistently that under increasing input, homeostatic plasticity generates distinct dynamic states, from bursting, to close-to-critical, reverberating, and irregular states. This implies that the dynamic state of a neural network is not fixed but can readily adapt to the input strengths. Indeed, our results match experimental spike recordings in vitro and in vivo: The in vitro bursting behavior is consistent with a state generated by very low network input ($<$0.1\%), whereas in vivo activity suggests that on the order of 1\% recorded spikes are input driven, resulting in reverberating dynamics. Importantly, this predicts that one can abolish the ubiquitous bursts of in vitro preparations, and instead impose dynamics comparable to in vivo activity by exposing the system to weak long-term stimulation, thereby opening new paths to establish an in vivo-like assay in vitro for basic as well as neurological studies.},
  number = {3},
  journal = {Physical Review X},
  author = {Zierenberg, Johannes and Wilting, Jens and Priesemann, Viola},
  month = jul,
  year = {2018},
  pages = {031018},
  file = {articles/Zierenberg2018.pdf}
}


